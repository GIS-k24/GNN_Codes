{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "import math\n",
    "import yaml\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import argparse\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Module\n",
    "import torch.optim as optim\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import sklearn\n",
    "from sklearn import neighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_distances as cos_dis, euclidean_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--gpu_id', default='0', help='Visible GPU id')\n",
    "parser.add_argument('--model_version', default='DHGNN_v1', help='DHGNN model version, acceptable: DHGNN_v1, DHGNN_v2')\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config(dir):\n",
    "    # add direction join function when parse the yaml file\n",
    "    def join(loader, node):\n",
    "        seq = loader.construct_sequence(node)\n",
    "        return os.path.sep.join(seq)\n",
    "\n",
    "    # add string concatenation function when parse the yaml file\n",
    "    def concat(loader, node):\n",
    "        seq = loader.construct_sequence(node)\n",
    "        return ''.join(seq)\n",
    "\n",
    "    yaml.add_constructor('!join', join)\n",
    "    yaml.add_constructor('!concat', concat)\n",
    "    with open(dir, 'r') as f:\n",
    "        cfg = yaml.load(f)\n",
    "\n",
    "    return cfg\n",
    "\n",
    "\n",
    "def check_dir(folder):\n",
    "    if not osp.exists(folder):\n",
    "        os.mkdir(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_index_file(filename):\n",
    "    \"\"\"\n",
    "    Copied from gcn\n",
    "    Parse index file.\n",
    "    \"\"\"\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_features(features):\n",
    "    \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
    "    rowsum = np.array(features.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    features = r_mat_inv.dot(features)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_citation_data(cfg):\n",
    "    names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
    "    objects = []\n",
    "    for i in range(len(names)):\n",
    "        with open(\"{}/ind.{}.{}\".format(cfg['data_root'], cfg['activate_dataset'], names[i]), 'rb') as f:\n",
    "            objects.append(pkl.load(f, encoding='latin1'))\n",
    "\n",
    "    x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
    "    test_idx_reorder = parse_index_file(\"{}/ind.{}.test.index\".format(cfg['data_root'], cfg['activate_dataset']))\n",
    "    test_idx_range = np.sort(test_idx_reorder)\n",
    "\n",
    "    if cfg['activate_dataset'] == 'citeseer':\n",
    "        # Fix citeseer dataset (there are some isolated nodes in the graph)\n",
    "        # Find isolated nodes, add them as zero-vecs into the right position\n",
    "        test_idx_range_full = range(min(test_idx_reorder), max(test_idx_reorder) + 1)\n",
    "        tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\n",
    "        tx_extended[test_idx_range - min(test_idx_range), :] = tx\n",
    "        tx = tx_extended\n",
    "        ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))\n",
    "        ty_extended[test_idx_range - min(test_idx_range), :] = ty\n",
    "        ty = ty_extended\n",
    "\n",
    "    features = sp.vstack((allx, tx)).tolil()\n",
    "    features[test_idx_reorder, :] = features[test_idx_range, :]\n",
    "    features = preprocess_features(features)\n",
    "    features = features.todense()\n",
    "\n",
    "    G = nx.from_dict_of_lists(graph)\n",
    "    edge_list = G.adjacency_list()\n",
    "\n",
    "    degree = [0] * len(edge_list)\n",
    "    if cfg['add_self_loop']:\n",
    "        for i in range(len(edge_list)):\n",
    "            edge_list[i].append(i)\n",
    "            degree[i] = len(edge_list[i])\n",
    "    max_deg = max(degree)\n",
    "    mean_deg = sum(degree) / len(degree)\n",
    "    print(f'max degree: {max_deg}, mean degree:{mean_deg}')\n",
    "\n",
    "    labels = np.vstack((ally, ty))\n",
    "    labels[test_idx_reorder, :] = labels[test_idx_range, :]     # one-hot labels\n",
    "    n_sample = labels.shape[0]\n",
    "    n_category = labels.shape[1]\n",
    "    lbls = np.zeros((n_sample,))\n",
    "    if cfg['activate_dataset'] == 'citeseer':\n",
    "        n_category += 1                                         # one-hot labels all zero: new category\n",
    "        for i in range(n_sample):\n",
    "            try:\n",
    "                lbls[i] = np.where(labels[i]==1)[0]                     # numerical labels\n",
    "            except ValueError:                              # labels[i] all zeros\n",
    "                lbls[i] = n_category + 1                        # new category\n",
    "    else:\n",
    "        for i in range(n_sample):\n",
    "            lbls[i] = np.where(labels[i]==1)[0]                     # numerical labels\n",
    "\n",
    "    idx_test = test_idx_range.tolist()\n",
    "    idx_train = list(range(len(y)))\n",
    "    idx_val = list(range(len(y), len(y) + 500))\n",
    "    \n",
    "    return features, lbls, idx_train, idx_val, idx_test, n_category, edge_list, edge_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def source_select(cfg):\n",
    "    return load_citation_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_dis(X):\n",
    "    X = nn.functional.normalize(X)\n",
    "    XT = X.transpose(0, 1)\n",
    "    return torch.matmul(X, XT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_ids(ids, k):\n",
    "    df = pd.DataFrame(ids)\n",
    "    sampled_ids = df.sample(k - 1, replace=True).values\n",
    "    sampled_ids = sampled_ids.flatten().tolist()\n",
    "    sampled_ids.append(ids[-1])  # must sample the centroid node itself\n",
    "    return sampled_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_ids_v2(ids, k):\n",
    "    df = pd.DataFrame(ids)\n",
    "    sampled_ids = df.sample(k, replace=True).values\n",
    "    sampled_ids = sampled_ids.flatten().tolist()\n",
    "    return sampled_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transform(nn.Module):\n",
    "    \"\"\"\n",
    "    A Vertex Transformation module\n",
    "    Permutation invariant transformation: (N, k, d) -> (N, k, d)\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_in, k):\n",
    "        \"\"\"\n",
    "        :param dim_in: input feature dimension\n",
    "        :param k: k neighbors\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.convKK = nn.Conv1d(k, k * k, dim_in, groups=k)\n",
    "        self.activation = nn.Softmax(dim=-1)\n",
    "        self.dp = nn.Dropout()\n",
    "\n",
    "    def forward(self, region_feats):\n",
    "        \"\"\"\n",
    "        :param region_feats: (N, k, d)\n",
    "        :return: (N, k, d)\n",
    "        \"\"\"\n",
    "        N, k, _ = region_feats.size()  # (N, k, d)\n",
    "        conved = self.convKK(region_feats)  # (N, k*k, 1)\n",
    "        multiplier = conved.view(N, k, k)  # (N, k, k)\n",
    "        multiplier = self.activation(multiplier)  # softmax along last dimension\n",
    "        transformed_feats = torch.matmul(multiplier, region_feats)  # (N, k, d)\n",
    "        return transformed_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VertexConv(nn.Module):\n",
    "    \"\"\"\n",
    "    A Vertex Convolution layer\n",
    "    Transform (N, k, d) feature to (N, d) feature by transform matrix and 1-D convolution\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_in, k):\n",
    "        \"\"\"\n",
    "        :param dim_in: input feature dimension\n",
    "        :param k: k neighbors\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.trans = Transform(dim_in, k)                   # (N, k, d) -> (N, k, d)\n",
    "        self.convK1 = nn.Conv1d(k, 1, 1)                    # (N, k, d) -> (N, 1, d)\n",
    "\n",
    "    def forward(self, region_feats):\n",
    "        \"\"\"\n",
    "        :param region_feats: (N, k, d)\n",
    "        :return: (N, d)\n",
    "        \"\"\"\n",
    "        transformed_feats = self.trans(region_feats)\n",
    "        pooled_feats = self.convK1(transformed_feats)             # (N, 1, d)\n",
    "        pooled_feats = pooled_feats.squeeze(1)\n",
    "        return pooled_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(nn.Module):\n",
    "    \"\"\"\n",
    "    A GCN layer\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        \"\"\"\n",
    "        :param kwargs:\n",
    "        # dim_in,\n",
    "        # dim_out,\n",
    "        # dropout_rate=0.5,\n",
    "        # activation\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim_in = kwargs['dim_in']\n",
    "        self.dim_out = kwargs['dim_out']\n",
    "        self.fc = nn.Linear(self.dim_in, self.dim_out, bias=kwargs['has_bias'])\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.activation = kwargs['activation']\n",
    "\n",
    "    def _region_aggregate(self, feats, edge_dict):\n",
    "        N = feats.size()[0]\n",
    "        pooled_feats = torch.stack([torch.mean(feats[edge_dict[i]], dim=0) for i in range(N)])\n",
    "\n",
    "        return pooled_feats\n",
    "\n",
    "    def forward(self, ids, feats, edge_dict, G, ite):\n",
    "        \"\"\"\n",
    "        :param ids: compatible with `MultiClusterConvolution`\n",
    "        :param feats:\n",
    "        :param edge_dict:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        x = feats  # (N, d)\n",
    "        x = self.dropout(self.activation(self.fc(x)))  # (N, d')\n",
    "        x = self._region_aggregate(x, edge_dict)  # (N, d)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EdgeConv(nn.Module):\n",
    "    \"\"\"\n",
    "    A Hyperedge Convolution layer\n",
    "    Using self-attention to aggregate hyperedges\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_ft, hidden):\n",
    "        \"\"\"\n",
    "        :param dim_ft: feature dimension\n",
    "        :param hidden: number of hidden layer neurons\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(nn.Linear(dim_ft, hidden), nn.ReLU(), nn.Linear(hidden, 1))\n",
    "\n",
    "    def forward(self, ft):\n",
    "        \"\"\"\n",
    "        use self attention coefficient to compute weighted average on dim=-2\n",
    "        :param ft (N, t, d)\n",
    "        :return: y (N, d)\n",
    "        \"\"\"\n",
    "        scores = []\n",
    "        n_edges = ft.size(1)\n",
    "        for i in range(n_edges):\n",
    "            scores.append(self.fc(ft[:, i]))\n",
    "        scores = torch.softmax(torch.stack(scores, 1), 1)\n",
    "        \n",
    "        return (scores * ft).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DHGLayer(GraphConvolution):\n",
    "    \"\"\"\n",
    "    A Dynamic Hypergraph Convolution Layer\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.ks = kwargs['structured_neighbor'] # number of sampled nodes in graph adjacency\n",
    "        self.n_cluster = kwargs['n_cluster']              # number of clusters\n",
    "        self.n_center = kwargs['n_center']                # a node has #n_center adjacent clusters\n",
    "        self.kn = kwargs['nearest_neighbor']    # number of the 'k' in k-NN\n",
    "        self.kc = kwargs['cluster_neighbor']    # number of sampled nodes in a adjacent k-means cluster\n",
    "        self.wu_knn=kwargs['wu_knn']\n",
    "        self.wu_kmeans=kwargs['wu_kmeans']\n",
    "        self.wu_struct=kwargs['wu_struct']\n",
    "        self.vc_sn = VertexConv(self.dim_in, self.ks+self.kn)    # structured trans\n",
    "        self.vc_s = VertexConv(self.dim_in, self.ks)    # structured trans\n",
    "        self.vc_n = VertexConv(self.dim_in, self.kn)    # nearest trans\n",
    "        self.vc_c = VertexConv(self.dim_in, self.kc)   # k-means cluster trans\n",
    "        self.ec = EdgeConv(self.dim_in, hidden=self.dim_in//4)\n",
    "        self.kmeans = None\n",
    "        self.structure = None\n",
    "\n",
    "    def _vertex_conv(self, func, x):\n",
    "        return func(x)\n",
    "\n",
    "    def _structure_select(self, ids, feats, edge_dict):\n",
    "        \"\"\"\n",
    "        :param ids: indices selected during train/valid/test, torch.LongTensor\n",
    "        :param feats:\n",
    "        :param edge_dict: torch.LongTensor\n",
    "        :return: mapped graph neighbors\n",
    "        \"\"\"\n",
    "        if self.structure is None:\n",
    "            _N = feats.size(0)\n",
    "            idx = torch.LongTensor([sample_ids(edge_dict[i], self.ks) for i in range(_N)])    # (_N, ks)\n",
    "            self.structure = idx\n",
    "        else:\n",
    "            idx = self.structure\n",
    "\n",
    "        idx = idx[ids]\n",
    "        N = idx.size(0)\n",
    "        d = feats.size(1)\n",
    "        region_feats = feats[idx.view(-1)].view(N, self.ks, d)          # (N, ks, d)\n",
    "        return region_feats\n",
    "\n",
    "    def _nearest_select(self, ids, feats):\n",
    "        \"\"\"\n",
    "        :param ids: indices selected during train/valid/test, torch.LongTensor\n",
    "        :param feats:\n",
    "        :return: mapped nearest neighbors\n",
    "        \"\"\"\n",
    "        dis = cos_dis(feats)\n",
    "        _, idx = torch.topk(dis, self.kn, dim=1)\n",
    "        idx = idx[ids]\n",
    "        N = len(idx)\n",
    "        d = feats.size(1)\n",
    "        nearest_feature = feats[idx.view(-1)].view(N, self.kn, d)         # (N, kn, d)\n",
    "        return nearest_feature\n",
    "\n",
    "    def _cluster_select(self, ids, feats):\n",
    "        \"\"\"\n",
    "        compute k-means centers and cluster labels of each node\n",
    "        return top #n_cluster nearest cluster transformed features\n",
    "        :param ids: indices selected during train/valid/test, torch.LongTensor\n",
    "        :param feats:\n",
    "        :return: top #n_cluster nearest cluster mapped features\n",
    "        \"\"\"\n",
    "        if self.kmeans is None:\n",
    "            _N = feats.size(0)\n",
    "            np_feats = feats.detach().cpu().numpy()\n",
    "            kmeans = KMeans(n_clusters=self.n_cluster, random_state=0, n_jobs=-1).fit(np_feats)\n",
    "            centers = kmeans.cluster_centers_\n",
    "            dis = euclidean_distances(np_feats, centers)\n",
    "            _, cluster_center_dict = torch.topk(torch.Tensor(dis), self.n_center, largest=False)\n",
    "            cluster_center_dict = cluster_center_dict.numpy()\n",
    "            point_labels = kmeans.labels_\n",
    "            point_in_which_cluster = [np.where(point_labels == i)[0] for i in range(self.n_cluster)]\n",
    "            idx = torch.LongTensor([[sample_ids_v2(point_in_which_cluster[cluster_center_dict[point][i]], self.kc)   \n",
    "                        for i in range(self.n_center)] for point in range(_N)])    # (_N, n_center, kc)\n",
    "            self.kmeans = idx\n",
    "        else:\n",
    "            idx = self.kmeans\n",
    "        \n",
    "        idx = idx[ids]\n",
    "        N = idx.size(0)\n",
    "        d = feats.size(1)\n",
    "        cluster_feats = feats[idx.view(-1)].view(N, self.n_center, self.kc, d)\n",
    "\n",
    "        return cluster_feats                    # (N, n_center, kc, d)\n",
    "\n",
    "    def _edge_conv(self, x):\n",
    "        return self.ec(x)\n",
    "\n",
    "    def _fc(self, x):\n",
    "        return self.activation(self.fc(self.dropout(x)))\n",
    "\n",
    "    def forward(self, ids, feats, edge_dict, G, ite):\n",
    "        hyperedges = []    \n",
    "        if ite >= self.wu_kmeans:\n",
    "            c_feat = self._cluster_select(ids, feats)\n",
    "            for c_idx in range(c_feat.size(1)):\n",
    "                xc = self._vertex_conv(self.vc_c, c_feat[:, c_idx, :, :])\n",
    "                xc  = xc.view(len(ids), 1, feats.size(1))               # (N, 1, d)          \n",
    "                hyperedges.append(xc)\n",
    "        if ite >= self.wu_knn:\n",
    "            n_feat = self._nearest_select(ids, feats)\n",
    "            xn = self._vertex_conv(self.vc_n, n_feat)\n",
    "            xn  = xn.view(len(ids), 1, feats.size(1))                   # (N, 1, d)\n",
    "            hyperedges.append(xn)\n",
    "        if ite >= self.wu_struct:\n",
    "            s_feat = self._structure_select(ids, feats, edge_dict)\n",
    "            xs = self._vertex_conv(self.vc_s, s_feat)\n",
    "            xs  = xs.view(len(ids), 1, feats.size(1))                   # (N, 1, d)\n",
    "            hyperedges.append(xs)\n",
    "        x = torch.cat(hyperedges, dim=1)\n",
    "        x = self._edge_conv(x)                                          # (N, d)\n",
    "        x = self._fc(x)                                                 # (N, d')\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HGNN_conv(nn.Module):\n",
    "    \"\"\"\n",
    "    A HGNN layer\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(HGNN_conv, self).__init__()\n",
    "\n",
    "        self.dim_in = kwargs['dim_in']\n",
    "        self.dim_out = kwargs['dim_out']\n",
    "        self.fc = nn.Linear(self.dim_in, self.dim_out, bias=kwargs['has_bias'])\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.activation = kwargs['activation']\n",
    "\n",
    "\n",
    "    def forward(self, ids, feats, edge_dict, G, ite):\n",
    "        x = feats\n",
    "        x = self.activation(self.fc(x))\n",
    "        x = G.matmul(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DHGNN_v1(nn.Module):\n",
    "    \"\"\"\n",
    "    Dynamic Hypergraph Convolution Neural Network with a GCN-style input layer\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim_feat = kwargs['dim_feat']\n",
    "        self.n_categories = kwargs['n_categories']\n",
    "        self.n_layers = kwargs['n_layers']\n",
    "        layer_spec = kwargs['layer_spec']\n",
    "        self.dims_in = [self.dim_feat] + layer_spec\n",
    "        self.dims_out = layer_spec + [self.n_categories]\n",
    "        activations = nn.ModuleList([nn.ReLU() for i in range(self.n_layers - 1)] + [nn.LogSoftmax(dim=-1)])\n",
    "        self.gcs = nn.ModuleList([GraphConvolution(\n",
    "            dim_in=self.dims_in[0],\n",
    "            dim_out=self.dims_out[0],\n",
    "            dropout_rate=kwargs['dropout_rate'],\n",
    "            activation=activations[0],\n",
    "            has_bias=kwargs['has_bias'])]\n",
    "            + [DHGLayer(\n",
    "            dim_in=self.dims_in[i],\n",
    "            dim_out=self.dims_out[i],\n",
    "            dropout_rate=kwargs['dropout_rate'],\n",
    "            activation=activations[i],\n",
    "            structured_neighbor=kwargs['k_structured'],\n",
    "            nearest_neighbor=kwargs['k_nearest'],\n",
    "            cluster_neighbor=kwargs['k_cluster'],\n",
    "            wu_knn=kwargs['wu_knn'],\n",
    "            wu_kmeans=kwargs['wu_kmeans'],\n",
    "            wu_struct=kwargs['wu_struct'],\n",
    "            n_cluster=kwargs['clusters'],\n",
    "            n_center=kwargs['adjacent_centers'],\n",
    "            has_bias=kwargs['has_bias']) for i in range(1, self.n_layers)])\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        \"\"\"\n",
    "        :param feats:\n",
    "        :param edge_dict:\n",
    "        :param G:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        ids = kwargs['ids']\n",
    "        feats = kwargs['feats']\n",
    "        edge_dict = kwargs['edge_dict']\n",
    "        G = kwargs['G']\n",
    "        ite = kwargs['ite']\n",
    "\n",
    "        x = feats\n",
    "        for i_layer in range(self.n_layers):\n",
    "            x = self.gcs[i_layer](ids, x, edge_dict, G, ite)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DHGNN_v2(nn.Module):\n",
    "    \"\"\"\n",
    "    Dynamic Hypergraph Convolution Neural Network with a HGNN-style input layer\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim_feat = kwargs['dim_feat']\n",
    "        self.n_categories = kwargs['n_categories']\n",
    "        self.n_layers = kwargs['n_layers']\n",
    "        layer_spec = kwargs['layer_spec']\n",
    "        self.dims_in = [self.dim_feat] + layer_spec\n",
    "        self.dims_out = layer_spec + [self.n_categories]\n",
    "        activations = nn.ModuleList([nn.ReLU() for i in range(self.n_layers - 1)] + [nn.LogSoftmax(dim=-1)])\n",
    "        self.gcs = nn.ModuleList([HGNN_conv(\n",
    "            dim_in=self.dims_in[0],\n",
    "            dim_out=self.dims_out[0],\n",
    "            dropout_rate=kwargs['dropout_rate'],\n",
    "            activation=activations[0],\n",
    "            has_bias=kwargs['has_bias'])]\n",
    "            + [DHGLayer(\n",
    "            dim_in=self.dims_in[i],\n",
    "            dim_out=self.dims_out[i],\n",
    "            dropout_rate=kwargs['dropout_rate'],\n",
    "            activation=activations[i],\n",
    "            structured_neighbor=kwargs['k_structured'],\n",
    "            nearest_neighbor=kwargs['k_nearest'],\n",
    "            cluster_neighbor=kwargs['k_cluster'],\n",
    "            wu_knn=kwargs['wu_knn'],\n",
    "            wu_kmeans=kwargs['wu_kmeans'],\n",
    "            wu_struct=kwargs['wu_struct'],\n",
    "            n_cluster=kwargs['clusters'],\n",
    "            n_center=kwargs['adjacent_centers'],\n",
    "            has_bias=kwargs['has_bias']) for i in range(1, self.n_layers)])\n",
    "\n",
    "    def forward(self, **kwargs):\n",
    "        \"\"\"\n",
    "        :param feats:\n",
    "        :param edge_dict:\n",
    "        :param G:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        ids = kwargs['ids']\n",
    "        feats = kwargs['feats']\n",
    "        edge_dict = kwargs['edge_dict']\n",
    "        G = kwargs['G']\n",
    "        ite = kwargs['ite']\n",
    "\n",
    "        x = feats\n",
    "        for i_layer in range(self.n_layers):\n",
    "            x = self.gcs[i_layer](ids, x, edge_dict, G, ite)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_select(activate_model):\n",
    "    if activate_model == 'DHGNN_v1':\n",
    "        return DHGNN_v1\n",
    "    elif activate_model == 'DHGNN_v2':\n",
    "        return DHGNN_v2\n",
    "    else:\n",
    "        raise ValueError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _edge_dict_to_H(edge_dict):\n",
    "    n_nodes = len(edge_dict)\n",
    "    H = np.zeros(shape=(n_nodes, n_nodes))\n",
    "    for center_id, adj_list in enumerate(edge_dict):\n",
    "        H[center_id, center_id] = 1.0\n",
    "        for adj_id in adj_list:\n",
    "            H[adj_id, center_id] = 1.0\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_G_from_H(H, variable_weight=False):\n",
    "    H = np.array(H)\n",
    "    n_edge = H.shape[1]\n",
    "    # the weight of the hyperedge\n",
    "    W = np.ones(n_edge)\n",
    "    # the degree of the node\n",
    "    DV = np.sum(H * W, axis=1)\n",
    "    # the degree of the hyperedge\n",
    "    DE = np.sum(H, axis=0)\n",
    "\n",
    "    invDE = np.mat(np.diag(np.power(DE, -1)))\n",
    "    DV2 = np.mat(np.diag(np.power(DV, -0.5)))\n",
    "    W = np.mat(np.diag(W))\n",
    "    H = np.mat(H)\n",
    "    HT = H.T\n",
    "\n",
    "    if variable_weight:\n",
    "        DV2_H = DV2 * H\n",
    "        invDE_HT_DV2 = invDE * HT * DV2\n",
    "        return DV2_H, W, invDE_HT_DV2\n",
    "    else:\n",
    "        G = DV2 * H * W * invDE * HT * DV2\n",
    "        return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 随机种子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using random seed:  1000\n"
     ]
    }
   ],
   "source": [
    "seed_num = 1000\n",
    "setup_seed(seed_num) \n",
    "print('Using random seed: ', seed_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_config(r'C:\\Users\\sss\\Desktop\\DHGNN-master\\config/config.yaml')\n",
    "cfg[\"model\"] = args.model_version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, fts, lbls, idx_train, idx_val, edge_dict, G, criterion, optimizer, scheduler, device, num_epochs=25, print_freq=500):\n",
    "    since = time.time()\n",
    "\n",
    "    state_dict_updates = 0          # number of epochs that updates state_dict\n",
    "\n",
    "    device = torch.cuda.is_available()\n",
    "    \n",
    "    if device:\n",
    "        model = model.cuda()\n",
    "    else:\n",
    "        model = model\n",
    "\n",
    "    model_wts_best_val_acc = copy.deepcopy(model.state_dict())\n",
    "    model_wts_lowest_val_loss = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    best_acc = 0.0\n",
    "    loss_min = 100.0\n",
    "    acc_epo = 0\n",
    "    loss_epo = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epo = epoch\n",
    "\n",
    "        if epoch % print_freq == 0:\n",
    "            print('-' * 10)\n",
    "            print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            idx = idx_train if phase == 'train' else idx_val\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                outputs = model(ids=idx, feats=fts, edge_dict=edge_dict, G=G, ite=epo)\n",
    "\n",
    "                loss = criterion(outputs, lbls[idx]) * len(idx)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            # statistics\n",
    "            running_loss += loss\n",
    "            running_corrects += torch.sum(preds == lbls.data[idx])\n",
    "\n",
    "            epoch_loss = running_loss / len(idx)\n",
    "            epoch_acc = running_corrects.double() / len(idx)\n",
    "\n",
    "            if epoch % print_freq == 0:\n",
    "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "\n",
    "                model_wts_best_val_acc = copy.deepcopy(model.state_dict())\n",
    "\n",
    "                acc_epo = epoch \n",
    "                state_dict_updates += 1\n",
    "\n",
    "            if phase == 'val' and epoch_loss < loss_min:\n",
    "                loss_min = epoch_loss\n",
    "\n",
    "                model_wts_lowest_val_loss = copy.deepcopy(model.state_dict())\n",
    "\n",
    "                loss_epo = epoch \n",
    "                state_dict_updates += 1\n",
    "\n",
    "            if epoch % print_freq == 0 and phase == 'val':\n",
    "                print(f'Best val Acc: {best_acc:4f}, Min val loss: {loss_min:4f}')\n",
    "                print('-' * 20)\n",
    "\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'\\nTraining complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'\\nState dict updates {state_dict_updates}')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    return (model_wts_best_val_acc, acc_epo), (model_wts_lowest_val_loss, loss_epo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, best_model_wts, fts, lbls, n_category, idx_test, edge_dict, G, device, test_time = 1):\n",
    "    best_model_wts, epo = best_model_wts\n",
    "    \n",
    "    device = torch.cuda.is_available()\n",
    "    \n",
    "    if device:\n",
    "        model = model.cuda()\n",
    "    else:\n",
    "        model = model\n",
    "        \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    model.eval()\n",
    "\n",
    "    running_corrects = 0.0\n",
    "\n",
    "    if device:\n",
    "        outputs = torch.zeros(len(idx_test), n_category).cuda()\n",
    "    else:\n",
    "        outputs = torch.zeros(len(idx_test), n_category)\n",
    "\n",
    "    for _ in range(test_time):\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            outputs += model(ids=idx_test, feats=fts, edge_dict=edge_dict, G=G, ite=epo)\n",
    "\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    running_corrects += torch.sum(preds == lbls.data[idx_test])\n",
    "    test_acc = running_corrects.double() / len(idx_test)\n",
    "\n",
    "    print('*' * 20)\n",
    "    print(f'Test acc: {test_acc} @Epoch-{epo}')\n",
    "    print('*' * 20)\n",
    "\n",
    "    return test_acc, epo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_model(cfg):\n",
    "    device = torch.cuda.is_available()\n",
    "\n",
    "    source = source_select(cfg)\n",
    "    print(f'Using {cfg[\"activate_dataset\"]} dataset')\n",
    "    fts, lbls, idx_train, idx_val, idx_test, n_category, _, edge_dict = source(cfg)\n",
    "\n",
    "    H = _edge_dict_to_H(edge_dict)\n",
    "    G = _generate_G_from_H(H)\n",
    "\n",
    "    if device:\n",
    "        G = torch.Tensor(G).cuda()\n",
    "        fts = torch.Tensor(fts).cuda()\n",
    "        lbls = torch.Tensor(lbls).squeeze().long().cuda()\n",
    "        \n",
    "    else:\n",
    "        G = torch.Tensor(G)\n",
    "        fts = torch.Tensor(fts)\n",
    "        lbls = torch.Tensor(lbls).squeeze().long()\n",
    "\n",
    "    model = model_select(cfg['model'])(\n",
    "        dim_feat=fts.size(1),\n",
    "        n_categories=n_category,\n",
    "        k_structured=cfg['k_structured'],\n",
    "        k_nearest=cfg['k_nearest'],\n",
    "        k_cluster=cfg['k_cluster'],\n",
    "        wu_knn=cfg['wu_knn'],\n",
    "        wu_kmeans=cfg['wu_kmeans'],\n",
    "        wu_struct=cfg['wu_struct'],\n",
    "        clusters=cfg['clusters'],\n",
    "        adjacent_centers=cfg['adjacent_centers'],\n",
    "        n_layers=cfg['n_layers'],\n",
    "        layer_spec=cfg['layer_spec'],\n",
    "        dropout_rate=cfg['drop_out'],\n",
    "        has_bias=cfg['has_bias']\n",
    "    )\n",
    "\n",
    "    #initialize model\n",
    "    state_dict = model.state_dict()\n",
    "    for key in state_dict:\n",
    "        if 'weight' in key:\n",
    "            nn.init.xavier_uniform_(state_dict[key])\n",
    "        elif 'bias' in key:\n",
    "            state_dict[key] = state_dict[key].zero_()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=cfg['lr'],weight_decay=cfg['weight_decay'], eps=1e-20)\n",
    "    schedular = optim.lr_scheduler.MultiStepLR(optimizer, milestones=cfg['milestones'], gamma=cfg['gamma'])\n",
    "    criterion = torch.nn.NLLLoss()\n",
    "\n",
    "    # transductive learning mode\n",
    "    model_wts_best_val_acc, model_wts_lowest_val_loss = train(model, fts, lbls, idx_train, idx_val, edge_dict, G, criterion, optimizer, schedular, device, cfg['max_epoch'], cfg['print_freq'])\n",
    "    \n",
    "    if idx_test is not None:\n",
    "        print('**** Model of lowest val loss ****')\n",
    "        test_acc_lvl, epo_lvl = test(model, model_wts_lowest_val_loss, fts, lbls, n_category, idx_test, edge_dict, G, device, cfg['test_time'])\n",
    "        print('**** Model of best val acc ****')\n",
    "        test_acc_bva, epo_bva = test(model, model_wts_best_val_acc, fts, lbls, n_category, idx_test, edge_dict, G, device, cfg['test_time'])\n",
    "        return (test_acc_lvl, epo_lvl), (test_acc_bva, epo_bva)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using random seed:  1000\n"
     ]
    }
   ],
   "source": [
    "seed_num = 1000\n",
    "setup_seed(seed_num)\n",
    "print('Using random seed: ', seed_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = r\"C:\\Users\\sss\\Desktop\\DHGNN-master/\"\n",
    "\n",
    "cfg = get_config(root + 'config/config.yaml')\n",
    "cfg['model'] = args.model_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cora dataset\n",
      "max degree: 169, mean degree:4.89807976366322\n",
      "----------\n",
      "Epoch 0/24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sss\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train Loss: 1.9457 Acc: 0.1786\n",
      "val Loss: 1.9401 Acc: 0.1300\n",
      "Best val Acc: 0.130000, Min val loss: 1.940101\n",
      "--------------------\n",
      "----------\n",
      "Epoch 1/24\n",
      "train Loss: 1.9388 Acc: 0.2000\n",
      "val Loss: 1.9108 Acc: 0.6220\n",
      "Best val Acc: 0.622000, Min val loss: 1.910787\n",
      "--------------------\n",
      "----------\n",
      "Epoch 2/24\n",
      "train Loss: 1.9137 Acc: 0.3500\n",
      "val Loss: 1.8588 Acc: 0.7000\n",
      "Best val Acc: 0.700000, Min val loss: 1.858844\n",
      "--------------------\n",
      "----------\n",
      "Epoch 3/24\n",
      "train Loss: 1.8730 Acc: 0.4429\n",
      "val Loss: 1.7715 Acc: 0.7500\n",
      "Best val Acc: 0.750000, Min val loss: 1.771529\n",
      "--------------------\n",
      "----------\n",
      "Epoch 4/24\n",
      "train Loss: 1.7879 Acc: 0.5429\n",
      "val Loss: 1.6423 Acc: 0.7740\n",
      "Best val Acc: 0.774000, Min val loss: 1.642348\n",
      "--------------------\n",
      "----------\n",
      "Epoch 5/24\n",
      "train Loss: 1.7174 Acc: 0.7000\n",
      "val Loss: 1.5523 Acc: 0.7760\n",
      "Best val Acc: 0.776000, Min val loss: 1.552295\n",
      "--------------------\n",
      "----------\n",
      "Epoch 6/24\n",
      "train Loss: 1.4879 Acc: 0.7857\n",
      "val Loss: 1.3174 Acc: 0.8000\n",
      "Best val Acc: 0.800000, Min val loss: 1.317384\n",
      "--------------------\n",
      "----------\n",
      "Epoch 7/24\n",
      "train Loss: 1.2110 Acc: 0.8143\n",
      "val Loss: 1.0639 Acc: 0.8000\n",
      "Best val Acc: 0.800000, Min val loss: 1.063897\n",
      "--------------------\n",
      "----------\n",
      "Epoch 8/24\n",
      "train Loss: 0.8907 Acc: 0.8571\n",
      "val Loss: 0.8393 Acc: 0.8100\n",
      "Best val Acc: 0.810000, Min val loss: 0.839315\n",
      "--------------------\n",
      "----------\n",
      "Epoch 9/24\n",
      "train Loss: 0.6301 Acc: 0.8857\n",
      "val Loss: 0.6793 Acc: 0.8120\n",
      "Best val Acc: 0.812000, Min val loss: 0.679345\n",
      "--------------------\n",
      "----------\n",
      "Epoch 10/24\n",
      "train Loss: 0.4381 Acc: 0.9000\n",
      "val Loss: 0.6208 Acc: 0.8160\n",
      "Best val Acc: 0.816000, Min val loss: 0.620765\n",
      "--------------------\n",
      "----------\n",
      "Epoch 11/24\n",
      "train Loss: 0.3130 Acc: 0.8786\n",
      "val Loss: 0.6449 Acc: 0.8000\n",
      "Best val Acc: 0.816000, Min val loss: 0.620765\n",
      "--------------------\n",
      "----------\n",
      "Epoch 12/24\n",
      "train Loss: 0.2131 Acc: 0.9286\n",
      "val Loss: 0.6691 Acc: 0.8080\n",
      "Best val Acc: 0.816000, Min val loss: 0.620765\n",
      "--------------------\n",
      "----------\n",
      "Epoch 13/24\n",
      "train Loss: 0.2104 Acc: 0.9286\n",
      "val Loss: 0.7015 Acc: 0.8120\n",
      "Best val Acc: 0.816000, Min val loss: 0.620765\n",
      "--------------------\n",
      "----------\n",
      "Epoch 14/24\n",
      "train Loss: 0.1194 Acc: 0.9714\n",
      "val Loss: 0.7988 Acc: 0.8100\n",
      "Best val Acc: 0.816000, Min val loss: 0.620765\n",
      "--------------------\n",
      "----------\n",
      "Epoch 15/24\n",
      "train Loss: 0.1408 Acc: 0.9357\n",
      "val Loss: 0.8417 Acc: 0.8100\n",
      "Best val Acc: 0.816000, Min val loss: 0.620765\n",
      "--------------------\n",
      "----------\n",
      "Epoch 16/24\n",
      "train Loss: 0.0831 Acc: 0.9714\n",
      "val Loss: 0.9405 Acc: 0.8000\n",
      "Best val Acc: 0.816000, Min val loss: 0.620765\n",
      "--------------------\n",
      "----------\n",
      "Epoch 17/24\n",
      "train Loss: 0.0530 Acc: 0.9786\n",
      "val Loss: 0.9883 Acc: 0.8000\n",
      "Best val Acc: 0.816000, Min val loss: 0.620765\n",
      "--------------------\n",
      "----------\n",
      "Epoch 18/24\n",
      "train Loss: 0.0670 Acc: 0.9714\n",
      "val Loss: 1.0014 Acc: 0.8060\n",
      "Best val Acc: 0.816000, Min val loss: 0.620765\n",
      "--------------------\n",
      "----------\n",
      "Epoch 19/24\n",
      "train Loss: 0.0299 Acc: 0.9929\n",
      "val Loss: 1.0267 Acc: 0.8140\n",
      "Best val Acc: 0.816000, Min val loss: 0.620765\n",
      "--------------------\n",
      "----------\n",
      "Epoch 20/24\n",
      "train Loss: 0.0487 Acc: 0.9786\n",
      "val Loss: 1.0688 Acc: 0.8240\n",
      "Best val Acc: 0.824000, Min val loss: 0.620765\n",
      "--------------------\n",
      "----------\n",
      "Epoch 21/24\n",
      "train Loss: 0.0681 Acc: 0.9786\n",
      "val Loss: 1.1027 Acc: 0.8200\n",
      "Best val Acc: 0.824000, Min val loss: 0.620765\n",
      "--------------------\n",
      "----------\n",
      "Epoch 22/24\n",
      "train Loss: 0.0088 Acc: 1.0000\n",
      "val Loss: 1.1596 Acc: 0.8060\n",
      "Best val Acc: 0.824000, Min val loss: 0.620765\n",
      "--------------------\n",
      "----------\n",
      "Epoch 23/24\n",
      "train Loss: 0.0161 Acc: 1.0000\n",
      "val Loss: 1.2078 Acc: 0.8040\n",
      "Best val Acc: 0.824000, Min val loss: 0.620765\n",
      "--------------------\n",
      "----------\n",
      "Epoch 24/24\n",
      "train Loss: 0.0620 Acc: 0.9714\n",
      "val Loss: 1.2157 Acc: 0.8260\n",
      "Best val Acc: 0.826000, Min val loss: 0.620765\n",
      "--------------------\n",
      "\n",
      "Training complete in 4m 49s\n",
      "\n",
      "State dict updates 23\n",
      "Best val Acc: 0.826000\n",
      "**** Model of lowest val loss ****\n",
      "********************\n",
      "Test acc: 0.82 @Epoch-10\n",
      "********************\n",
      "**** Model of best val acc ****\n",
      "********************\n",
      "Test acc: 0.836 @Epoch-24\n",
      "********************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((tensor(0.8200, dtype=torch.float64), 10),\n",
       " (tensor(0.8360, dtype=torch.float64), 24))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_model(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
