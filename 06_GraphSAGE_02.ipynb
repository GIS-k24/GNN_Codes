{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Simple supervised GraphSAGE model as well as examples running the model on the Cora and Pubmed datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cora数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cora():\n",
    "    num_nodes = 2708\n",
    "    num_feats = 1433\n",
    "    feat_data = np.zeros((num_nodes, num_feats))\n",
    "    labels = np.empty((num_nodes, 1), dtype=np.int64)\n",
    "    \n",
    "    node_map = {}\n",
    "    label_map = {}\n",
    "    \n",
    "    with open(r\"C:\\Users\\sss\\Desktop\\cora/cora.content\") as fp:\n",
    "        for i, line in enumerate(fp):\n",
    "            info = line.strip().split()\n",
    "            feat_data[i, :] = list(map(float, info[1: -1]))  # list\n",
    "            node_map[info[0]] = i\n",
    "            if not info[-1] in label_map:\n",
    "                label_map[info[-1]] = len(label_map)\n",
    "            labels[i] = label_map[info[-1]]\n",
    "            \n",
    "    \n",
    "    adj_lists = defaultdict(set)\n",
    "    with open(r\"C:\\Users\\sss\\Desktop\\cora/cora.cites\") as fp:\n",
    "        for i, line in enumerate(fp):\n",
    "            info = line.strip().split()\n",
    "            paper1 = node_map[info[0]]\n",
    "            paper2 = node_map[info[1]]\n",
    "            adj_lists[paper1].add(paper2)\n",
    "            adj_lists[paper2].add(paper1)\n",
    "            \n",
    "    return feat_data, labels, adj_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pubmed():\n",
    "    # hardcoded for simplicity...\n",
    "    num_nodes = 19717\n",
    "    num_feats = 500\n",
    "    feat_data = np.zeros((num_nodes, num_feats))\n",
    "    labels = np.empty((num_nodes, 1), dtype=np.int64)\n",
    "    \n",
    "    node_map = {}\n",
    "    \n",
    "    with open(r\"C:\\Users\\sss\\Desktop\\pubmed/Pubmed-Diabetes.NODE.paper.tab\") as fp:\n",
    "        fp.readline()\n",
    "        feat_map = {entry.split(\":\")[1]: i - 1 for i, entry in enumerate(fp.readline().split(\"\\t\"))}\n",
    "        for i, line in enumerate(fp):\n",
    "            info = line.split(\"\\t\")\n",
    "            node_map[info[0]] = i\n",
    "            labels[i] = int(info[1].split(\"=\")[1]) - 1\n",
    "            for word_info in info[2: -1]:\n",
    "                word_info = word_info.split(\"=\")\n",
    "                feat_data[i][feat_map[word_info[0]]] = float(word_info[1])\n",
    "                \n",
    "    adj_lists = defaultdict(set)\n",
    "    with open(r\"C:\\Users\\sss\\Desktop\\pubmed/Pubmed-Diabetes.DIRECTED.cites.tab\") as fp:\n",
    "        fp.readline()\n",
    "        fp.readline()\n",
    "        for line in fp:\n",
    "            info = line.strip().split(\"\\t\")\n",
    "            paper1 = node_map[info[1].split(\":\")[1]]\n",
    "            paper2 = node_map[info[-1].split(\":\")[1]]\n",
    "            adj_lists[paper1].add(paper2)\n",
    "            adj_lists[paper2].add(paper1)\n",
    "            \n",
    "    return feat_data, labels, adj_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_data, labels, adj_lists = load_pubmed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "- Encodes a node's using 'convolutional' GraphSage approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, features, feature_dim, embed_dim, adj_lists, aggregator, num_sample=10, base_model=None, gcn=False, cuda=False, feature_transform=False):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.features = features\n",
    "        self.feat_dim = feature_dim\n",
    "        self.adj_lists = adj_lists\n",
    "        self.aggregator = aggregator\n",
    "        self.num_sample = num_sample\n",
    "        if base_model != None:\n",
    "            self.base_model = base_model\n",
    "\n",
    "        self.gcn = gcn\n",
    "        self.embed_dim = embed_dim\n",
    "        self.cuda = cuda\n",
    "        self.aggregator.cuda = cuda\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.FloatTensor(embed_dim, self.feat_dim if self.gcn else 2 * self.feat_dim)\n",
    "        )\n",
    "        init.xavier_uniform(self.weight)\n",
    "        \n",
    "    def forward(self, nodes):\n",
    "        # Generates embeddings for a batch of nodes. | nodes     -- list of nodes\n",
    "        \n",
    "        neigh_feats = self.aggregator.forward(nodes, [self.adj_lists[int(node)] for node in nodes], self.num_sample)\n",
    "        if not self.gcn:\n",
    "            if self.cuda:\n",
    "                self_feats = self.features(torch.LongTensor(nodes).cuda())\n",
    "            else:\n",
    "                self_feats = self.features(torch.LongTensor(nodes))\n",
    "            combined = torch.cat([self_feats, neigh_feats], dim=1)\n",
    "        else:\n",
    "            combined = neigh_feats\n",
    "            \n",
    "        combined = F.relu(self.weight.mm(combined.t()))\n",
    "        \n",
    "        return combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MeanAggregator\n",
    "- Set of modules for aggregating embeddings of neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanAggregator(nn.Module):\n",
    "    # Aggregates a node's embeddings using mean of neighbors' embeddings\n",
    "    def __init__(self, features, cuda=False, gcn=False):\n",
    "        # Initializes the aggregator for a specific graph.\n",
    "        # features -- function mapping LongTensor of node ids to FloatTensor of feature values.\n",
    "        # cuda -- whether to use GPU\n",
    "        # gcn --- whether to perform concatenation GraphSAGE-style, or add self-loops GCN-style\n",
    "        super(MeanAggregator, self).__init__()\n",
    "        \n",
    "        self.features = features\n",
    "        self.cuda = cuda\n",
    "        self.gcn = gcn\n",
    "        \n",
    "    def forward(self, nodes, to_neighs, num_sample=10):\n",
    "        \"\"\"\n",
    "        nodes --- list of nodes in a batch\n",
    "        to_neighs --- list of sets, each set is the set of neighbors for node in batch\n",
    "        num_sample --- number of neighbors to sample. No sampling if None.\n",
    "        \"\"\"\n",
    "        # Local pointers to functions (speed hack)\n",
    "        \n",
    "        _set = set\n",
    "        if not num_sample is None:\n",
    "            _sample = random.sample\n",
    "            samp_neighs = [_set(\n",
    "                _sample(to_neigh, num_sample, )\n",
    "            ) if len(to_neigh) >= num_sample else to_neigh for to_neigh in to_neighs]\n",
    "            \n",
    "        else:\n",
    "            samp_neighs = to_neighs\n",
    "            \n",
    "        if self.gcn:\n",
    "            samp_neighs = [samp_neigh + set([nodes[i]]) for i, samp_neigh in enumerate(samp_neighs)]\n",
    "            \n",
    "        unique_nodes_list = list(set.union(*samp_neighs))\n",
    "        unique_nodes = {n: i for i, n in enumerate(unique_nodes_list)}\n",
    "        mask = Variable(torch.zeros(len(samp_neighs), len(unique_nodes)))\n",
    "        column_indices = [unique_nodes[n] for samp_neigh in samp_neighs for n in samp_neigh]   \n",
    "        row_indices = [i for i in range(len(samp_neighs)) for j in range(len(samp_neighs[i]))]\n",
    "        mask[row_indices, column_indices] = 1\n",
    "        \n",
    "        if self.cuda:\n",
    "            mask = mask.cuda()\n",
    "        num_neigh = mask.sum(1, keepdim=True)\n",
    "        mask = mask.div(num_neigh)\n",
    "        \n",
    "        if self.cuda:\n",
    "            embed_matrix = self.features(torch.LongTensor(unique_nodes_list).cuda())\n",
    "        else:\n",
    "            embed_matrix = self.features(torch.LongTensor(unique_nodes_list))\n",
    "        to_feats = mask.mm(embed_matrix)\n",
    "        \n",
    "        return to_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedGraphSage(nn.Module):\n",
    "    def __init__(self, num_classes, enc):\n",
    "        super(SupervisedGraphSage, self).__init__()\n",
    "        self.enc = enc\n",
    "        self.xent = nn.CrossEntropyLoss()\n",
    "        \n",
    "        self.weight = nn.Parameter(torch.FloatTensor(num_classes, enc.embed_dim))\n",
    "        init.xavier_uniform(self.weight)\n",
    "        \n",
    "    def forward(self, nodes):\n",
    "        embeds = self.enc(nodes)\n",
    "        scores = self.weight.mm(embeds)\n",
    "        return scores.t()\n",
    "    \n",
    "    def loss(self, nodes, labels):\n",
    "        scores = self.forward(nodes)\n",
    "        return self.xent(scores, labels.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cora():\n",
    "    np.random.seed(1)\n",
    "    random.seed(1)\n",
    "    num_nodes = 2708\n",
    "    feat_data, labels, adj_lists = load_cora()\n",
    "    features = nn.Embedding(2708, 1433)\n",
    "    features.weight = nn.Parameter(torch.FloatTensor(feat_data), requires_grad=False)  # weight参数\n",
    "    # feature.cuda()\n",
    "    \n",
    "    agg1 = MeanAggregator(features, cuda=True)\n",
    "    enc1 = Encoder(features, 1433, 128, adj_lists, agg1, gcn=True, cuda=False)\n",
    "    agg2 = MeanAggregator(lambda nodes : enc1(nodes).t(), cuda=False)\n",
    "    enc2 = Encoder(lambda nodes: enc1(nodes).t(), enc1.embed_dim, 128, adj_lists, agg2, base_model=enc1, gcn=True, cuda=False)\n",
    "    enc1.num_samples = 5\n",
    "    enc2.num_samples = 5\n",
    "    \n",
    "    graphsage = SupervisedGraphSage(7, enc2)\n",
    "    # graphsage.cuda()\n",
    "    rand_indices = np.random.permutation(num_nodes)\n",
    "    test = rand_indices[: 1000]\n",
    "    val = rand_indices[1000: 1500]\n",
    "    train = list(rand_indices[1500:])\n",
    "    \n",
    "    optimizer = torch.optim.SGD(filter(lambda p : p.requires_grad, graphsage.parameters()), lr=0.7)\n",
    "    \n",
    "    times = []\n",
    "    for batch in range(1, 101):\n",
    "        batch_nodes = train[: 256]\n",
    "        random.shuffle(train)\n",
    "        start_time = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        loss = graphsage.loss(batch_nodes, Variable(torch.LongTensor(labels[np.array(batch_nodes)])))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        end_time = time.time()\n",
    "        times.append(end_time - start_time)\n",
    "        if batch % 10 == 0:\n",
    "            print(batch, loss.item())\n",
    "    \n",
    "    val_output = graphsage.forward(val)\n",
    "    print(\"Validation F1:\", f1_score(labels[val], val_output.data.numpy().argmax(axis=1), average=\"micro\"))\n",
    "    print(\"Average batch time:\", np.mean(times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sss\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "c:\\users\\sss\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 1.618682622909546\n",
      "20 1.1853821277618408\n",
      "30 0.8934508562088013\n",
      "40 0.4756213426589966\n",
      "50 0.38028547167778015\n",
      "60 0.2593066394329071\n",
      "70 0.28790998458862305\n",
      "80 0.23371784389019012\n",
      "90 0.23076742887496948\n",
      "100 0.20929664373397827\n",
      "Validation F1: 0.868\n",
      "Average batch time: 0.06801058292388916\n"
     ]
    }
   ],
   "source": [
    "run_cora()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pubmed():\n",
    "    np.random.seed(1)\n",
    "    random.seed(1)\n",
    "    num_nodes = 19717\n",
    "    feat_data, labels, adj_lists = load_pubmed()\n",
    "    features = nn.Embedding(19717, 500)\n",
    "    features.weight = nn.Parameter(torch.FloatTensor(feat_data), requires_grad=False)\n",
    "    # features.cuda()\n",
    "    \n",
    "    agg1 = MeanAggregator(features, cuda=True)\n",
    "    enc1 = Encoder(features, 500, 128, adj_lists, agg1, gcn=True, cuda=False)\n",
    "    agg2 = MeanAggregator(lambda nodes : enc1(nodes).t(), cuda=False)\n",
    "    enc2 = Encoder(lambda nodes : enc1(nodes).t(), enc1.embed_dim, 128, adj_lists, agg2, base_model=enc1, gcn=True, cuda=False)\n",
    "    enc1.num_samples = 10\n",
    "    enc2.num_samples = 25\n",
    "    \n",
    "    graphsage = SupervisedGraphSage(3, enc2)\n",
    "    # graphsage.cuda()\n",
    "    rand_indices = np.random.permutation(num_nodes)\n",
    "    test = rand_indices[: 1000]\n",
    "    val = rand_indices[1000: 1500]\n",
    "    train = list(rand_indices[1500:])\n",
    "    \n",
    "    optimizer = torch.optim.SGD(filter(lambda p : p.requires_grad, graphsage.parameters()), lr=0.7)\n",
    "    times = []\n",
    "    for batch in range(1, 101):\n",
    "        batch_nodes = train[: 1024]\n",
    "        random.shuffle(train)\n",
    "        start_time = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        loss = graphsage.loss(batch_nodes, Variable(torch.LongTensor(labels[np.array(batch_nodes)])))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        end_time = time.time()\n",
    "        times.append(end_time - start_time)\n",
    "        if batch % 10 == 0:\n",
    "            print(batch, loss.item())\n",
    "            \n",
    "    val_output = graphsage.forward(val)\n",
    "    print(\"Validation F1:\", f1_score(labels[val], val_output.data.numpy().argmax(axis=1), average=\"micro\"))\n",
    "    print(\"Average batch time:\", np.mean(times))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sss\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "c:\\users\\sss\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 1.0965441465377808\n",
      "20 1.0941740274429321\n",
      "30 1.0901002883911133\n",
      "40 1.0866940021514893\n",
      "50 1.0807478427886963\n",
      "60 1.0695284605026245\n",
      "70 1.0590918064117432\n",
      "80 1.0319048166275024\n",
      "90 0.9996737837791443\n",
      "100 0.9550232887268066\n",
      "Validation F1: 0.514\n",
      "Average batch time: 0.45042718172073365\n"
     ]
    }
   ],
   "source": [
    "run_pubmed()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
