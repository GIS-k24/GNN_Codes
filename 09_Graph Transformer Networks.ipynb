{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math\n",
    "import pdb\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_positive(pred, target, num_classes):\n",
    "    out = []\n",
    "    for i in range(num_classes):\n",
    "        out.append(((pred == i) & (target == i)).sum())\n",
    "\n",
    "    return torch.tensor(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_negative(pred, target, num_classes):\n",
    "    out = []\n",
    "    for i in range(num_classes):\n",
    "        out.append(((pred != i) & (target != i)).sum())\n",
    "\n",
    "    return torch.tensor(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def false_positive(pred, target, num_classes):\n",
    "    out = []\n",
    "    for i in range(num_classes):\n",
    "        out.append(((pred == i) & (target != i)).sum())\n",
    "\n",
    "    return torch.tensor(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def false_negative(pred, target, num_classes):\n",
    "    out = []\n",
    "    for i in range(num_classes):\n",
    "        out.append(((pred != i) & (target == i)).sum())\n",
    "\n",
    "    return torch.tensor(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(pred, target, num_classes):\n",
    "    tp = true_positive(pred, target, num_classes).to(torch.float)\n",
    "    fp = false_positive(pred, target, num_classes).to(torch.float)\n",
    "\n",
    "    out = tp / (tp + fp)\n",
    "    out[torch.isnan(out)] = 0\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(pred, target, num_classes):\n",
    "    tp = true_positive(pred, target, num_classes).to(torch.float)\n",
    "    fn = false_negative(pred, target, num_classes).to(torch.float)\n",
    "\n",
    "    out = tp / (tp + fn)\n",
    "    out[torch.isnan(out)] = 0\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_score(pred, target, num_classes):\n",
    "    prec = precision(pred, target, num_classes)\n",
    "    rec = recall(pred, target, num_classes)\n",
    "\n",
    "    score = 2 * (prec * rec) / (prec + rec)\n",
    "    score[torch.isnan(score)] = 0\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset', type=str, help='Dataset')\n",
    "parser.add_argument('--epoch', type=int, default=40, help='Training Epochs')\n",
    "parser.add_argument('--node_dim', type=int, default=64, help='Node dimension')\n",
    "parser.add_argument('--num_channels', type=int, default=2, help='number of channels')\n",
    "parser.add_argument('--lr', type=float, default=0.005, help='learning rate')\n",
    "parser.add_argument('--weight_decay', type=float, default=0.001, help='l2 reg')\n",
    "parser.add_argument('--num_layers', type=int, default=2, help='number of layer')\n",
    "parser.add_argument('--norm', type=str, default='true', help='normalization')\n",
    "parser.add_argument('--adaptive_lr', type=str, default='false', help='adaptive learning rate')\n",
    "\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.dataset = 'ACM'\n",
    "args.num_layers = 2\n",
    "args.adaptive_lr = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(adaptive_lr=True, dataset='ACM', epoch=40, lr=0.005, node_dim=64, norm='true', num_channels=2, num_layers=2, weight_decay=0.001)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = args.epoch\n",
    "node_dim = args.node_dim\n",
    "num_channels = args.num_channels\n",
    "lr = args.lr\n",
    "weight_decay = args.weight_decay\n",
    "num_layers = args.num_layers\n",
    "norm = args.norm\n",
    "adaptive_lr = args.adaptive_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查看数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 节点特征\n",
    "with open(r\"C:\\Users\\sss\\Desktop\\Graph_Transformer_Networks\\data/\" + args.dataset + \"/node_features.pkl\", \"rb\") as f:\n",
    "    node_features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 链关系\n",
    "with open(r\"C:\\Users\\sss\\Desktop\\Graph_Transformer_Networks\\data/\" + args.dataset + \"/edges.pkl\", \"rb\") as f:\n",
    "    edges = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 节点标签\n",
    "with open(r\"C:\\Users\\sss\\Desktop\\Graph_Transformer_Networks\\data/\" + args.dataset + \"/labels.pkl\", \"rb\") as f:\n",
    "    labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(8994, 1902), 4, 3]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[node_features.shape, len(edges), len(labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = edges[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将edges组合成矩阵\n",
    "for i, edge in enumerate(edges):\n",
    "    if i == 0:\n",
    "        A = torch.from_numpy(edge.todense()).type(torch.FloatTensor).unsqueeze(-1)\n",
    "    else:\n",
    "        A = torch.cat([A, torch.from_numpy(edge.todense()).type(torch.FloatTensor).unsqueeze(-1)], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.cat([A, torch.eye(num_nodes).type(torch.FloatTensor).unsqueeze(-1)], dim=-1)  # 添加一个单位对角阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features = torch.from_numpy(node_features).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_node = torch.from_numpy(np.array(labels[0])[:,0]).type(torch.LongTensor)  # train\n",
    "train_target = torch.from_numpy(np.array(labels[0])[:,1]).type(torch.LongTensor)  \n",
    "valid_node = torch.from_numpy(np.array(labels[1])[:,0]).type(torch.LongTensor)  # val\n",
    "valid_target = torch.from_numpy(np.array(labels[1])[:,1]).type(torch.LongTensor)\n",
    "test_node = torch.from_numpy(np.array(labels[2])[:,0]).type(torch.LongTensor)  # test\n",
    "test_target = torch.from_numpy(np.array(labels[2])[:,1]).type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(600, 2), (300, 2), (2125, 2)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train | valid | test\n",
    "[labels[0].shape, labels[1].shape, labels[2].shape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([8994, 1902]),\n",
       " torch.Size([600]),\n",
       " torch.Size([600]),\n",
       " torch.Size([300]),\n",
       " torch.Size([300]),\n",
       " torch.Size([2125]),\n",
       " torch.Size([2125])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[node_features.shape, train_node.shape, train_target.shape, valid_node.shape, valid_target.shape, test_node.shape, test_target.shape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = torch.max(train_target).item() + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GTConv, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_channels, in_channels, 1, 1))\n",
    "        self.bias = None\n",
    "        self.scale = nn.Parameter(torch.Tensor([0, 1]), requires_grad=False)\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        n = self.in_channels\n",
    "        nn.init.constant_(self.weight, 0.1)\n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "            \n",
    "    def forward(self, A):\n",
    "        '''\n",
    "        0) 对weight(conv)进行softmax\n",
    "        1) 对每个节点在每个edgeType上进行[2, 5, 1, 1]的卷积操作;\n",
    "        2) 对每个edgeType进行加权求和，加权是通过0)softmax\n",
    "        '''\n",
    "        # F.softmax(self.weight, dim=1) 对self.weight做softmax：[2, 5, 1, 1]\n",
    "        # A：[1, 5, 8994, 8994]：带有edgeType的邻接矩阵\n",
    "        # [1, 5, 8994, 8994] * [2, 5, 1, 1] => [2, 5, 8994, 8994]\n",
    "        # sum：[2, 8994, 8994]\n",
    "        A = torch.sum(A * F.softmax(self.weight, dim=1), dim=1)\n",
    "        return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTLayer(nn.Module):\n",
    "    # 不同edge类型的组合\n",
    "    def __init__(self, in_channels, out_channels, first=True):\n",
    "        super(GTLayer, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.first = first\n",
    "        if self.first == True:\n",
    "            self.conv1 = GTConv(in_channels, out_channels)  # W1\n",
    "            self.conv2 = GTConv(in_channels, out_channels)  # W2\n",
    "        else:\n",
    "            self.conv1 = GTConv(in_channels, out_channels)  \n",
    "    \n",
    "    def forward(self, A, H_=None):  # A:[1,edgeType,N,N]\n",
    "        if self.first == True:\n",
    "            a = self.conv1(A)  # GTConv=>[2, N, N] #Q1\n",
    "            b = self.conv2(A)  # Q2\n",
    "            # 第一次矩阵相乘，得到A1\n",
    "            H = torch.bmm(a, b)  # torch.bmm(a,b),tensor a 的size为(b,h,w),tensor b的size为(b,w,h),注意两个tensor的维度必须为3;\n",
    "            W = [(F.softmax(self.conv1.weight, dim=1)).detach(),(F.softmax(self.conv2.weight, dim=1)).detach()]  # conv-softmax: 是为了下一次直接使用\n",
    "        else:\n",
    "            a = self.conv1(A)  # 第二层只有一个conv1; output:Conv输出归一化edge后的结果\n",
    "            H = torch.bmm(H_,a)  # H_上一层的输出矩阵A1; 输出这一层后的结果A2;\n",
    "            W = [(F.softmax(self.conv1.weight, dim=1)).detach()]\n",
    "        return H, W   # H = A(1) ... A(l); W = 归一化后的权重矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GTN(nn.Module):\n",
    "    def __init__(self, num_edge, num_channels, w_in, w_out, num_class,num_layers,norm):\n",
    "        super(GTN, self).__init__()\n",
    "        self.num_edge = num_edge\n",
    "        self.num_channels = num_channels\n",
    "        self.w_in = w_in\n",
    "        self.w_out = w_out\n",
    "        self.num_class = num_class\n",
    "        self.num_layers = num_layers\n",
    "        self.is_norm = norm\n",
    "        layers = []\n",
    "        for i in range(num_layers):  # layers多个GTLayer组成的; 多头channels\n",
    "            if i == 0:\n",
    "                layers.append(GTLayer(num_edge, num_channels, first=True))  # 第一个GT层,edge类别构建的矩阵\n",
    "            else:\n",
    "                layers.append(GTLayer(num_edge, num_channels, first=False))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.weight = nn.Parameter(torch.Tensor(w_in, w_out))  # GCN\n",
    "        self.bias = nn.Parameter(torch.Tensor(w_out))\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.linear1 = nn.Linear(self.w_out * self.num_channels, self.w_out)\n",
    "        self.linear2 = nn.Linear(self.w_out, self.num_class)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        nn.init.zeros_(self.bias)\n",
    "\n",
    "    def gcn_conv(self,X,H):  # 自己写了一个GCN\n",
    "        X = torch.mm(X, self.weight)  # X-features; self.weight-weight\n",
    "        H = self.norm(H, add=True)  # H-第i个channel下邻接矩阵;\n",
    "        return torch.mm(H.t(),X)\n",
    "\n",
    "    def normalization(self, H):\n",
    "        for i in range(self.num_channels):\n",
    "            if i==0:\n",
    "                H_ = self.norm(H[i,:,:]).unsqueeze(0)  # Q1\n",
    "            else:\n",
    "                H_ = torch.cat((H_,self.norm(H[i,:,:]).unsqueeze(0)), dim=0)  # Q2\n",
    "        return H_\n",
    "\n",
    "    def norm(self, H, add=False):\n",
    "        H = H.t()   # t\n",
    "        if add == False:\n",
    "            H = H * ((torch.eye(H.shape[0])==0).type(torch.FloatTensor))  # 建立一个对角阵; 除了自身节点，对应位置相乘。Degree(排除本身)\n",
    "        else:\n",
    "            H = H * ((torch.eye(H.shape[0])==0).type(torch.FloatTensor)) + torch.eye(H.shape[0]).type(torch.FloatTensor)\n",
    "        deg = torch.sum(H, dim=1)  # 按行求和, 即每个节点的dgree的和\n",
    "        deg_inv = deg.pow(-1)  # deg-1 归一化操作\n",
    "        deg_inv[deg_inv == float('inf')] = 0\n",
    "        deg_inv = deg_inv * torch.eye(H.shape[0]).type(torch.FloatTensor)  # 转换成n*n的矩阵\n",
    "        H = torch.mm(deg_inv,H)  # 矩阵内积\n",
    "        H = H.t()\n",
    "        return H\n",
    "\n",
    "    def forward(self, A, X, target_x, target):\n",
    "        A = A.unsqueeze(0).permute(0,3,1,2)   # A.unsqueeze(0)=[1,N,N,edgeType]=>[1,edgeType,N,N]; 卷积输出的channel数量\n",
    "        Ws = []\n",
    "        for i in range(self.num_layers):  # 两层GTLayer:{edgeType}\n",
    "            if i == 0:\n",
    "                H, W = self.layers[i](A)  # GTN0:两层GTConv; A:edgeType的邻接矩阵; output: H(A(l)), W:归一化的Conv\n",
    "            else:\n",
    "                H = self.normalization(H)   # Conv矩阵，D-1*A的操作\n",
    "                H, W = self.layers[i](A, H)  # 第一层计算完了A(原始矩阵), H(上一次计算后的A(l)); output: A2, W(第二层Conv1)\n",
    "            Ws.append(W)\n",
    "        \n",
    "        #H,W1 = self.layer1(A)\n",
    "        #H = self.normalization(H)\n",
    "        #H,W2 = self.layer2(A, H)\n",
    "        #H = self.normalization(H)\n",
    "        #H,W3 = self.layer3(A, H)\n",
    "        for i in range(self.num_channels):   # conv的channel数量\n",
    "            if i == 0:\n",
    "                X_ = F.relu(self.gcn_conv(X,H[i]))  # X-features; H[i]-第i个channel输出的邻接矩阵Al[i]; gcn_conv:Linear\n",
    "            else:\n",
    "                X_tmp = F.relu(self.gcn_conv(X,H[i]))\n",
    "                X_ = torch.cat((X_,X_tmp), dim=1)  # X_拼接之后输出\n",
    "        X_ = self.linear1(X_)\n",
    "        X_ = F.relu(X_)\n",
    "        y = self.linear2(X_[target_x])\n",
    "        loss = self.loss(y, target)\n",
    "        return loss, y, Ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GTN(\n",
    "    num_edge = A.shape[-1],  # edge类别的数量; 还有一个单位阵;\n",
    "    num_channels = num_channels,\n",
    "    w_in = node_features.shape[1],\n",
    "    w_out = node_dim,\n",
    "    num_class = num_classes,\n",
    "    num_layers = num_layers,  # GTLayer\n",
    "    norm = norm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GTN(\n",
       "  (layers): ModuleList(\n",
       "    (0): GTLayer(\n",
       "      (conv1): GTConv()\n",
       "      (conv2): GTConv()\n",
       "    )\n",
       "    (1): GTLayer(\n",
       "      (conv1): GTConv()\n",
       "    )\n",
       "  )\n",
       "  (loss): CrossEntropyLoss()\n",
       "  (linear1): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (linear2): Linear(in_features=64, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if adaptive_lr == \"false\":\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=0.001)\n",
    "else:\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {\"params\": model.weight},\n",
    "        {\"params\": model.linear1.parameters()},\n",
    "        {\"params\": model.linear2.parameters()},\n",
    "        {\"params\": model.layers.parameters(), \"lr\":0.5}\n",
    "    ], lr=0.005, weight_decay=0.001)\n",
    "    \n",
    "loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_f1 = 0\n",
    "\n",
    "# Train & Valid & Test\n",
    "best_val_loss = 10000\n",
    "best_test_loss = 10000\n",
    "best_train_loss = 10000\n",
    "best_train_f1 = 0\n",
    "best_val_f1 = 0\n",
    "best_test_f1 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epochs):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        if param_group['lr'] > 0.005:\n",
    "            param_group['lr'] = param_group['lr'] * 0.9\n",
    "            \n",
    "    print('Epoch:  ',i + 1)\n",
    "    model.zero_grad()\n",
    "    model.train()  # A:[8994, 8994, 5],5个edgeType; node_features;\n",
    "    \n",
    "    loss, y_train,Ws = model(A, node_features, train_node, train_target)\n",
    "    train_f1 = torch.mean(f1_score(torch.argmax(y_train.detach(), dim=1), train_target, num_classes=num_classes)).cpu().numpy()\n",
    "    print('Train - Loss: {}, Macro_F1: {}'.format(loss.detach().cpu().numpy(), train_f1))\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    # Valid\n",
    "    with torch.no_grad():\n",
    "        val_loss, y_valid, _ = model.forward(A, node_features, valid_node, valid_target)\n",
    "        val_f1 = torch.mean(f1_score(torch.argmax(y_valid, dim=1), valid_target, num_classes=num_classes)).cpu().numpy()\n",
    "        print('Valid - Loss: {}, Macro_F1: {}'.format(val_loss.detach().cpu().numpy(), val_f1))\n",
    "        \n",
    "        test_loss, y_test, W = model.forward(A, node_features, test_node, test_target)\n",
    "        test_f1 = torch.mean(f1_score(torch.argmax(y_test, dim=1), test_target, num_classes=num_classes)).cpu().numpy()\n",
    "        print('Test - Loss: {}, Macro_F1: {}\\n'.format(test_loss.detach().cpu().numpy(), test_f1))\n",
    "        \n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_loss = val_loss.detach().cpu().numpy()\n",
    "        best_test_loss = test_loss.detach().cpu().numpy()\n",
    "        best_train_loss = loss.detach().cpu().numpy()\n",
    "        best_train_f1 = train_f1\n",
    "        best_val_f1 = val_f1\n",
    "        best_test_f1 = test_f1 \n",
    "        \n",
    "print('---------------Best Results--------------------')\n",
    "print('Train - Loss: {}, Macro_F1: {}'.format(best_train_loss, best_train_f1))\n",
    "print('Valid - Loss: {}, Macro_F1: {}'.format(best_val_loss, best_val_f1))\n",
    "print('Test - Loss: {}, Macro_F1: {}'.format(best_test_loss, best_test_f1))\n",
    "final_f1 += best_test_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\CPUAllocator.cpp:75] data. DefaultCPUAllocator: not enough memory: you tried to allocate 3235681440 bytes. Buy new RAM!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-7cd57eb757b3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_node\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\sss\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-27-be601adc8006>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, A, X, target_x, target)\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                 \u001b[0mH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mH\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# Conv矩阵，D-1*A的操作\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m                 \u001b[0mH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mH\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 第一层计算完了A(原始矩阵), H(上一次计算后的A(l)); output: A2, W(第二层Conv1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m             \u001b[0mWs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sss\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-49d1c2bd27ea>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, A, H_)\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0mW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# conv-softmax: 是为了下一次直接使用\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m             \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 第二层只有一个conv1; output:Conv输出归一化edge后的结果\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m             \u001b[0mH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mH_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# H_上一层的输出矩阵A1; 输出这一层后的结果A2;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sss\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-c1858b1ea583>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, A)\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;31m# [1, 5, 8994, 8994] * [2, 5, 1, 1] => [2, 5, 8994, 8994]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[1;31m# sum：[2, 8994, 8994]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m         \u001b[0mA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\CPUAllocator.cpp:75] data. DefaultCPUAllocator: not enough memory: you tried to allocate 3235681440 bytes. Buy new RAM!"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
