{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GCN：从图数据中提取特征的方法 -- 带分类操作\n",
    "- 从而可以使用这些特征去对图数据进行\n",
    "    - 节点分类（node classification）\n",
    "    - 图分类（graph classification）\n",
    "    - 边预测（link prediction）\n",
    "    - 图的嵌入表示（graph embedding）\n",
    "- 结合应用点特征、点连接关系、点属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))  #  矩阵行求和\n",
    "    r_inv = np.power(rowsum, -1).flatten()  # 求和的-1次方\n",
    "    r_inv[np.isinf(r_inv)] = 0.   # 如果是inf，转换成0\n",
    "    r_mat_inv = sp.diags(r_inv)  # 构造对角戏矩阵\n",
    "    mx = r_mat_inv.dot(mx)  # 构造D-1*A，非对称方式，简化方式\n",
    "    return mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cora Dataset\n",
    "    该数据集共2708个样本点，每个样本点都是一篇科学论文，所有样本点被分为7个类别，类别分别是：\n",
    "        1）基于案例；2）遗传算法；3）神经网络；4）概率方法；5）强化学习；6）规则学习；7）理论\n",
    "\n",
    "    每篇论文都由一个1433维的词向量表示，即有1433个特征。词向量的每个元素都对应一个词，且该元素只有0或1两个取值。\n",
    "\n",
    "    每篇论文都至少引用了一篇其他论文，或者被其他论文引用，也就是样本点之间存在联系，如果将样本点看做图中的点，则这是一个连通的图。\n",
    "       \n",
    "    cora.content：每一行由三部分组成，分别是论文的编号 | 论文的词向量 | 论文的类别\n",
    "    \n",
    "    cora.cites：每一行有两个论文编号，第一个是被引用论文，第二个是论文引用前一篇论文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path=r\"C:\\Users\\sss\\Desktop\\cora/\", dataset=\"cora\"):\n",
    "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
    "                                        dtype=np.dtype(str))\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)  # 取特征feature\n",
    "    labels = encode_onehot(idx_features_labels[:, -1])  # one-hot label\n",
    "\n",
    "    # build graph\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)  # 节点\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}   # 构建节点的索引字典\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),  # 导入edge的数据\n",
    "                                    dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)    # 将之前的转换成字典编号后的边\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),  # 构建边的邻接矩阵\n",
    "                        shape=(labels.shape[0], labels.shape[0]),\n",
    "                        dtype=np.float32)\n",
    "\n",
    "    # build symmetric adjacency matrix，计算转置矩阵。将有向图转成无向图\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    features = normalize(features)   # 对特征做了归一化的操作\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))   # 对A+I归一化\n",
    "    \n",
    "    # 训练，验证，测试的样本\n",
    "    idx_train = range(140)\n",
    "    idx_val = range(200, 500)\n",
    "    idx_test = range(500, 1500)\n",
    "    # 将numpy的数据转换成torch格式\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    }
   ],
   "source": [
    "adj, features, labels, idx_train, idx_val, idx_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 文章的矩阵关系 \t # 特征 \t # 标签\n",
      "torch.Size([2708, 2708]) \t torch.Size([2708, 1433]) \t torch.Size([2708])\n"
     ]
    }
   ],
   "source": [
    "print(\"# 文章的矩阵关系 \\t # 特征 \\t # 标签\")\n",
    "print(adj.shape, \"\\t\", features.shape, \"\\t\", labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))  # input_features, out_features\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    # 标准化\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)  # 随机化参数\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)  # GraphConvolution forward。input*weight\n",
    "        output = torch.spmm(adj, support)  # 稀疏矩阵的相乘，和mm一样的效果\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid) # 构建第一层 GCN\n",
    "        self.gc2 = GraphConvolution(nhid, nclass) # 构建第二层 GCN\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = 16\n",
    "dropout = 0.5\n",
    "\n",
    "model = GCN(\n",
    "    nfeat=features.shape[1],\n",
    "    nhid=hidden,\n",
    "    nclass=labels.max().item() + 1,\n",
    "    dropout=dropout\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GCN(\n",
       "  (gc1): GraphConvolution (1433 -> 16)\n",
       "  (gc2): GraphConvolution (16 -> 7)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "cuda = not False and torch.cuda.is_available()\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cuda:\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    adj = adj.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练和测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad() # GraphConvolution forward\n",
    "    # 重点输出 ！！！\n",
    "    output = model(features, adj)   # 运行模型，输入参数 (features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if not False:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        model.eval()\n",
    "        output = model(features, adj)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 1.9502 acc_train: 0.1500 loss_val: 1.9400 acc_val: 0.1267 time: 9.7513s\n",
      "Epoch: 0002 loss_train: 1.9274 acc_train: 0.1500 loss_val: 1.9243 acc_val: 0.1267 time: 0.0760s\n",
      "Epoch: 0003 loss_train: 1.9181 acc_train: 0.1500 loss_val: 1.9098 acc_val: 0.1267 time: 0.0389s\n",
      "Epoch: 0004 loss_train: 1.9130 acc_train: 0.1643 loss_val: 1.8963 acc_val: 0.1267 time: 0.0429s\n",
      "Epoch: 0005 loss_train: 1.8930 acc_train: 0.1786 loss_val: 1.8834 acc_val: 0.1267 time: 0.2114s\n",
      "Epoch: 0006 loss_train: 1.8848 acc_train: 0.1929 loss_val: 1.8712 acc_val: 0.1900 time: 0.0549s\n",
      "Epoch: 0007 loss_train: 1.8662 acc_train: 0.2786 loss_val: 1.8595 acc_val: 0.4400 time: 0.0443s\n",
      "Epoch: 0008 loss_train: 1.8598 acc_train: 0.3143 loss_val: 1.8483 acc_val: 0.3533 time: 0.0472s\n",
      "Epoch: 0009 loss_train: 1.8493 acc_train: 0.3071 loss_val: 1.8375 acc_val: 0.3467 time: 0.0310s\n",
      "Epoch: 0010 loss_train: 1.8275 acc_train: 0.3214 loss_val: 1.8268 acc_val: 0.3500 time: 0.0362s\n",
      "Epoch: 0011 loss_train: 1.8331 acc_train: 0.2857 loss_val: 1.8164 acc_val: 0.3500 time: 0.0550s\n",
      "Epoch: 0012 loss_train: 1.8086 acc_train: 0.3000 loss_val: 1.8061 acc_val: 0.3500 time: 0.0314s\n",
      "Epoch: 0013 loss_train: 1.8064 acc_train: 0.2714 loss_val: 1.7961 acc_val: 0.3500 time: 0.0346s\n",
      "Epoch: 0014 loss_train: 1.7995 acc_train: 0.2857 loss_val: 1.7864 acc_val: 0.3500 time: 0.0267s\n",
      "Epoch: 0015 loss_train: 1.7891 acc_train: 0.2786 loss_val: 1.7772 acc_val: 0.3500 time: 0.0419s\n",
      "Epoch: 0016 loss_train: 1.7717 acc_train: 0.3071 loss_val: 1.7682 acc_val: 0.3500 time: 0.0368s\n",
      "Epoch: 0017 loss_train: 1.7793 acc_train: 0.3000 loss_val: 1.7598 acc_val: 0.3500 time: 0.0339s\n",
      "Epoch: 0018 loss_train: 1.7418 acc_train: 0.2929 loss_val: 1.7517 acc_val: 0.3500 time: 0.0439s\n",
      "Epoch: 0019 loss_train: 1.7472 acc_train: 0.2929 loss_val: 1.7440 acc_val: 0.3500 time: 0.0451s\n",
      "Epoch: 0020 loss_train: 1.7437 acc_train: 0.2929 loss_val: 1.7368 acc_val: 0.3500 time: 0.0350s\n",
      "Epoch: 0021 loss_train: 1.7610 acc_train: 0.3000 loss_val: 1.7299 acc_val: 0.3500 time: 0.0485s\n",
      "Epoch: 0022 loss_train: 1.7295 acc_train: 0.3143 loss_val: 1.7231 acc_val: 0.3500 time: 0.0642s\n",
      "Epoch: 0023 loss_train: 1.7070 acc_train: 0.3000 loss_val: 1.7164 acc_val: 0.3500 time: 0.0658s\n",
      "Epoch: 0024 loss_train: 1.6955 acc_train: 0.2929 loss_val: 1.7098 acc_val: 0.3500 time: 0.0469s\n",
      "Epoch: 0025 loss_train: 1.7041 acc_train: 0.2929 loss_val: 1.7032 acc_val: 0.3500 time: 0.0329s\n",
      "Epoch: 0026 loss_train: 1.6974 acc_train: 0.2857 loss_val: 1.6966 acc_val: 0.3500 time: 0.0333s\n",
      "Epoch: 0027 loss_train: 1.6652 acc_train: 0.3071 loss_val: 1.6899 acc_val: 0.3500 time: 0.0322s\n",
      "Epoch: 0028 loss_train: 1.6543 acc_train: 0.3000 loss_val: 1.6832 acc_val: 0.3500 time: 0.0429s\n",
      "Epoch: 0029 loss_train: 1.6536 acc_train: 0.3071 loss_val: 1.6764 acc_val: 0.3500 time: 0.0302s\n",
      "Epoch: 0030 loss_train: 1.6508 acc_train: 0.2929 loss_val: 1.6695 acc_val: 0.3500 time: 0.0253s\n",
      "Epoch: 0031 loss_train: 1.6400 acc_train: 0.3143 loss_val: 1.6623 acc_val: 0.3467 time: 0.0427s\n",
      "Epoch: 0032 loss_train: 1.6054 acc_train: 0.3071 loss_val: 1.6550 acc_val: 0.3467 time: 0.0282s\n",
      "Epoch: 0033 loss_train: 1.6408 acc_train: 0.3429 loss_val: 1.6473 acc_val: 0.3467 time: 0.0240s\n",
      "Epoch: 0034 loss_train: 1.6123 acc_train: 0.3357 loss_val: 1.6395 acc_val: 0.3500 time: 0.0249s\n",
      "Epoch: 0035 loss_train: 1.5728 acc_train: 0.3643 loss_val: 1.6313 acc_val: 0.3567 time: 0.0294s\n",
      "Epoch: 0036 loss_train: 1.5787 acc_train: 0.3929 loss_val: 1.6228 acc_val: 0.3733 time: 0.0339s\n",
      "Epoch: 0037 loss_train: 1.5814 acc_train: 0.3929 loss_val: 1.6141 acc_val: 0.3833 time: 0.0260s\n",
      "Epoch: 0038 loss_train: 1.5491 acc_train: 0.4286 loss_val: 1.6048 acc_val: 0.3867 time: 0.0585s\n",
      "Epoch: 0039 loss_train: 1.5217 acc_train: 0.4357 loss_val: 1.5951 acc_val: 0.3933 time: 0.0463s\n",
      "Epoch: 0040 loss_train: 1.5098 acc_train: 0.4143 loss_val: 1.5851 acc_val: 0.4067 time: 0.0738s\n",
      "Epoch: 0041 loss_train: 1.4998 acc_train: 0.4214 loss_val: 1.5749 acc_val: 0.4200 time: 0.0432s\n",
      "Epoch: 0042 loss_train: 1.4976 acc_train: 0.5071 loss_val: 1.5644 acc_val: 0.4300 time: 0.0478s\n",
      "Epoch: 0043 loss_train: 1.4725 acc_train: 0.4643 loss_val: 1.5537 acc_val: 0.4367 time: 0.0657s\n",
      "Epoch: 0044 loss_train: 1.5209 acc_train: 0.4643 loss_val: 1.5429 acc_val: 0.4367 time: 0.0601s\n",
      "Epoch: 0045 loss_train: 1.4731 acc_train: 0.4786 loss_val: 1.5319 acc_val: 0.4467 time: 0.0308s\n",
      "Epoch: 0046 loss_train: 1.4452 acc_train: 0.5000 loss_val: 1.5207 acc_val: 0.4567 time: 0.0229s\n",
      "Epoch: 0047 loss_train: 1.4205 acc_train: 0.4786 loss_val: 1.5095 acc_val: 0.4567 time: 0.0269s\n",
      "Epoch: 0048 loss_train: 1.4093 acc_train: 0.5500 loss_val: 1.4983 acc_val: 0.4700 time: 0.0400s\n",
      "Epoch: 0049 loss_train: 1.3857 acc_train: 0.5500 loss_val: 1.4871 acc_val: 0.4900 time: 0.0299s\n",
      "Epoch: 0050 loss_train: 1.3985 acc_train: 0.5286 loss_val: 1.4757 acc_val: 0.5067 time: 0.0568s\n",
      "Epoch: 0051 loss_train: 1.3582 acc_train: 0.5643 loss_val: 1.4639 acc_val: 0.5133 time: 0.0788s\n",
      "Epoch: 0052 loss_train: 1.3628 acc_train: 0.5071 loss_val: 1.4522 acc_val: 0.5267 time: 0.0653s\n",
      "Epoch: 0053 loss_train: 1.3284 acc_train: 0.5714 loss_val: 1.4406 acc_val: 0.5300 time: 0.0813s\n",
      "Epoch: 0054 loss_train: 1.3162 acc_train: 0.5643 loss_val: 1.4287 acc_val: 0.5300 time: 0.0655s\n",
      "Epoch: 0055 loss_train: 1.3017 acc_train: 0.5643 loss_val: 1.4170 acc_val: 0.5333 time: 0.0359s\n",
      "Epoch: 0056 loss_train: 1.2855 acc_train: 0.6214 loss_val: 1.4050 acc_val: 0.5433 time: 0.0289s\n",
      "Epoch: 0057 loss_train: 1.2567 acc_train: 0.6429 loss_val: 1.3929 acc_val: 0.5433 time: 0.0329s\n",
      "Epoch: 0058 loss_train: 1.2502 acc_train: 0.6071 loss_val: 1.3811 acc_val: 0.5500 time: 0.0540s\n",
      "Epoch: 0059 loss_train: 1.2553 acc_train: 0.6143 loss_val: 1.3694 acc_val: 0.5567 time: 0.0504s\n",
      "Epoch: 0060 loss_train: 1.2287 acc_train: 0.5929 loss_val: 1.3579 acc_val: 0.5700 time: 0.0369s\n",
      "Epoch: 0061 loss_train: 1.1915 acc_train: 0.6643 loss_val: 1.3465 acc_val: 0.6067 time: 0.0319s\n",
      "Epoch: 0062 loss_train: 1.2038 acc_train: 0.6643 loss_val: 1.3355 acc_val: 0.6333 time: 0.0300s\n",
      "Epoch: 0063 loss_train: 1.1688 acc_train: 0.6786 loss_val: 1.3243 acc_val: 0.6500 time: 0.0329s\n",
      "Epoch: 0064 loss_train: 1.1981 acc_train: 0.6929 loss_val: 1.3129 acc_val: 0.6500 time: 0.0373s\n",
      "Epoch: 0065 loss_train: 1.1708 acc_train: 0.6714 loss_val: 1.3015 acc_val: 0.6600 time: 0.0466s\n",
      "Epoch: 0066 loss_train: 1.1233 acc_train: 0.7643 loss_val: 1.2899 acc_val: 0.6733 time: 0.0517s\n",
      "Epoch: 0067 loss_train: 1.1220 acc_train: 0.7071 loss_val: 1.2781 acc_val: 0.6867 time: 0.0388s\n",
      "Epoch: 0068 loss_train: 1.1048 acc_train: 0.7286 loss_val: 1.2666 acc_val: 0.6967 time: 0.0481s\n",
      "Epoch: 0069 loss_train: 1.0965 acc_train: 0.7714 loss_val: 1.2548 acc_val: 0.7000 time: 0.0519s\n",
      "Epoch: 0070 loss_train: 1.0605 acc_train: 0.7714 loss_val: 1.2428 acc_val: 0.7067 time: 0.0489s\n",
      "Epoch: 0071 loss_train: 1.0324 acc_train: 0.7643 loss_val: 1.2309 acc_val: 0.7100 time: 0.0450s\n",
      "Epoch: 0072 loss_train: 1.0473 acc_train: 0.7571 loss_val: 1.2185 acc_val: 0.7067 time: 0.0379s\n",
      "Epoch: 0073 loss_train: 1.0481 acc_train: 0.7571 loss_val: 1.2066 acc_val: 0.7100 time: 0.0332s\n",
      "Epoch: 0074 loss_train: 1.0521 acc_train: 0.7500 loss_val: 1.1952 acc_val: 0.7167 time: 0.0259s\n",
      "Epoch: 0075 loss_train: 1.0229 acc_train: 0.7357 loss_val: 1.1845 acc_val: 0.7267 time: 0.0329s\n",
      "Epoch: 0076 loss_train: 1.0151 acc_train: 0.7571 loss_val: 1.1745 acc_val: 0.7333 time: 0.0339s\n",
      "Epoch: 0077 loss_train: 0.9912 acc_train: 0.7714 loss_val: 1.1646 acc_val: 0.7400 time: 0.0289s\n",
      "Epoch: 0078 loss_train: 0.9861 acc_train: 0.8000 loss_val: 1.1550 acc_val: 0.7500 time: 0.0259s\n",
      "Epoch: 0079 loss_train: 0.9805 acc_train: 0.8357 loss_val: 1.1451 acc_val: 0.7667 time: 0.0299s\n",
      "Epoch: 0080 loss_train: 0.9313 acc_train: 0.8214 loss_val: 1.1341 acc_val: 0.7700 time: 0.0242s\n",
      "Epoch: 0081 loss_train: 0.9028 acc_train: 0.8500 loss_val: 1.1228 acc_val: 0.7700 time: 0.0239s\n",
      "Epoch: 0082 loss_train: 0.9540 acc_train: 0.7643 loss_val: 1.1107 acc_val: 0.7700 time: 0.0239s\n",
      "Epoch: 0083 loss_train: 0.9177 acc_train: 0.8214 loss_val: 1.0986 acc_val: 0.7700 time: 0.0339s\n",
      "Epoch: 0084 loss_train: 0.9193 acc_train: 0.8357 loss_val: 1.0871 acc_val: 0.7667 time: 0.0638s\n",
      "Epoch: 0085 loss_train: 0.8844 acc_train: 0.8429 loss_val: 1.0762 acc_val: 0.7700 time: 0.0432s\n",
      "Epoch: 0086 loss_train: 0.9078 acc_train: 0.7929 loss_val: 1.0659 acc_val: 0.7733 time: 0.0241s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0087 loss_train: 0.8869 acc_train: 0.8000 loss_val: 1.0562 acc_val: 0.7733 time: 0.0278s\n",
      "Epoch: 0088 loss_train: 0.9004 acc_train: 0.8286 loss_val: 1.0468 acc_val: 0.7767 time: 0.0423s\n",
      "Epoch: 0089 loss_train: 0.8835 acc_train: 0.7786 loss_val: 1.0375 acc_val: 0.7767 time: 0.0395s\n",
      "Epoch: 0090 loss_train: 0.8904 acc_train: 0.7714 loss_val: 1.0285 acc_val: 0.7767 time: 0.0264s\n",
      "Epoch: 0091 loss_train: 0.8940 acc_train: 0.8000 loss_val: 1.0201 acc_val: 0.7800 time: 0.0258s\n",
      "Epoch: 0092 loss_train: 0.8185 acc_train: 0.8500 loss_val: 1.0123 acc_val: 0.7833 time: 0.0259s\n",
      "Epoch: 0093 loss_train: 0.8215 acc_train: 0.8714 loss_val: 1.0041 acc_val: 0.7833 time: 0.0507s\n",
      "Epoch: 0094 loss_train: 0.8107 acc_train: 0.8429 loss_val: 0.9962 acc_val: 0.7833 time: 0.0388s\n",
      "Epoch: 0095 loss_train: 0.7727 acc_train: 0.8357 loss_val: 0.9885 acc_val: 0.7833 time: 0.0302s\n",
      "Epoch: 0096 loss_train: 0.7780 acc_train: 0.8500 loss_val: 0.9808 acc_val: 0.7833 time: 0.0300s\n",
      "Epoch: 0097 loss_train: 0.7522 acc_train: 0.8786 loss_val: 0.9730 acc_val: 0.7833 time: 0.0447s\n",
      "Epoch: 0098 loss_train: 0.7764 acc_train: 0.8429 loss_val: 0.9647 acc_val: 0.7800 time: 0.0298s\n",
      "Epoch: 0099 loss_train: 0.7002 acc_train: 0.8857 loss_val: 0.9570 acc_val: 0.7867 time: 0.0493s\n",
      "Epoch: 0100 loss_train: 0.7430 acc_train: 0.8714 loss_val: 0.9495 acc_val: 0.7867 time: 0.0359s\n",
      "Epoch: 0101 loss_train: 0.7197 acc_train: 0.8714 loss_val: 0.9425 acc_val: 0.7833 time: 0.0464s\n",
      "Epoch: 0102 loss_train: 0.7401 acc_train: 0.8429 loss_val: 0.9358 acc_val: 0.7933 time: 0.0369s\n",
      "Epoch: 0103 loss_train: 0.6873 acc_train: 0.9000 loss_val: 0.9295 acc_val: 0.7933 time: 0.0548s\n",
      "Epoch: 0104 loss_train: 0.6962 acc_train: 0.8571 loss_val: 0.9243 acc_val: 0.7867 time: 0.0338s\n",
      "Epoch: 0105 loss_train: 0.7042 acc_train: 0.8643 loss_val: 0.9196 acc_val: 0.7867 time: 0.0408s\n",
      "Epoch: 0106 loss_train: 0.7415 acc_train: 0.8429 loss_val: 0.9137 acc_val: 0.7933 time: 0.0409s\n",
      "Epoch: 0107 loss_train: 0.6957 acc_train: 0.8500 loss_val: 0.9075 acc_val: 0.7867 time: 0.0398s\n",
      "Epoch: 0108 loss_train: 0.6505 acc_train: 0.9000 loss_val: 0.9014 acc_val: 0.8000 time: 0.0473s\n",
      "Epoch: 0109 loss_train: 0.6162 acc_train: 0.8929 loss_val: 0.8962 acc_val: 0.8000 time: 0.0289s\n",
      "Epoch: 0110 loss_train: 0.6591 acc_train: 0.8857 loss_val: 0.8918 acc_val: 0.8067 time: 0.0369s\n",
      "Epoch: 0111 loss_train: 0.6595 acc_train: 0.8714 loss_val: 0.8868 acc_val: 0.8067 time: 0.0419s\n",
      "Epoch: 0112 loss_train: 0.6628 acc_train: 0.9000 loss_val: 0.8807 acc_val: 0.8033 time: 0.0379s\n",
      "Epoch: 0113 loss_train: 0.6412 acc_train: 0.8857 loss_val: 0.8753 acc_val: 0.8033 time: 0.0648s\n",
      "Epoch: 0114 loss_train: 0.5630 acc_train: 0.9357 loss_val: 0.8706 acc_val: 0.8033 time: 0.0403s\n",
      "Epoch: 0115 loss_train: 0.6260 acc_train: 0.8929 loss_val: 0.8660 acc_val: 0.8000 time: 0.0349s\n",
      "Epoch: 0116 loss_train: 0.6309 acc_train: 0.8929 loss_val: 0.8613 acc_val: 0.7933 time: 0.0322s\n",
      "Epoch: 0117 loss_train: 0.6429 acc_train: 0.8500 loss_val: 0.8567 acc_val: 0.7933 time: 0.0312s\n",
      "Epoch: 0118 loss_train: 0.6106 acc_train: 0.8857 loss_val: 0.8526 acc_val: 0.7967 time: 0.0249s\n",
      "Epoch: 0119 loss_train: 0.6072 acc_train: 0.9143 loss_val: 0.8492 acc_val: 0.7933 time: 0.0269s\n",
      "Epoch: 0120 loss_train: 0.5819 acc_train: 0.9143 loss_val: 0.8457 acc_val: 0.7967 time: 0.0364s\n",
      "Epoch: 0121 loss_train: 0.6364 acc_train: 0.8857 loss_val: 0.8418 acc_val: 0.7967 time: 0.0320s\n",
      "Epoch: 0122 loss_train: 0.6300 acc_train: 0.9071 loss_val: 0.8377 acc_val: 0.7967 time: 0.0297s\n",
      "Epoch: 0123 loss_train: 0.5954 acc_train: 0.8857 loss_val: 0.8332 acc_val: 0.7967 time: 0.0341s\n",
      "Epoch: 0124 loss_train: 0.5795 acc_train: 0.9286 loss_val: 0.8287 acc_val: 0.7967 time: 0.0528s\n",
      "Epoch: 0125 loss_train: 0.5960 acc_train: 0.8929 loss_val: 0.8247 acc_val: 0.8033 time: 0.0428s\n",
      "Epoch: 0126 loss_train: 0.5650 acc_train: 0.9071 loss_val: 0.8211 acc_val: 0.8000 time: 0.0345s\n",
      "Epoch: 0127 loss_train: 0.5880 acc_train: 0.9143 loss_val: 0.8173 acc_val: 0.8100 time: 0.0604s\n",
      "Epoch: 0128 loss_train: 0.5450 acc_train: 0.9214 loss_val: 0.8139 acc_val: 0.8100 time: 0.0409s\n",
      "Epoch: 0129 loss_train: 0.5400 acc_train: 0.9286 loss_val: 0.8105 acc_val: 0.8100 time: 0.0509s\n",
      "Epoch: 0130 loss_train: 0.5499 acc_train: 0.9071 loss_val: 0.8078 acc_val: 0.8033 time: 0.0517s\n",
      "Epoch: 0131 loss_train: 0.5724 acc_train: 0.9214 loss_val: 0.8057 acc_val: 0.8033 time: 0.0688s\n",
      "Epoch: 0132 loss_train: 0.5840 acc_train: 0.8929 loss_val: 0.8046 acc_val: 0.8033 time: 0.0547s\n",
      "Epoch: 0133 loss_train: 0.5709 acc_train: 0.9071 loss_val: 0.8017 acc_val: 0.8033 time: 0.0489s\n",
      "Epoch: 0134 loss_train: 0.5673 acc_train: 0.9214 loss_val: 0.7981 acc_val: 0.8033 time: 0.0352s\n",
      "Epoch: 0135 loss_train: 0.5070 acc_train: 0.9214 loss_val: 0.7938 acc_val: 0.7967 time: 0.0285s\n",
      "Epoch: 0136 loss_train: 0.5561 acc_train: 0.8929 loss_val: 0.7885 acc_val: 0.8033 time: 0.0369s\n",
      "Epoch: 0137 loss_train: 0.5497 acc_train: 0.9143 loss_val: 0.7846 acc_val: 0.8033 time: 0.0407s\n",
      "Epoch: 0138 loss_train: 0.4984 acc_train: 0.9143 loss_val: 0.7807 acc_val: 0.8033 time: 0.0469s\n",
      "Epoch: 0139 loss_train: 0.5170 acc_train: 0.9214 loss_val: 0.7767 acc_val: 0.8033 time: 0.0320s\n",
      "Epoch: 0140 loss_train: 0.5306 acc_train: 0.8786 loss_val: 0.7732 acc_val: 0.8033 time: 0.0302s\n",
      "Epoch: 0141 loss_train: 0.5342 acc_train: 0.9071 loss_val: 0.7710 acc_val: 0.8067 time: 0.0271s\n",
      "Epoch: 0142 loss_train: 0.5134 acc_train: 0.9214 loss_val: 0.7688 acc_val: 0.8033 time: 0.0621s\n",
      "Epoch: 0143 loss_train: 0.5069 acc_train: 0.9143 loss_val: 0.7669 acc_val: 0.8033 time: 0.0497s\n",
      "Epoch: 0144 loss_train: 0.5448 acc_train: 0.9000 loss_val: 0.7653 acc_val: 0.8033 time: 0.0363s\n",
      "Epoch: 0145 loss_train: 0.5060 acc_train: 0.9000 loss_val: 0.7637 acc_val: 0.8067 time: 0.0269s\n",
      "Epoch: 0146 loss_train: 0.4790 acc_train: 0.9143 loss_val: 0.7612 acc_val: 0.8067 time: 0.0309s\n",
      "Epoch: 0147 loss_train: 0.4761 acc_train: 0.9143 loss_val: 0.7580 acc_val: 0.8033 time: 0.0292s\n",
      "Epoch: 0148 loss_train: 0.5152 acc_train: 0.9571 loss_val: 0.7548 acc_val: 0.8033 time: 0.0369s\n",
      "Epoch: 0149 loss_train: 0.5028 acc_train: 0.9143 loss_val: 0.7521 acc_val: 0.8033 time: 0.0369s\n",
      "Epoch: 0150 loss_train: 0.4858 acc_train: 0.9357 loss_val: 0.7498 acc_val: 0.8067 time: 0.0259s\n",
      "Epoch: 0151 loss_train: 0.4906 acc_train: 0.9429 loss_val: 0.7476 acc_val: 0.8067 time: 0.0270s\n",
      "Epoch: 0152 loss_train: 0.4966 acc_train: 0.9214 loss_val: 0.7456 acc_val: 0.8067 time: 0.0268s\n",
      "Epoch: 0153 loss_train: 0.4736 acc_train: 0.9286 loss_val: 0.7430 acc_val: 0.8067 time: 0.0477s\n",
      "Epoch: 0154 loss_train: 0.4714 acc_train: 0.9500 loss_val: 0.7410 acc_val: 0.8133 time: 0.0578s\n",
      "Epoch: 0155 loss_train: 0.4414 acc_train: 0.9500 loss_val: 0.7390 acc_val: 0.8100 time: 0.0399s\n",
      "Epoch: 0156 loss_train: 0.4563 acc_train: 0.9357 loss_val: 0.7369 acc_val: 0.8100 time: 0.0276s\n",
      "Epoch: 0157 loss_train: 0.4565 acc_train: 0.9500 loss_val: 0.7351 acc_val: 0.8100 time: 0.0299s\n",
      "Epoch: 0158 loss_train: 0.4752 acc_train: 0.9500 loss_val: 0.7325 acc_val: 0.8100 time: 0.0319s\n",
      "Epoch: 0159 loss_train: 0.4121 acc_train: 0.9500 loss_val: 0.7307 acc_val: 0.8133 time: 0.0605s\n",
      "Epoch: 0160 loss_train: 0.4389 acc_train: 0.9357 loss_val: 0.7295 acc_val: 0.8033 time: 0.0488s\n",
      "Epoch: 0161 loss_train: 0.4263 acc_train: 0.9357 loss_val: 0.7284 acc_val: 0.8033 time: 0.0366s\n",
      "Epoch: 0162 loss_train: 0.4542 acc_train: 0.9286 loss_val: 0.7273 acc_val: 0.8067 time: 0.0423s\n",
      "Epoch: 0163 loss_train: 0.4094 acc_train: 0.9357 loss_val: 0.7255 acc_val: 0.8067 time: 0.0661s\n",
      "Epoch: 0164 loss_train: 0.4017 acc_train: 0.9500 loss_val: 0.7224 acc_val: 0.8067 time: 0.0339s\n",
      "Epoch: 0165 loss_train: 0.4273 acc_train: 0.9571 loss_val: 0.7202 acc_val: 0.8033 time: 0.0359s\n",
      "Epoch: 0166 loss_train: 0.4107 acc_train: 0.9357 loss_val: 0.7188 acc_val: 0.8033 time: 0.0378s\n",
      "Epoch: 0167 loss_train: 0.4335 acc_train: 0.9500 loss_val: 0.7184 acc_val: 0.8133 time: 0.0499s\n",
      "Epoch: 0168 loss_train: 0.4908 acc_train: 0.9143 loss_val: 0.7169 acc_val: 0.8067 time: 0.0426s\n",
      "Epoch: 0169 loss_train: 0.4509 acc_train: 0.9071 loss_val: 0.7146 acc_val: 0.8100 time: 0.0333s\n",
      "Epoch: 0170 loss_train: 0.4062 acc_train: 0.9500 loss_val: 0.7127 acc_val: 0.8100 time: 0.0581s\n",
      "Epoch: 0171 loss_train: 0.4291 acc_train: 0.9071 loss_val: 0.7109 acc_val: 0.8133 time: 0.0399s\n",
      "Epoch: 0172 loss_train: 0.4251 acc_train: 0.9500 loss_val: 0.7094 acc_val: 0.8067 time: 0.0435s\n",
      "Epoch: 0173 loss_train: 0.4206 acc_train: 0.9214 loss_val: 0.7086 acc_val: 0.8067 time: 0.0355s\n",
      "Epoch: 0174 loss_train: 0.4374 acc_train: 0.9357 loss_val: 0.7083 acc_val: 0.8033 time: 0.0284s\n",
      "Epoch: 0175 loss_train: 0.4020 acc_train: 0.9571 loss_val: 0.7076 acc_val: 0.8067 time: 0.0266s\n",
      "Epoch: 0176 loss_train: 0.4117 acc_train: 0.9429 loss_val: 0.7079 acc_val: 0.8067 time: 0.0299s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0177 loss_train: 0.3960 acc_train: 0.9143 loss_val: 0.7083 acc_val: 0.8000 time: 0.0335s\n",
      "Epoch: 0178 loss_train: 0.3927 acc_train: 0.9429 loss_val: 0.7078 acc_val: 0.8000 time: 0.0384s\n",
      "Epoch: 0179 loss_train: 0.3983 acc_train: 0.9500 loss_val: 0.7070 acc_val: 0.8000 time: 0.0419s\n",
      "Epoch: 0180 loss_train: 0.4297 acc_train: 0.9143 loss_val: 0.7063 acc_val: 0.8067 time: 0.0341s\n",
      "Epoch: 0181 loss_train: 0.3729 acc_train: 0.9500 loss_val: 0.7052 acc_val: 0.8100 time: 0.0259s\n",
      "Epoch: 0182 loss_train: 0.4353 acc_train: 0.9214 loss_val: 0.7025 acc_val: 0.8100 time: 0.0279s\n",
      "Epoch: 0183 loss_train: 0.4385 acc_train: 0.9429 loss_val: 0.6983 acc_val: 0.8133 time: 0.0423s\n",
      "Epoch: 0184 loss_train: 0.3867 acc_train: 0.9571 loss_val: 0.6954 acc_val: 0.8167 time: 0.0359s\n",
      "Epoch: 0185 loss_train: 0.3816 acc_train: 0.9643 loss_val: 0.6941 acc_val: 0.8167 time: 0.0480s\n",
      "Epoch: 0186 loss_train: 0.3960 acc_train: 0.9571 loss_val: 0.6938 acc_val: 0.8167 time: 0.0338s\n",
      "Epoch: 0187 loss_train: 0.3559 acc_train: 0.9786 loss_val: 0.6935 acc_val: 0.8167 time: 0.0338s\n",
      "Epoch: 0188 loss_train: 0.4254 acc_train: 0.9500 loss_val: 0.6928 acc_val: 0.8200 time: 0.0659s\n",
      "Epoch: 0189 loss_train: 0.3781 acc_train: 0.9643 loss_val: 0.6927 acc_val: 0.8133 time: 0.0329s\n",
      "Epoch: 0190 loss_train: 0.3799 acc_train: 0.9786 loss_val: 0.6937 acc_val: 0.8100 time: 0.0406s\n",
      "Epoch: 0191 loss_train: 0.3578 acc_train: 0.9571 loss_val: 0.6947 acc_val: 0.8067 time: 0.0349s\n",
      "Epoch: 0192 loss_train: 0.3529 acc_train: 0.9500 loss_val: 0.6940 acc_val: 0.8067 time: 0.0459s\n",
      "Epoch: 0193 loss_train: 0.3700 acc_train: 0.9286 loss_val: 0.6933 acc_val: 0.8067 time: 0.0313s\n",
      "Epoch: 0194 loss_train: 0.3888 acc_train: 0.9286 loss_val: 0.6909 acc_val: 0.8133 time: 0.0262s\n",
      "Epoch: 0195 loss_train: 0.3907 acc_train: 0.9571 loss_val: 0.6880 acc_val: 0.8133 time: 0.0279s\n",
      "Epoch: 0196 loss_train: 0.4048 acc_train: 0.9571 loss_val: 0.6855 acc_val: 0.8133 time: 0.0277s\n",
      "Epoch: 0197 loss_train: 0.3853 acc_train: 0.9643 loss_val: 0.6825 acc_val: 0.8200 time: 0.0410s\n",
      "Epoch: 0198 loss_train: 0.4291 acc_train: 0.9429 loss_val: 0.6811 acc_val: 0.8133 time: 0.0289s\n",
      "Epoch: 0199 loss_train: 0.3759 acc_train: 0.9357 loss_val: 0.6812 acc_val: 0.8133 time: 0.0259s\n",
      "Epoch: 0200 loss_train: 0.3835 acc_train: 0.9357 loss_val: 0.6809 acc_val: 0.8167 time: 0.0280s\n",
      "Optimization Finished\n",
      "Total time elapsed: 19.4959s\n"
     ]
    }
   ],
   "source": [
    "t_total = time.time()\n",
    "for epoch in range(200):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 0.7037 accuracy= 0.8370\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
