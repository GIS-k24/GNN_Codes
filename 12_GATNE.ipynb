{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GATNE: General Attributed Multiplex Heterogeneous Network Embedding\n",
    "- 每个节点在不同类型边中有不同的表示\n",
    "- 分为两种模型：GATNE-T模型：直推式学习 | GATNE-I模型：归纳式学习【更优】\n",
    "\n",
    "#### 此代码不支持Windows系统"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "import random\n",
    "import math\n",
    "\n",
    "from numpy import random\n",
    "from collections import defaultdict\n",
    "from operator import index\n",
    "from six import iteritems\n",
    "from sklearn.metrics import auc, f1_score, precision_recall_curve, roc_auc_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn.parameter import Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    root = r\"C:\\Users\\sss\\Desktop\\GATNE/\"\n",
    "    \n",
    "    parser.add_argument('--input', type=str, default=root+'data/amazon', help='Input dataset path')\n",
    "    parser.add_argument('--features', type=str, default=None, help='Input node features')\n",
    "    parser.add_argument('--walk-file', type=str, default=None, help='Input random walks')\n",
    "    parser.add_argument('--epoch', type=int, default=100, help='Number of epoch. Default is 100.')\n",
    "    parser.add_argument('--batch-size', type=int, default=64, help='Number of batch_size. Default is 64.')\n",
    "    parser.add_argument('--eval-type', type=str, default='all', help='The edge type(s) for evaluation.')\n",
    "    parser.add_argument('--schema', type=str, default=None, help='The metapath schema (e.g., U-I-U,I-U-I).')\n",
    "    parser.add_argument('--dimensions', type=int, default=200, help='Number of dimensions. Default is 200.')\n",
    "    parser.add_argument('--edge-dim', type=int, default=10, help='Number of edge embedding dimensions. Default is 10.')\n",
    "    parser.add_argument('--att-dim', type=int, default=20, help='Number of attention dimensions. Default is 20.')\n",
    "    parser.add_argument('--walk-length', type=int, default=10, help='Length of walk per source. Default is 10.')\n",
    "    parser.add_argument('--num-walks', type=int, default=20, help='Number of walks per source. Default is 20.')\n",
    "    parser.add_argument('--window-size', type=int, default=5, help='Context size for optimization. Default is 5.')\n",
    "    parser.add_argument('--negative-samples', type=int, default=5, help='Negative samples for optimization. Default is 5.')\n",
    "    parser.add_argument('--neighbor-samples', type=int, default=10, help='Neighbor samples for aggregation. Default is 10.')\n",
    "    parser.add_argument('--patience', type=int, default=5, help='Early stopping patience. Default is 5.')\n",
    "    parser.add_argument('--num-workers', type=int, default=16, help='Number of workers for generating random walks. Default is 16.')\n",
    "    \n",
    "    return parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(object):\n",
    "    def __init__(self, count, index):\n",
    "        self.count = count\n",
    "        self.index = index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据加载及预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 节点与节点的连接关系\n",
    "\n",
    "def load_training_data(f_name):\n",
    "    print('We are loading data from:', f_name)\n",
    "    edge_data_by_type = dict()  # 每个type对应到的相连接节点\n",
    "    all_nodes = list()  # 所有节点的集合\n",
    "    with open(f_name, 'r') as f:\n",
    "        for line in f:\n",
    "            words = line[:-1].split(' ')  # 去除/n\n",
    "            if words[0] not in edge_data_by_type:  # edge type涉及到的节点\n",
    "                edge_data_by_type[words[0]] = list()\n",
    "            x, y = words[1], words[2]\n",
    "            edge_data_by_type[words[0]].append((x, y))\n",
    "            all_nodes.append(x)\n",
    "            all_nodes.append(y)\n",
    "    all_nodes = list(set(all_nodes))  # nodes去重\n",
    "    print('Total training nodes: ' + str(len(all_nodes)))\n",
    "    return edge_data_by_type  # 每个type连接的点边情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 节点与节点的连接关系【有真假之分】\n",
    "\n",
    "def load_testing_data(f_name):\n",
    "    print('We are loading data from:', f_name)\n",
    "    true_edge_data_by_type = dict()  # true样本\n",
    "    false_edge_data_by_type = dict()  # false样本\n",
    "    all_nodes = list()\n",
    "    with open(f_name, 'r') as f:\n",
    "        for line in f:\n",
    "            words = line[:-1].split(' ')\n",
    "            x, y = words[1], words[2]\n",
    "            if int(words[3]) == 1:  # true 对应到的节点\n",
    "                if words[0] not in true_edge_data_by_type:\n",
    "                    true_edge_data_by_type[words[0]] = list()  # true对应到的type相连接节点\n",
    "                true_edge_data_by_type[words[0]].append((x, y))\n",
    "            else:  # false 对应到的节点\n",
    "                if words[0] not in false_edge_data_by_type:\n",
    "                    false_edge_data_by_type[words[0]] = list()\n",
    "                false_edge_data_by_type[words[0]].append((x, y))\n",
    "            all_nodes.append(x)\n",
    "            all_nodes.append(y)\n",
    "    all_nodes = list(set(all_nodes))\n",
    "    return true_edge_data_by_type, false_edge_data_by_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 节点特征\n",
    "\n",
    "def load_feature_data(f_name):\n",
    "    feature_dic = {}\n",
    "    with open(f_name, 'r') as f:\n",
    "        first = True\n",
    "        for line in f:\n",
    "            if first:\n",
    "                first = False\n",
    "                continue\n",
    "            items = line.strip().split()\n",
    "            feature_dic[items[0]] = items[1:]\n",
    "    return feature_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机游走生成MetaPath[采样][负采样]\n",
    "- walk + initializer -- RWGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk(args):\n",
    "    walk_length, start, schema = args\n",
    "    # 随机游走\n",
    "    rand = random.Random()\n",
    "    \n",
    "    if schema:\n",
    "        schema_items = schema.split(\"-\")\n",
    "        assert schema_items[0] == schema_items[-1]  # metapath前后一致; A-B-A\n",
    "    \n",
    "    walk = [start]\n",
    "    while len(walk) < walk_length:\n",
    "        cur = walk[-1]  # 当前节点\n",
    "        candidates = []\n",
    "        for node in G[cur]:  # 和cur节点相连接的节点; 候选节点\n",
    "            if schema == \"\" or node_type[node] == schema_items[len(walk) % (len(schema_items) - 1)]:\n",
    "                candidates.append(node)\n",
    "        if candidates:\n",
    "            walk.append(rand.choice(candidates))\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return [str(node) for node in walk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initializer(init_G, init_node_type):\n",
    "    global G\n",
    "    G = init_G\n",
    "    global node_type\n",
    "    node_type = init_node_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RWGraph():\n",
    "    def __init__(self, nx_G, node_type_arr=None, num_workers=0):\n",
    "        self.G = nx_G\n",
    "        self.node_type = node_type_arr\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "    def node_list(self, nodes, num_walks):  \n",
    "        for loop in range(num_walks):  # 循环num_walks次数\n",
    "            for node in nodes:\n",
    "                yield node\n",
    "                \n",
    "    def simulate_walks(self, num_walks, walk_length, schema=None):\n",
    "        all_walks = []\n",
    "        nodes = list(self.G.keys())  # 节点顶点数量\n",
    "        random.shuffle(nodes)\n",
    "        \n",
    "        if schema is None:\n",
    "            with multiprocessing.Pool(self.num_workers, initializer=initializer, initargs=(self.G, self.node_type)) as pool:\n",
    "                all_walks = list(pool.imap(walk, ((walk_length, node, '') for node in tqdm(self.node_list(nodes, num_walks))), chunksize=256))\n",
    "        else:\n",
    "            schema_list = schema.split(',')\n",
    "            for schema_iter in schema_list:\n",
    "                with multiprocessing.Pool(self.num_workers, initializer=initializer, initargs=(self.G, self.node_type)) as pool:\n",
    "                    walks = list(pool.imap(walk, ((walk_length, node, schema_iter) for node in tqdm(self.node_list(nodes, num_walks)) if schema_iter.split('-')[0] == self.node_type[node]), chunksize=512))\n",
    "                all_walks.extend(walks)\n",
    "                \n",
    "        return all_walks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- load_node_type + get_G_from_edges + RWGraph -- generate_walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_node_type(f_name):\n",
    "    print(\"We are loading node type from:\", f_name)\n",
    "    node_type = {}\n",
    "    with open(f_name, \"r\") as f:\n",
    "        for line in f:\n",
    "            items = line.strip().split()\n",
    "            node_type[items[0]] = items[1]\n",
    "    return node_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_G_from_edges(edges):\n",
    "    edge_dict = defaultdict(set)\n",
    "    for edge in edges:\n",
    "        u, v = str(edge[0]), str(edge[1])\n",
    "        edge_dict[u].add(v)\n",
    "        edge_dict[v].add(u)\n",
    "    return edge_dict  # 每个节点和它相连接的节点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_walks(network_data, num_walks, walk_length, schema, file_name, num_workers):\n",
    "    if schema is not None:  # schema：节点类型\n",
    "        # load_node_type\n",
    "        node_type = load_node_type(file_name + '/node_type.txt')\n",
    "    else:\n",
    "        node_type = None\n",
    "\n",
    "    all_walks = []  # 所有游走的list\n",
    "    for layer_id, layer_name in enumerate(network_data):\n",
    "        tmp_data = network_data[layer_name]  # 每个type对应到的点边信息\n",
    "        # start to do the random walk on a layer\n",
    "        # get_G_from_edges(tmp_data): 每个节点对应到相连接的点\n",
    "\n",
    "        # get_G_from_edges\n",
    "        layer_walker = RWGraph(get_G_from_edges(tmp_data), node_type, num_workers)  # RandomWalk Graph\n",
    "        print('Generating random walks for layer', layer_id)\n",
    "        layer_walks = layer_walker.simulate_walks(num_walks, walk_length, schema=schema)  # 生成随机游走的序列; 每个节点游走次数; 游走长度;\n",
    "\n",
    "        all_walks.append(layer_walks)\n",
    "\n",
    "    print('Finish generating the walks')\n",
    "\n",
    "    return all_walks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- generate_walks + generate_vocab + generate_pairs + load_walks + save_walks -- generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vocab(all_walks):\n",
    "    index2word = []\n",
    "    raw_vocab = defaultdict(int)\n",
    "    # 随机游走每个单词出现的次数\n",
    "    for layer_id, walks in enumerate(all_walks):  # 按照type类别\n",
    "        print('Counting vocab for layer', layer_id)\n",
    "        for walk in tqdm(walks):\n",
    "            for word in walk:  # 记录每个单词出现的次数\n",
    "                raw_vocab[word] += 1\n",
    "\n",
    "    vocab = {}\n",
    "    for word, v in iteritems(raw_vocab):\n",
    "        vocab[word] = Vocab(count=v, index=len(index2word))  # 用一个类表示节点的次数和index\n",
    "        index2word.append(word)\n",
    "\n",
    "    index2word.sort(key=lambda word: vocab[word].count, reverse=True)  # 按出现次数降序排列\n",
    "    for i, word in enumerate(index2word):  # 节点按出现次数降序更新索引\n",
    "        vocab[word].index = i\n",
    "    # 节点统计信息; 节点信息\n",
    "    return vocab, index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pairs(all_walks, vocab, window_size, num_workers):\n",
    "    pairs = []\n",
    "    skip_window = window_size // 2\n",
    "    for layer_id, walks in enumerate(all_walks):\n",
    "        print('Generating training pairs for layer', layer_id)\n",
    "        for walk in tqdm(walks):  # type的游走语料进行循环\n",
    "            for i in range(len(walk)):  # 每个单词循环\n",
    "                for j in range(1, skip_window + 1):  # 向前向后的窗口长度\n",
    "                    if i - j >= 0:\n",
    "                        pairs.append((vocab[walk[i]].index, vocab[walk[i - j]].index, layer_id))  # 向前窗口涉及到的单词\n",
    "                    if i + j < len(walk):\n",
    "                        pairs.append((vocab[walk[i]].index, vocab[walk[i + j]].index, layer_id))  # 向后窗口涉及到的单词\n",
    "    return pairs  # 所有单词上线文的索引, type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_walks(walk_file):\n",
    "    print('Loading walks')\n",
    "    all_walks = []\n",
    "    with open(walk_file, 'r') as f:\n",
    "        for line in f:\n",
    "            content = line.strip().split()\n",
    "            layer_id = int(content[0])\n",
    "            if layer_id >= len(all_walks):\n",
    "                all_walks.append([])\n",
    "            all_walks[layer_id].append(content[1:])\n",
    "    return all_walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_walks(walk_file, all_walks):\n",
    "    with open(walk_file, 'w') as f:\n",
    "        for layer_id, walks in enumerate(all_walks):\n",
    "            print('Saving walks for layer', layer_id)\n",
    "            for walk in tqdm(walks):\n",
    "                f.write(' '.join([str(layer_id)] + [str(x) for x in walk]) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(network_data, num_walks, walk_length, schema, file_name, window_size, num_workers, walk_file):\n",
    "    if walk_file is not None:  # 如果没有walk_file，则自己生成walks\n",
    "        all_walks = load_walks(walk_file)\n",
    "    else:  # 边的信息; 每个节点游走次数; 游走长度;\n",
    "        all_walks = generate_walks(network_data, num_walks, walk_length, schema, file_name, num_workers)  # 生成随机游走序列\n",
    "        save_walks(file_name + '/walks.txt', all_walks)\n",
    "    vocab, index2word = generate_vocab(all_walks)  # 生成节点index; 降序方式;\n",
    "    train_pairs = generate_pairs(all_walks, vocab, window_size, num_workers)  # (center, context, type)\n",
    "    # vocab:节点统计信息; index2word:节点; train_pairs:skip-gram训练样本\n",
    "    return vocab, index2word, train_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_neighbors(network_data, vocab, num_nodes, edge_types, neighbor_samples):\n",
    "    edge_type_count = len(edge_types)\n",
    "    neighbors = [[[] for __ in range(edge_type_count)] for _ in range(num_nodes)]\n",
    "    for r in range(edge_type_count):\n",
    "        print('Generating neighbors for layer', r)\n",
    "        g = network_data[edge_types[r]]  # 每个type涉及到的节点\n",
    "        for (x, y) in tqdm(g):\n",
    "            ix = vocab[x].index  # x对应到的索引\n",
    "            iy = vocab[y].index  # y对应到的索引\n",
    "            neighbors[ix][r].append(iy)  # 邻居信息\n",
    "            neighbors[iy][r].append(ix)\n",
    "        for i in range(num_nodes):\n",
    "            if len(neighbors[i][r]) == 0:  # 节点在这个类别下，如果没有节点和它连接，邻居就是该节点本身\n",
    "                neighbors[i][r] = [i] * neighbor_samples\n",
    "            elif len(neighbors[i][r]) < neighbor_samples:  # 如果邻居节点数量小于采样邻居数量，进行重采样\n",
    "                neighbors[i][r].extend(list(np.random.choice(neighbors[i][r], size=neighbor_samples-len(neighbors[i][r]))))\n",
    "            elif len(neighbors[i][r]) > neighbor_samples:  # 如果邻居节点数量大于采样邻居数量，进行邻居大小数量的采样\n",
    "                neighbors[i][r] = list(np.random.choice(neighbors[i][r], size=neighbor_samples))\n",
    "    return neighbors  # 每个节点的邻居采样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 指定一个batch大小的数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(pairs, neighbors, batch_size):\n",
    "    n_batches = (len(pairs) + (batch_size - 1)) // batch_size  # 迭代次数\n",
    "    \n",
    "    for idx in range(n_batches):\n",
    "        x, y, t, neigh = [], [], [], []  # src, dst, type, neigh\n",
    "        for i in range(batch_size):\n",
    "            index = idx * batch_size + i\n",
    "            if index >= len(pairs):\n",
    "                break\n",
    "            x.append(pairs[index][0])\n",
    "            y.append(pairs[index][1])\n",
    "            t.append(pairs[index][2])\n",
    "            neigh.append(neighbors[pairs[index][0]])  # src的邻居节点\n",
    "        yield torch.tensor(x), torch.tensor(y), torch.tensor(t), torch.tensor(neigh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATNEModel(nn.Module):\n",
    "    def __init__(self, num_nodes, embedding_size, embedding_u_size, edge_type_count, dim_a, features):\n",
    "        super(GATNEModel, self).__init__()\n",
    "        self.num_nodes = num_nodes  # 节点数量\n",
    "        self.embedding_size = embedding_size  # 每个节点输出的embedding_size\n",
    "        self.embedding_u_size = embedding_u_size  # 节点作为邻居初始化size\n",
    "        self.edge_type_count = edge_type_count  # 类别数量\n",
    "        self.dim_a = dim_a  # 中间隐藏层特征数量\n",
    "        \n",
    "        self.features = None\n",
    "        if features is not None:  # GATNE-I\n",
    "            self.features = features\n",
    "            feature_dim = self.features.shape[-1]\n",
    "            self.embed_trans = Parameter(\n",
    "                torch.FloatTensor(feature_dim, embedding_size)\n",
    "            )  # [142, 200]; bi-base embedding\n",
    "            self.u_embed_trans = Parameter(\n",
    "                torch.FloatTensor(edge_type_count, feature_dim, embedding_u_size)\n",
    "            )  # [2, 142, 10]; 初始化ui\n",
    "        else:  # 初始化 base embedding GATNE-T\n",
    "            self.node_embeddings = Parameter(\n",
    "                torch.FloatTensor(num_nodes, embedding_size)\n",
    "            )  # [511, 200]\n",
    "            self.node_type_embeddings = Parameter(  # 初始化 edge embedding\n",
    "                torch.FloatTensor(num_nodes, edge_type_count, embedding_u_size)\n",
    "            )  #  [511, 2, 10]\n",
    "        self.trans_weights = Parameter(  # [2, 10, 200]: 定义Mr矩阵\n",
    "            torch.FloatTensor(edge_type_count, embedding_u_size, embedding_size)\n",
    "        )\n",
    "        self.trans_weights_s1 = Parameter(  # [2, 10, 20]  计算attention使用\n",
    "            torch.FloatTensor(edge_type_count, embedding_u_size, dim_a)\n",
    "        )\n",
    "        self.trans_weights_s2 = Parameter(\n",
    "            torch.FloatTensor(edge_type_count, dim_a, 1)\n",
    "        )  # [2, 20, 1]\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        if self.features is not None:\n",
    "            self.embed_trans.data.normal_(std=1.0 / math.sqrt(self.embedding_size))\n",
    "            self.u_embed_trans.data.normal_(std=1.0 / math.sqrt(self.embedding_size))\n",
    "        else:\n",
    "            self.node_embeddings.data.uniform_(-1.0, 1.0)\n",
    "            self.node_type_embeddings.data.uniform_(-1.0, 1.0)\n",
    "        self.trans_weights.data.normal_(std=1.0 / math.sqrt(self.embedding_size))\n",
    "        self.trans_weights_s1.data.normal_(std=1.0 / math.sqrt(self.embedding_size))\n",
    "        self.trans_weights_s2.data.normal_(std=1.0 / math.sqrt(self.embedding_size))\n",
    "    \n",
    "    def forward(self, train_inputs, train_types, node_neigh):\n",
    "        if self.features is None:\n",
    "            node_embed = self.node_embeddings[train_inputs]  # 每个节点对应的embedding\n",
    "            node_embed_neighbors = self.node_type_embeddings[node_neigh]  # 每个节点对应的neighbors\n",
    "        else:  # self.features:节点特征; self.embed_trans\n",
    "            node_embed = torch.mm(self.features[train_inputs], self.embed_trans)  # [64, 200]\n",
    "            node_embed_neighbors = torch.einsum('bijk,akm->bijam', self.features[node_neigh], self.u_embed_trans)  # 生成ui; [64, 2, 10, 142] * [2, 142, 10]\n",
    "        node_embed_tmp = torch.cat([\n",
    "            node_embed_neighbors[:, i, :, i, :].unsqueeze(1)  # [64, 1, 10, 10]\n",
    "            for i in range(self.edge_type_count)\n",
    "        ], dim = 1)\n",
    "        node_type_embed = torch.sum(node_embed_tmp, dim=2)  # Ui; 对邻居信息求和; [64, 2, 10]\n",
    "        \n",
    "        trans_w = self.trans_weights[train_types]  # [64, 10, 200]\n",
    "        trans_w_s1 = self.trans_weights_s1[train_types]  # [64, 10, 20]\n",
    "        trans_w_s2 = self.trans_weights_s2[train_types]  # [64, 20, 1]\n",
    "        \n",
    "        attention = F.softmax(  # [64, 1, 2]\n",
    "            torch.matmul(\n",
    "                torch.tanh(torch.matmul(node_type_embed, trans_w_s1)), trans_w_s2\n",
    "            ).squeeze(2),\n",
    "            dim = 1\n",
    "        ).unsqueeze(1)\n",
    "        node_type_embed = torch.matmul(attention, node_type_embed)  # [64, 1, 2] * [64, 2, 10] 对node_type_embed做attention求和\n",
    "        node_embed = node_embed + torch.matmul(node_type_embed, trans_w).squeeze(1)  # [64, 200] + [64, 1, 10] * [64, 10, 200] => [64, 200]\n",
    "        \n",
    "        last_node_embed = F.normalize(node_embed, dim=1)  # dim=1, L2-norm; (last_node_embed * last_node_embed).sum(axis=1)\n",
    "        \n",
    "        return last_node_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NSLoss(nn.Module):\n",
    "    def __init__(self, num_nodes, num_sampled, embedding_size):\n",
    "        super(NSLoss, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.num_sampled = num_sampled\n",
    "        self.embedding_size = embedding_size\n",
    "        self.weights = Parameter(\n",
    "            torch.FloatTensor(num_nodes, embedding_size)\n",
    "        )  # [511, 200]; Cj\n",
    "        self.sample_weights = F.normalize(  # [511]; 对节点进行初始化\n",
    "            torch.Tensor([\n",
    "                (math.log(k + 2) - math.log(k + 1)) / math.log(num_nodes + 1)\n",
    "                for k in range(num_nodes)\n",
    "            ]),\n",
    "            dim=0,\n",
    "        )\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        self.weights.data.normal_(std=1.0 / math.sqrt(self.embedding_size))\n",
    "        \n",
    "    def forward(self, input, embs, label):\n",
    "        n = input.shape[0]\n",
    "        log_target = torch.log(  # torch.mul: 对应位置相乘\n",
    "            torch.sigmoid(torch.sum(torch.mul(embs, self.weights[label]), 1))  # sigmoid([64]); input_embeddings*labels_embeddings\n",
    "        )\n",
    "        negs = torch.multinomial(  # 抽样函数，self.sample_weights的权重抽样\n",
    "            self.sample_weights, self.num_sampled * n, replacement=True\n",
    "        ).view(n, self.num_sampled)\n",
    "        \n",
    "        noise = torch.neg(self.weights[negs])  # Cj; 所有值 * -1\n",
    "        sum_log_sampled = torch.sum(\n",
    "            torch.log(torch.sigmoid(torch.bmm(noise, embs.unsqueeze(2)))), 1  # [64, 5, 1]\n",
    "        ).squeeze()\n",
    "        \n",
    "        loss = log_target + sum_log_sampled\n",
    "        return -loss.sum() / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(network_data, feature_dic):\n",
    "    vocab, index2word, train_pairs = generate(network_data, args.num_walks, args.walk_length, args.schema, file_name, args.window_size, args.num_workers, args.walk_file)\n",
    "    # 生成随机游走训练序列和训练语料\n",
    "    edge_types = list(network_data.keys())  # 边的类别\n",
    "\n",
    "    num_nodes = len(index2word)  # 节点数量\n",
    "    edge_type_count = len(edge_types)  # 类别数量\n",
    "    epochs = args.epoch\n",
    "    batch_size = args.batch_size\n",
    "    embedding_size = args.dimensions  # base embedding_size; embedding_size;\n",
    "    embedding_u_size = args.edge_dim  # edge_embedding_size\n",
    "    u_num = edge_type_count\n",
    "    num_sampled = args.negative_samples\n",
    "    dim_a = args.att_dim  # 计算attention的中间变量维度\n",
    "    att_head = 1\n",
    "    neighbor_samples = args.neighbor_samples  # 邻居采样数量\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # 每个类别节点的邻居节点; 在计算ui时使用;\n",
    "    neighbors = generate_neighbors(network_data, vocab, num_nodes, edge_types, neighbor_samples)\n",
    "\n",
    "    features = None\n",
    "    if feature_dic is not None:  # GATNE-I\n",
    "        feature_dim = len(list(feature_dic.values())[0])  # 特征长度\n",
    "        print('feature dimension: ' + str(feature_dim))\n",
    "        features = np.zeros((num_nodes, feature_dim), dtype=np.float32)  # feature array\n",
    "        for key, value in feature_dic.items():\n",
    "            if key in vocab:\n",
    "                features[vocab[key].index, :] = np.array(value)\n",
    "        features = torch.FloatTensor(features).to(device)  # 特征矩阵\n",
    "    # 建立GATNE model\n",
    "    model = GATNEModel(\n",
    "        num_nodes, embedding_size, embedding_u_size, edge_type_count, dim_a, features\n",
    "    )\n",
    "    nsloss = NSLoss(num_nodes, num_sampled, embedding_size)\n",
    "\n",
    "    model.to(device)\n",
    "    nsloss.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        [{\"params\": model.parameters()}, {\"params\": nsloss.parameters()}], lr=1e-4\n",
    "    )\n",
    "\n",
    "    best_score = 0\n",
    "    test_score = (0.0, 0.0, 0.0)\n",
    "    patience = 0\n",
    "    for epoch in range(epochs):\n",
    "        random.shuffle(train_pairs)\n",
    "        batches = get_batches(train_pairs, neighbors, batch_size)\n",
    "\n",
    "        data_iter = tqdm(\n",
    "            batches,\n",
    "            desc=\"epoch %d\" % (epoch),\n",
    "            total=(len(train_pairs) + (batch_size - 1)) // batch_size,\n",
    "            bar_format=\"{l_bar}{r_bar}\",\n",
    "        )\n",
    "        avg_loss = 0.0\n",
    "\n",
    "        for i, data in enumerate(data_iter):\n",
    "            optimizer.zero_grad()  # center, context, types, neigh\n",
    "            embs = model(data[0].to(device), data[2].to(device), data[3].to(device),)  # 节点的embeddings\n",
    "            loss = nsloss(data[0].to(device), embs, data[1].to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            avg_loss += loss.item()\n",
    "\n",
    "            if i % 5000 == 0:\n",
    "                post_fix = {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"iter\": i,\n",
    "                    \"avg_loss\": avg_loss / (i + 1),\n",
    "                    \"loss\": loss.item(),\n",
    "                }\n",
    "                data_iter.write(str(post_fix))\n",
    "\n",
    "            '''  调试使用  '''\n",
    "            if i==0:\n",
    "                break\n",
    "\n",
    "        final_model = dict(zip(edge_types, [dict() for _ in range(edge_type_count)]))  # 每个类别下节点的embedding;\n",
    "        for i in range(num_nodes):\n",
    "            train_inputs = torch.tensor([i for _ in range(edge_type_count)]).to(device)  # 节点的多个类别，求每个类别下的embedding\n",
    "            train_types = torch.tensor(list(range(edge_type_count))).to(device)\n",
    "            node_neigh = torch.tensor(  # 节点在每个类别下的neighbors\n",
    "                [neighbors[i] for _ in range(edge_type_count)]\n",
    "            ).to(device)\n",
    "            node_emb = model(train_inputs, train_types, node_neigh)  # [node1, node1]; [type1, type2]; [node1_neigh, node1_neigh]\n",
    "            for j in range(edge_type_count):  # 每个节点在各个类别下的embedding\n",
    "                final_model[edge_types[j]][index2word[i]] = (\n",
    "                    node_emb[j].cpu().detach().numpy()\n",
    "                )\n",
    "\n",
    "        valid_aucs, valid_f1s, valid_prs = [], [], []\n",
    "        test_aucs, test_f1s, test_prs = [], [], []\n",
    "        for i in range(edge_type_count):\n",
    "            if args.eval_type == \"all\" or edge_types[i] in args.eval_type.split(\",\"):\n",
    "                tmp_auc, tmp_f1, tmp_pr = evaluate(\n",
    "                    final_model[edge_types[i]],\n",
    "                    valid_true_data_by_edge[edge_types[i]],\n",
    "                    valid_false_data_by_edge[edge_types[i]],\n",
    "                )\n",
    "                valid_aucs.append(tmp_auc)\n",
    "                valid_f1s.append(tmp_f1)\n",
    "                valid_prs.append(tmp_pr)\n",
    "\n",
    "                tmp_auc, tmp_f1, tmp_pr = evaluate(\n",
    "                    final_model[edge_types[i]],\n",
    "                    testing_true_data_by_edge[edge_types[i]],\n",
    "                    testing_false_data_by_edge[edge_types[i]],\n",
    "                )\n",
    "                test_aucs.append(tmp_auc)\n",
    "                test_f1s.append(tmp_f1)\n",
    "                test_prs.append(tmp_pr)\n",
    "        print(\"valid auc:\", np.mean(valid_aucs))\n",
    "        print(\"valid pr:\", np.mean(valid_prs))\n",
    "        print(\"valid f1:\", np.mean(valid_f1s))\n",
    "\n",
    "        average_auc = np.mean(test_aucs)\n",
    "        average_f1 = np.mean(test_f1s)\n",
    "        average_pr = np.mean(test_prs)\n",
    "\n",
    "        cur_score = np.mean(valid_aucs)\n",
    "        if cur_score > best_score:\n",
    "            best_score = cur_score\n",
    "            test_score = (average_auc, average_f1, average_pr)\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience > args.patience:\n",
    "                print(\"Early Stopping\")\n",
    "                break\n",
    "    return test_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 模型评价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(local_model, node1, node2):\n",
    "    try:\n",
    "        vector1 = local_model[node1]\n",
    "        vector2 = local_model[node2]\n",
    "        return np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, true_edges, false_edges):\n",
    "    true_list = list()\n",
    "    prediction_list = list()\n",
    "    true_num = 0\n",
    "    for edge in true_edges:\n",
    "        tmp_score = get_score(model, str(edge[0]), str(edge[1]))\n",
    "        if tmp_score is not None:\n",
    "            true_list.append(1)\n",
    "            prediction_list.append(tmp_score)\n",
    "            true_num += 1\n",
    "\n",
    "    for edge in false_edges:\n",
    "        tmp_score = get_score(model, str(edge[0]), str(edge[1]))\n",
    "        if tmp_score is not None:\n",
    "            true_list.append(0)\n",
    "            prediction_list.append(tmp_score)\n",
    "\n",
    "    sorted_pred = prediction_list[:]\n",
    "    sorted_pred.sort()\n",
    "    threshold = sorted_pred[-true_num]\n",
    "\n",
    "    y_pred = np.zeros(len(prediction_list), dtype=np.int32)\n",
    "    for i in range(len(prediction_list)):\n",
    "        if prediction_list[i] >= threshold:\n",
    "            y_pred[i] = 1  # 预测输出的结果\n",
    "\n",
    "    y_true = np.array(true_list)  # true label\n",
    "    y_scores = np.array(prediction_list)  # predict proba\n",
    "    ps, rs, _ = precision_recall_curve(y_true, y_scores)\n",
    "    return roc_auc_score(y_true, y_scores), f1_score(y_true, y_pred), auc(rs, ps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 主函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(att_dim=20, batch_size=64, dimensions=200, edge_dim=10, epoch=100, eval_type='all', features='C:\\\\Users\\\\sss\\\\Desktop\\\\GATNE\\\\data\\\\example/feature.txt', input='C:\\\\Users\\\\sss\\\\Desktop\\\\GATNE\\\\data\\\\example', negative_samples=5, neighbor_samples=10, num_walks=20, num_workers=0, patience=5, schema=None, walk_file=None, walk_length=10, window_size=5)\n",
      "We are loading data from: C:\\Users\\sss\\Desktop\\GATNE\\data\\example/train.txt\n",
      "Total training nodes: 511\n",
      "We are loading data from: C:\\Users\\sss\\Desktop\\GATNE\\data\\example/valid.txt\n",
      "We are loading data from: C:\\Users\\sss\\Desktop\\GATNE\\data\\example/test.txt\n",
      "Generating random walks for layer 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of processes must be at least 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-11d9c060ca16>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m     )\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0maverage_auc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage_f1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage_pr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data_by_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_dic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Overall ROC-AUC:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage_auc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-49-7990c1586adf>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(network_data, feature_dic)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeature_dic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex2word\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_pairs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_walks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwalk_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwalk_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;31m# 生成随机游走训练序列和训练语料\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0medge_types\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 边的类别\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-eeb123500182>\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(network_data, num_walks, walk_length, schema, file_name, window_size, num_workers, walk_file)\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mall_walks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_walks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwalk_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# 边的信息; 每个节点游走次数; 游走长度;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mall_walks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_walks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_walks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwalk_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 生成随机游走序列\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0msave_walks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/walks.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_walks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex2word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_walks\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 生成节点index; 降序方式;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-ff952b4dcc34>\u001b[0m in \u001b[0;36mgenerate_walks\u001b[1;34m(network_data, num_walks, walk_length, schema, file_name, num_workers)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mlayer_walker\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRWGraph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_G_from_edges\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# RandomWalk Graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Generating random walks for layer'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mlayer_walks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer_walker\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimulate_walks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_walks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwalk_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 生成随机游走的序列; 每个节点游走次数; 游走长度;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mall_walks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer_walks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-9f79014b3c61>\u001b[0m in \u001b[0;36msimulate_walks\u001b[1;34m(self, num_walks, walk_length, schema)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[1;32mwith\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_workers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitargs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnode_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m                 \u001b[0mall_walks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwalk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwalk_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnode_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_walks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sss\\appdata\\local\\programs\\python\\python36\\lib\\multiprocessing\\context.py\u001b[0m in \u001b[0;36mPool\u001b[1;34m(self, processes, initializer, initargs, maxtasksperchild)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpool\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPool\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         return Pool(processes, initializer, initargs, maxtasksperchild,\n\u001b[1;32m--> 119\u001b[1;33m                     context=self.get_context())\n\u001b[0m\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mRawValue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtypecode_or_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sss\\appdata\\local\\programs\\python\\python36\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, processes, initializer, initargs, maxtasksperchild, context)\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[0mprocesses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    166\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mprocesses\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 167\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Number of processes must be at least 1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minitializer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minitializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Number of processes must be at least 1"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    args.input = r'C:\\Users\\sss\\Desktop\\GATNE\\data\\example'\n",
    "    args.features = r'C:\\Users\\sss\\Desktop\\GATNE\\data\\example/feature.txt'\n",
    "    file_name = args.input\n",
    "    print(args)\n",
    "    if args.features is not None:  # 每个节点对应到的特征; GATNE-T;\n",
    "        feature_dic = load_feature_data(args.features)\n",
    "    else:\n",
    "        feature_dic = None\n",
    "\n",
    "    training_data_by_type = load_training_data(file_name + \"/train.txt\")\n",
    "    valid_true_data_by_edge, valid_false_data_by_edge = load_testing_data(\n",
    "        file_name + \"/valid.txt\"\n",
    "    )\n",
    "    testing_true_data_by_edge, testing_false_data_by_edge = load_testing_data(\n",
    "        file_name + \"/test.txt\"\n",
    "    )\n",
    "\n",
    "    average_auc, average_f1, average_pr = train_model(training_data_by_type, feature_dic)\n",
    "\n",
    "    print(\"Overall ROC-AUC:\", average_auc)\n",
    "    print(\"Overall PR-AUC\", average_pr)\n",
    "    print(\"Overall F1:\", average_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
