{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import dgl\n",
    "import errno\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import argparse\n",
    "\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from scipy import sparse\n",
    "from scipy import io as sio\n",
    "from pprint import pprint\n",
    "from sklearn.metrics import f1_score\n",
    "from dgl.data.utils import download, get_download_dir, _get_dgl_url\n",
    "from dgl.nn.pytorch import GATConv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 部分参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The configuration below is from the paper.\n",
    "default_configure = {\n",
    "    'lr': 0.005,             # Learning rate\n",
    "    'num_heads': [8],        # Number of attention heads for node-level attention\n",
    "    'hidden_units': 8,\n",
    "    'dropout': 0.6,\n",
    "    'weight_decay': 0.001,\n",
    "    'num_epochs': 200,\n",
    "    'patience': 100\n",
    "}\n",
    "\n",
    "sampling_configure = {\n",
    "    'batch_size': 20\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup(args):\n",
    "    args.update(default_configure)\n",
    "    set_random_seed(args['seed'])\n",
    "    args['dataset'] = 'ACMRaw' if args['hetero'] else 'ACM'\n",
    "    args['device'] = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_for_sampling(args):\n",
    "    args.update(default_configure)\n",
    "    args.update(sampling_configure)\n",
    "    set_random_seed()\n",
    "    args['device'] = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 随机种子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 表可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pd_show(data):\n",
    "    data = np.array(data)\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_binary_mask(total_size, indices):\n",
    "    mask = torch.zeros(total_size)\n",
    "    mask[indices] = 1\n",
    "    return mask.byte()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 图可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nauthor_g 为邻接矩阵\\nG = author_g.to_networkx()\\nimg(G)\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def img(graph):\n",
    "    nx.draw(graph, with_labels=False)\n",
    "    plt.show()\n",
    "    \n",
    "\"\"\"\n",
    "author_g 为邻接矩阵\n",
    "G = author_g.to_networkx()\n",
    "img(G)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最有模型保存和使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping(object):\n",
    "    def __init__(self, patience=10):\n",
    "        dt = datetime.datetime.now()\n",
    "        self.filename = 'early_stop_{}_{:02d}-{:02d}-{:02d}.pth'.format(dt.date(), dt.hour, dt.minute, dt.second)\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_acc = None\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def step(self, loss, acc, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_acc = acc\n",
    "            self.best_loss = loss\n",
    "            self.save_checkpoint(model)\n",
    "        elif (loss > self.best_loss) and (acc < self.best_acc):\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            if (loss <= self.best_loss) and (acc >= self.best_acc):\n",
    "                self.save_checkpoint(model)\n",
    "            self.best_loss = np.min((loss, self.best_loss))\n",
    "            self.best_acc = np.max((acc, self.best_acc))\n",
    "            self.counter = 0\n",
    "        return self.early_stop\n",
    "\n",
    "    def save_checkpoint(self, model):\n",
    "        \"\"\"Saves model when validation loss decreases.\"\"\"\n",
    "        torch.save(model.state_dict(), self.filename)\n",
    "\n",
    "    def load_checkpoint(self, model):\n",
    "        \"\"\"Load the latest checkpoint.\"\"\"\n",
    "        model.load_state_dict(torch.load(self.filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据（二选一）：ACM or ACMRaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_acm(remove_self_loop):\n",
    "    url = 'dataset/ACM3025.pkl'\n",
    "    data_path = get_download_dir() + '/ACM3025.pkl'\n",
    "    # download(_get_dgl_url(url), path=data_path) # 数据下载\n",
    "\n",
    "    with open(data_path, 'rb') as f:  # 导入data数据。dict_keys(['label', 'feature', 'PAP', 'PLP', 'train_idx', 'val_idx', 'test_idx'])\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    # 点标签 和 点特征\n",
    "    labels, features = torch.from_numpy(data['label'].todense()).long(), \\\n",
    "                       torch.from_numpy(data['feature'].todense()).float()\n",
    "    num_classes = labels.shape[1]\n",
    "    labels = labels.nonzero()[:, 1]  # 将label的one-hot转换成类别\n",
    "\n",
    "    # 去环\n",
    "    if remove_self_loop:\n",
    "        num_nodes = data['label'].shape[0]\n",
    "        data['PAP'] = sparse.csr_matrix(data['PAP'] - np.eye(num_nodes))\n",
    "        data['PLP'] = sparse.csr_matrix(data['PLP'] - np.eye(num_nodes))\n",
    "\n",
    "    # 点 形成邻接矩阵\n",
    "    # Adjacency matrices for meta path based neighbors\n",
    "    # (Mufei): I verified both of them are binary adjacency matrices with self loops\n",
    "    author_g = dgl.from_scipy(data['PAP'])   # 定义p-a-p的meta-path; # 建立dgl格式的graph\n",
    "    subject_g = dgl.from_scipy(data['PLP'])  # 定义p-s-p的meta-path\n",
    "    gs = [author_g, subject_g]  # 将两个meta-path形成的图组合在一起\n",
    "\n",
    "    train_idx = torch.from_numpy(data['train_idx']).long().squeeze(0)\n",
    "    val_idx = torch.from_numpy(data['val_idx']).long().squeeze(0)\n",
    "    test_idx = torch.from_numpy(data['test_idx']).long().squeeze(0)\n",
    "\n",
    "    num_nodes = author_g.number_of_nodes()  # 节点数量\n",
    "    train_mask = get_binary_mask(num_nodes, train_idx)  # 对应位置上的节点设置为1，其余位置为0\n",
    "    val_mask = get_binary_mask(num_nodes, val_idx)\n",
    "    test_mask = get_binary_mask(num_nodes, test_idx)\n",
    "\n",
    "    print('dataset loaded')\n",
    "    pprint({\n",
    "        'dataset': 'ACM',\n",
    "        'train': train_mask.sum().item() / num_nodes,\n",
    "        'val': val_mask.sum().item() / num_nodes,\n",
    "        'test': test_mask.sum().item() / num_nodes\n",
    "    })\n",
    "    \n",
    "    # Returns:\n",
    "    # gs - PAP,PSP下的图; fetures - 节点特征; labels:labels; num_classes:label数量\n",
    "    return gs, features, labels, num_classes, train_idx, val_idx, test_idx, train_mask, val_mask, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_acm_raw(remove_self_loop):\n",
    "    assert not remove_self_loop\n",
    "    url = 'dataset/ACM.mat'\n",
    "    data_path = get_download_dir() + '/ACM.mat'\n",
    "    # download(_get_dgl_url(url), path=data_path)\n",
    "    \n",
    "    # 数据没有定义meta-path的邻居; 这里通过边，自定义metapath的点边关系。\n",
    "    data = sio.loadmat(data_path)\n",
    "    p_vs_l = data['PvsL']       # paper-field?\n",
    "    p_vs_a = data['PvsA']       # paper-author\n",
    "    p_vs_t = data['PvsT']       # paper-term, bag of words\n",
    "    p_vs_c = data['PvsC']       # paper-conference, labels come from that\n",
    "\n",
    "    # We assign\n",
    "    # (1) KDD papers as class 0 (data mining),\n",
    "    # (2) SIGMOD and VLDB papers as class 1 (database),\n",
    "    # (3) SIGCOMM and MOBICOMM papers as class 2 (communication)\n",
    "    conf_ids = [0, 1, 9, 10, 13]  # 选择这5个会议的数据\n",
    "    label_ids = [0, 1, 2, 2, 1]  # 分别标记为不同的labels\n",
    "\n",
    "    p_vs_c_filter = p_vs_c[:, conf_ids]   # paper-conference; 选择对应到的paper\n",
    "    p_selected = (p_vs_c_filter.sum(1) != 0).A1.nonzero()[0]   # 不发表在寻找会议中的文章，将其去除\n",
    "    p_vs_l = p_vs_l[p_selected]  # 选择对应的节点\n",
    "    p_vs_a = p_vs_a[p_selected]\n",
    "    p_vs_t = p_vs_t[p_selected]\n",
    "    p_vs_c = p_vs_c[p_selected]\n",
    "\n",
    "    '''\n",
    "    构建异构图\n",
    "    >>> data_dict = {\n",
    "    ...     ('user', 'follows', 'user'): (torch.tensor([0, 1]), torch.tensor([1, 2])),\n",
    "    ...     ('user', 'follows', 'topic'): (torch.tensor([1, 1]), torch.tensor([1, 2])),\n",
    "    ...     ('user', 'plays', 'game'): (torch.tensor([0, 3]), torch.tensor([3, 4]))\n",
    "    ... }\n",
    "    >>> g = dgl.heterograph(data_dict)\n",
    "    '''\n",
    "    # metapah:pa; pf\n",
    "    hg = dgl.heterograph({\n",
    "        ('paper', 'pa', 'author'): p_vs_a.nonzero(),   # paper-author构成边，关系='pa'\n",
    "        ('author', 'ap', 'paper'): p_vs_a.transpose().nonzero(),\n",
    "        ('paper', 'pf', 'field'): p_vs_l.nonzero(),\n",
    "        ('field', 'fp', 'paper'): p_vs_l.transpose().nonzero()\n",
    "    })\n",
    "    # p_vs_t: 节点特征\n",
    "    features = torch.FloatTensor(p_vs_t.toarray())\n",
    "    # p_vs_c: labels\n",
    "    pc_p, pc_c = p_vs_c.nonzero()\n",
    "    labels = np.zeros(len(p_selected), dtype=np.int64)  # 定义labels\n",
    "    for conf_id, label_id in zip(conf_ids, label_ids):  # 将label转换成\n",
    "        labels[pc_p[pc_c == conf_id]] = label_id\n",
    "    labels = torch.LongTensor(labels)\n",
    "\n",
    "    num_classes = 3\n",
    "    # trian,val,test\n",
    "    float_mask = np.zeros(len(pc_p))  # 节点数量\n",
    "    for conf_id in conf_ids:  # 对每个类别生层[0,1]之间等距的值; 目的是对每个类别按比例去train，val，test\n",
    "        pc_c_mask = (pc_c == conf_id)\n",
    "        float_mask[pc_c_mask] = np.random.permutation(np.linspace(0, 1, pc_c_mask.sum()))  # 每个节点对应的类别生成随机数，train，val使用\n",
    "    train_idx = np.where(float_mask <= 0.2)[0]\n",
    "    val_idx = np.where((float_mask > 0.2) & (float_mask <= 0.3))[0]\n",
    "    test_idx = np.where(float_mask > 0.3)[0]\n",
    "\n",
    "    num_nodes = hg.number_of_nodes('paper')   # 节点数量\n",
    "    train_mask = get_binary_mask(num_nodes, train_idx)  # 对训练集mask\n",
    "    val_mask = get_binary_mask(num_nodes, val_idx)\n",
    "    test_mask = get_binary_mask(num_nodes, test_idx)\n",
    "    # hg:异构图; features:节点特征; labels:labels; num_classes:类别数量\n",
    "    return hg, features, labels, num_classes, train_idx, val_idx, test_idx, train_mask, val_mask, test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(dataset, remove_self_loop=False):\n",
    "    if dataset == 'ACM':\n",
    "        return load_acm(remove_self_loop)\n",
    "    elif dataset == 'ACMRaw':\n",
    "        return load_acm_raw(remove_self_loop)\n",
    "    else:\n",
    "        return NotImplementedError('Unsupported dataset {}'.format(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 配置模型（二选一）：框架现存HAN or 原生HAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticAttention(nn.Module):\n",
    "    def __init__(self, in_size, hidden_size=128):\n",
    "        super(SemanticAttention, self).__init__()\n",
    "        # input:[Node, metapath, in_size]; output:[None, metapath, 1]; 所有节点在每个meta-path上的重要性值\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Linear(in_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        w = self.project(z).mean(0)    # 每个节点在metapath维度的均值; mean(0): 每个meta-path上的均值(/|V|); (MetaPath, 1)\n",
    "        beta = torch.softmax(w, dim=0)       # 归一化          # (M, 1)\n",
    "        beta = beta.expand((z.shape[0],) + beta.shape) #  拓展到N个节点上的metapath的值   (N, M, 1)\n",
    "\n",
    "        return (beta * z).sum(1)     #  (beta * z)=>所有节点，在metapath上的attention值;    (beta * z).sum(1)=>节点最终的值      (N, D * K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 框架现存HAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HANLayer(nn.Module):\n",
    "    def __init__(self, num_meta_paths, in_size, out_size, layer_num_heads, dropout):\n",
    "        super(HANLayer, self).__init__()\n",
    "\n",
    "        # One GAT layer for each meta path based adjacency matrix\n",
    "        self.gat_layers = nn.ModuleList()\n",
    "        for i in range(num_meta_paths):  # meta-path Layers; 两个meta-path的维度是一致的\n",
    "            self.gat_layers.append(GATConv(in_size, out_size, layer_num_heads,\n",
    "                                           dropout, dropout, activation=F.elu))\n",
    "        self.semantic_attention = SemanticAttention(in_size=out_size * layer_num_heads)  # 语义attention; out-size*layers\n",
    "        self.num_meta_paths = num_meta_paths\n",
    "\n",
    "    def forward(self, gs, h):\n",
    "        semantic_embeddings = []  # 语义级别的embeddings\n",
    "        # 每个meta-path下对应到的节点级别的attention layer\n",
    "        for i, g in enumerate(gs):  # 每个meta-path的图信息，求节点的attention. 两个GAT; gat_layers[i](g, h) g:图; h:features => [3025, 8, 8] => [3025, 64]\n",
    "            semantic_embeddings.append(self.gat_layers[i](g, h).flatten(1))  # 两个GAT; gat_layers[i](g, h) 每个metapath对应一个GAT\n",
    "        semantic_embeddings = torch.stack(semantic_embeddings, dim=1)   # (N, M, D * K) 每个节点对应到metapath下的每个节点的embedding值（Node Attention）\n",
    "        # 聚合meta-path下，每个节点最终的输出值\n",
    "        return self.semantic_attention(semantic_embeddings)                            # (N, D * K)\n",
    "\n",
    "class HAN(nn.Module):\n",
    "    def __init__(self, num_meta_paths, in_size, hidden_size, out_size, num_heads, dropout):\n",
    "        super(HAN, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(HANLayer(num_meta_paths, in_size, hidden_size, num_heads[0], dropout)) # meta-path数量 + semantic_attention\n",
    "        for l in range(1, len(num_heads)): # 多层多头，目前是没有\n",
    "            self.layers.append(HANLayer(num_meta_paths, hidden_size * num_heads[l-1],\n",
    "                                        hidden_size, num_heads[l], dropout))\n",
    "        self.predict = nn.Linear(hidden_size * num_heads[-1], out_size)  # hidden*heads, classes; HAN->classes\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        for gnn in self.layers:  # GAT-GAT 节点级别的GAT; semantic_attention语义级别attention;\n",
    "            h = gnn(g, h)  # HANLayer\n",
    "            # 输出的是：节点, meta-path数量, embedding; Returns:节点HAN后输出的embedding\n",
    "        return self.predict(h)  # HAN输出节点embedding后接一个Linear层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 原生HAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Or_HANLayer(nn.Module):\n",
    "    def __init__(self, meta_paths, in_size, out_size, layer_num_heads, dropout):\n",
    "        super(Or_HANLayer, self).__init__()\n",
    "\n",
    "        # One GAT layer for each meta path based adjacency matrix\n",
    "        self.gat_layers = nn.ModuleList()\n",
    "        for i in range(len(meta_paths)):\n",
    "            self.gat_layers.append(GATConv(in_size, out_size, layer_num_heads,\n",
    "                                           dropout, dropout, activation=F.elu,\n",
    "                                           allow_zero_in_degree=True))\n",
    "        self.semantic_attention = SemanticAttention(in_size=out_size * layer_num_heads)\n",
    "        self.meta_paths = list(tuple(meta_path) for meta_path in meta_paths)  # 将meta-path转换成元组形式\n",
    "\n",
    "        self._cached_graph = None\n",
    "        self._cached_coalesced_graph = {}\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        semantic_embeddings = []\n",
    "\n",
    "        if self._cached_graph is None or self._cached_graph is not g:  # 第一次，建立一张metapath下的异构图\n",
    "            self._cached_graph = g\n",
    "            self._cached_coalesced_graph.clear()\n",
    "            for meta_path in self.meta_paths:\n",
    "                self._cached_coalesced_graph[meta_path] = dgl.metapath_reachable_graph(\n",
    "                        g, meta_path)  # 构建异构图的邻居;\n",
    "        # self._cached_coalesced_graph 多个metapath下的异构图\n",
    "        for i, meta_path in enumerate(self.meta_paths):\n",
    "            new_g = self._cached_coalesced_graph[meta_path]  # meta-path下的节点邻居图\n",
    "            semantic_embeddings.append(self.gat_layers[i](new_g, h).flatten(1))   # 图attention\n",
    "        semantic_embeddings = torch.stack(semantic_embeddings, dim=1)                  # (N, M, D * K)\n",
    "\n",
    "        return self.semantic_attention(semantic_embeddings)                            # (N, D * K)\n",
    "\n",
    "class Or_HAN(nn.Module):\n",
    "    def __init__(self, meta_paths, in_size, hidden_size, out_size, num_heads, dropout):\n",
    "        super(Or_HAN, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(Or_HANLayer(meta_paths, in_size, hidden_size, num_heads[0], dropout))\n",
    "        for l in range(1, len(num_heads)):\n",
    "            self.layers.append(Or_HANLayer(meta_paths, hidden_size * num_heads[l-1],\n",
    "                                        hidden_size, num_heads[l], dropout))\n",
    "        self.predict = nn.Linear(hidden_size * num_heads[-1], out_size)\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        for gnn in self.layers:\n",
    "            h = gnn(g, h)\n",
    "\n",
    "        return self.predict(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 新增参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser('HAN')\n",
    "parser.add_argument('-s', '--seed', type=int, default=1, help='Random seed')\n",
    "parser.add_argument('-ld', '--log-dir', type=str, default='results', help='Dir for saving training results')\n",
    "parser.add_argument('--hetero', action='store_true', help='Use metapath coalescing with DGL\\'s own dataset')\n",
    "args = parser.parse_args(args=[]).__dict__\n",
    "args['hetero'] = False  # 自建异构图\n",
    "args = setup(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset loaded\n",
      "{'dataset': 'ACM',\n",
      " 'test': 0.7024793388429752,\n",
      " 'train': 0.19834710743801653,\n",
      " 'val': 0.09917355371900827}\n"
     ]
    }
   ],
   "source": [
    "g, features, labels, num_classes, train_idx, val_idx, test_idx, train_mask, val_mask, test_mask = load_data(args['dataset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(torch, 'BoolTensor'):\n",
    "    train_mask = train_mask.bool()  # 布尔类型转换\n",
    "    val_mask = val_mask.bool()\n",
    "    test_mask = test_mask.bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    features = features.to(args['device'])\n",
    "    labels = labels.to(args['device'])\n",
    "    train_mask = train_mask.to(args['device'])\n",
    "    val_mask = val_mask.to(args['device'])\n",
    "    test_mask = test_mask.to(args['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'seed': 1,\n",
       " 'log_dir': 'results',\n",
       " 'hetero': False,\n",
       " 'lr': 0.005,\n",
       " 'num_heads': [8],\n",
       " 'hidden_units': 8,\n",
       " 'dropout': 0.6,\n",
       " 'weight_decay': 0.001,\n",
       " 'num_epochs': 200,\n",
       " 'patience': 100,\n",
       " 'dataset': 'ACM',\n",
       " 'device': 'cpu'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args['hetero']:   # 构建异构图的邻居节点\n",
    "    # 原生HAN\n",
    "    model = Or_HAN(\n",
    "        meta_paths=[['pa', 'ap'], ['pf', 'fp']],  # 之前构建的边: pa, ap,组合成meta-path: PAP\n",
    "        in_size=features.shape[1],\n",
    "        hidden_size=args['hidden_units'],\n",
    "        out_size=num_classes,\n",
    "        num_heads=args['num_heads'],\n",
    "        dropout=args['dropout']\n",
    "    ).to(args['device'])\n",
    "    g = g.to(args['device'])\n",
    "else:\n",
    "    # 框架现存HAN\n",
    "    model = HAN(\n",
    "        num_meta_paths=len(g),\n",
    "        in_size=features.shape[1],\n",
    "        hidden_size=args['hidden_units'],\n",
    "        out_size=num_classes,\n",
    "        num_heads=args['num_heads'],\n",
    "        dropout=args['dropout']\n",
    "    ).to(args['device'])\n",
    "    g = [graph.to(args['device']) for graph in g]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- layers层是对点进行编码？？？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HAN(\n",
       "  (layers): ModuleList(\n",
       "    (0): HANLayer(\n",
       "      (gat_layers): ModuleList(\n",
       "        (0): GATConv(\n",
       "          (fc): Linear(in_features=1870, out_features=64, bias=False)\n",
       "          (feat_drop): Dropout(p=0.6, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.6, inplace=False)\n",
       "          (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
       "        )\n",
       "        (1): GATConv(\n",
       "          (fc): Linear(in_features=1870, out_features=64, bias=False)\n",
       "          (feat_drop): Dropout(p=0.6, inplace=False)\n",
       "          (attn_drop): Dropout(p=0.6, inplace=False)\n",
       "          (leaky_relu): LeakyReLU(negative_slope=0.2)\n",
       "        )\n",
       "      )\n",
       "      (semantic_attention): SemanticAttention(\n",
       "        (project): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=128, bias=True)\n",
       "          (1): Tanh()\n",
       "          (2): Linear(in_features=128, out_features=1, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (predict): Linear(in_features=64, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopper = EarlyStopping(patience=args['patience'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fcn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 准确率 & 评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(logits, labels):\n",
    "    _, indices = torch.max(logits, dim=1)\n",
    "    prediction = indices.long().cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "\n",
    "    accuracy = (prediction == labels).sum() / len(prediction)\n",
    "    micro_f1 = f1_score(labels, prediction, average='micro')\n",
    "    macro_f1 = f1_score(labels, prediction, average='macro')\n",
    "\n",
    "    return accuracy, micro_f1, macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, g, features, labels, mask, loss_func):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(g, features)\n",
    "    loss = loss_func(logits[mask], labels[mask])\n",
    "    accuracy, micro_f1, macro_f1 = score(logits[mask], labels[mask])\n",
    "\n",
    "    return loss, accuracy, micro_f1, macro_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sss\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss 1.1285 | Train Acc 0.2300 | Train Micro f1 0.2300 | Train Macro f1 0.2224 | Val Loss 1.0002 | Val Micro f1 0.6500 | Val Macro f1 0.5208\n",
      "Epoch 2 | Train Loss 0.9952 | Train Acc 0.6133 | Train Micro f1 0.6133 | Train Macro f1 0.5156 | Val Loss 0.8695 | Val Micro f1 0.9200 | Val Macro f1 0.9205\n",
      "Epoch 3 | Train Loss 0.8612 | Train Acc 0.9050 | Train Micro f1 0.9050 | Train Macro f1 0.9053 | Val Loss 0.7423 | Val Micro f1 0.9400 | Val Macro f1 0.9405\n",
      "Epoch 4 | Train Loss 0.7244 | Train Acc 0.9217 | Train Micro f1 0.9217 | Train Macro f1 0.9218 | Val Loss 0.6140 | Val Micro f1 0.9300 | Val Macro f1 0.9301\n",
      "Epoch 5 | Train Loss 0.5860 | Train Acc 0.9383 | Train Micro f1 0.9383 | Train Macro f1 0.9380 | Val Loss 0.4972 | Val Micro f1 0.9400 | Val Macro f1 0.9397\n",
      "Epoch 6 | Train Loss 0.4704 | Train Acc 0.9433 | Train Micro f1 0.9433 | Train Macro f1 0.9428 | Val Loss 0.3898 | Val Micro f1 0.9533 | Val Macro f1 0.9532\n",
      "Epoch 7 | Train Loss 0.3744 | Train Acc 0.9517 | Train Micro f1 0.9517 | Train Macro f1 0.9515 | Val Loss 0.3116 | Val Micro f1 0.9467 | Val Macro f1 0.9468\n",
      "Epoch 8 | Train Loss 0.3096 | Train Acc 0.9533 | Train Micro f1 0.9533 | Train Macro f1 0.9534 | Val Loss 0.2666 | Val Micro f1 0.9500 | Val Macro f1 0.9503\n",
      "Epoch 9 | Train Loss 0.2402 | Train Acc 0.9483 | Train Micro f1 0.9483 | Train Macro f1 0.9485 | Val Loss 0.2321 | Val Micro f1 0.9533 | Val Macro f1 0.9534\n",
      "Epoch 10 | Train Loss 0.2011 | Train Acc 0.9533 | Train Micro f1 0.9533 | Train Macro f1 0.9535 | Val Loss 0.2058 | Val Micro f1 0.9500 | Val Macro f1 0.9500\n",
      "Epoch 11 | Train Loss 0.1677 | Train Acc 0.9667 | Train Micro f1 0.9667 | Train Macro f1 0.9666 | Val Loss 0.1841 | Val Micro f1 0.9533 | Val Macro f1 0.9533\n",
      "Epoch 12 | Train Loss 0.1514 | Train Acc 0.9583 | Train Micro f1 0.9583 | Train Macro f1 0.9585 | Val Loss 0.1676 | Val Micro f1 0.9533 | Val Macro f1 0.9533\n",
      "Epoch 13 | Train Loss 0.1357 | Train Acc 0.9583 | Train Micro f1 0.9583 | Train Macro f1 0.9583 | Val Loss 0.1539 | Val Micro f1 0.9567 | Val Macro f1 0.9566\n",
      "Epoch 14 | Train Loss 0.1119 | Train Acc 0.9783 | Train Micro f1 0.9783 | Train Macro f1 0.9783 | Val Loss 0.1405 | Val Micro f1 0.9600 | Val Macro f1 0.9601\n",
      "Epoch 15 | Train Loss 0.1214 | Train Acc 0.9650 | Train Micro f1 0.9650 | Train Macro f1 0.9651 | Val Loss 0.1269 | Val Micro f1 0.9500 | Val Macro f1 0.9502\n",
      "Epoch 16 | Train Loss 0.1080 | Train Acc 0.9633 | Train Micro f1 0.9633 | Train Macro f1 0.9635 | Val Loss 0.1161 | Val Micro f1 0.9600 | Val Macro f1 0.9601\n",
      "Epoch 17 | Train Loss 0.0939 | Train Acc 0.9717 | Train Micro f1 0.9717 | Train Macro f1 0.9718 | Val Loss 0.1130 | Val Micro f1 0.9600 | Val Macro f1 0.9601\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch 18 | Train Loss 0.0799 | Train Acc 0.9867 | Train Micro f1 0.9867 | Train Macro f1 0.9867 | Val Loss 0.1148 | Val Micro f1 0.9567 | Val Macro f1 0.9567\n",
      "Epoch 19 | Train Loss 0.0901 | Train Acc 0.9800 | Train Micro f1 0.9800 | Train Macro f1 0.9799 | Val Loss 0.1112 | Val Micro f1 0.9600 | Val Macro f1 0.9599\n",
      "Epoch 20 | Train Loss 0.0850 | Train Acc 0.9750 | Train Micro f1 0.9750 | Train Macro f1 0.9750 | Val Loss 0.1022 | Val Micro f1 0.9600 | Val Macro f1 0.9600\n",
      "Epoch 21 | Train Loss 0.0741 | Train Acc 0.9850 | Train Micro f1 0.9850 | Train Macro f1 0.9850 | Val Loss 0.0887 | Val Micro f1 0.9700 | Val Macro f1 0.9699\n",
      "Epoch 22 | Train Loss 0.0548 | Train Acc 0.9917 | Train Micro f1 0.9917 | Train Macro f1 0.9917 | Val Loss 0.0836 | Val Micro f1 0.9733 | Val Macro f1 0.9733\n",
      "EarlyStopping counter: 1 out of 100\n",
      "Epoch 23 | Train Loss 0.0628 | Train Acc 0.9817 | Train Micro f1 0.9817 | Train Macro f1 0.9817 | Val Loss 0.0841 | Val Micro f1 0.9667 | Val Macro f1 0.9667\n",
      "EarlyStopping counter: 2 out of 100\n",
      "Epoch 24 | Train Loss 0.0524 | Train Acc 0.9867 | Train Micro f1 0.9867 | Train Macro f1 0.9867 | Val Loss 0.0861 | Val Micro f1 0.9633 | Val Macro f1 0.9633\n",
      "EarlyStopping counter: 3 out of 100\n",
      "Epoch 25 | Train Loss 0.0541 | Train Acc 0.9900 | Train Micro f1 0.9900 | Train Macro f1 0.9900 | Val Loss 0.0892 | Val Micro f1 0.9700 | Val Macro f1 0.9700\n",
      "EarlyStopping counter: 4 out of 100\n",
      "Epoch 26 | Train Loss 0.0582 | Train Acc 0.9850 | Train Micro f1 0.9850 | Train Macro f1 0.9850 | Val Loss 0.0932 | Val Micro f1 0.9700 | Val Macro f1 0.9699\n",
      "EarlyStopping counter: 5 out of 100\n",
      "Epoch 27 | Train Loss 0.0458 | Train Acc 0.9867 | Train Micro f1 0.9867 | Train Macro f1 0.9867 | Val Loss 0.0954 | Val Micro f1 0.9667 | Val Macro f1 0.9666\n",
      "EarlyStopping counter: 6 out of 100\n",
      "Epoch 28 | Train Loss 0.0344 | Train Acc 0.9900 | Train Micro f1 0.9900 | Train Macro f1 0.9900 | Val Loss 0.0983 | Val Micro f1 0.9667 | Val Macro f1 0.9666\n",
      "EarlyStopping counter: 7 out of 100\n",
      "Epoch 29 | Train Loss 0.0461 | Train Acc 0.9867 | Train Micro f1 0.9867 | Train Macro f1 0.9867 | Val Loss 0.0988 | Val Micro f1 0.9667 | Val Macro f1 0.9667\n",
      "EarlyStopping counter: 8 out of 100\n",
      "Epoch 30 | Train Loss 0.0388 | Train Acc 0.9950 | Train Micro f1 0.9950 | Train Macro f1 0.9950 | Val Loss 0.0993 | Val Micro f1 0.9633 | Val Macro f1 0.9633\n",
      "EarlyStopping counter: 9 out of 100\n",
      "Epoch 31 | Train Loss 0.0428 | Train Acc 0.9883 | Train Micro f1 0.9883 | Train Macro f1 0.9883 | Val Loss 0.1009 | Val Micro f1 0.9633 | Val Macro f1 0.9633\n",
      "EarlyStopping counter: 10 out of 100\n",
      "Epoch 32 | Train Loss 0.0432 | Train Acc 0.9883 | Train Micro f1 0.9883 | Train Macro f1 0.9883 | Val Loss 0.0980 | Val Micro f1 0.9633 | Val Macro f1 0.9633\n",
      "EarlyStopping counter: 11 out of 100\n",
      "Epoch 33 | Train Loss 0.0378 | Train Acc 0.9850 | Train Micro f1 0.9850 | Train Macro f1 0.9850 | Val Loss 0.0983 | Val Micro f1 0.9633 | Val Macro f1 0.9634\n",
      "EarlyStopping counter: 12 out of 100\n",
      "Epoch 34 | Train Loss 0.0412 | Train Acc 0.9900 | Train Micro f1 0.9900 | Train Macro f1 0.9900 | Val Loss 0.0979 | Val Micro f1 0.9633 | Val Macro f1 0.9633\n",
      "EarlyStopping counter: 13 out of 100\n",
      "Epoch 35 | Train Loss 0.0325 | Train Acc 0.9933 | Train Micro f1 0.9933 | Train Macro f1 0.9933 | Val Loss 0.0973 | Val Micro f1 0.9633 | Val Macro f1 0.9633\n",
      "EarlyStopping counter: 14 out of 100\n",
      "Epoch 36 | Train Loss 0.0339 | Train Acc 0.9883 | Train Micro f1 0.9883 | Train Macro f1 0.9883 | Val Loss 0.0953 | Val Micro f1 0.9633 | Val Macro f1 0.9633\n",
      "EarlyStopping counter: 15 out of 100\n",
      "Epoch 37 | Train Loss 0.0327 | Train Acc 0.9983 | Train Micro f1 0.9983 | Train Macro f1 0.9983 | Val Loss 0.0955 | Val Micro f1 0.9633 | Val Macro f1 0.9633\n",
      "EarlyStopping counter: 16 out of 100\n",
      "Epoch 38 | Train Loss 0.0418 | Train Acc 0.9867 | Train Micro f1 0.9867 | Train Macro f1 0.9867 | Val Loss 0.0910 | Val Micro f1 0.9700 | Val Macro f1 0.9699\n",
      "EarlyStopping counter: 17 out of 100\n",
      "Epoch 39 | Train Loss 0.0324 | Train Acc 0.9933 | Train Micro f1 0.9933 | Train Macro f1 0.9933 | Val Loss 0.0890 | Val Micro f1 0.9633 | Val Macro f1 0.9633\n",
      "EarlyStopping counter: 18 out of 100\n",
      "Epoch 40 | Train Loss 0.0416 | Train Acc 0.9917 | Train Micro f1 0.9917 | Train Macro f1 0.9917 | Val Loss 0.0900 | Val Micro f1 0.9633 | Val Macro f1 0.9633\n",
      "EarlyStopping counter: 19 out of 100\n",
      "Epoch 41 | Train Loss 0.0381 | Train Acc 0.9917 | Train Micro f1 0.9917 | Train Macro f1 0.9917 | Val Loss 0.0972 | Val Micro f1 0.9633 | Val Macro f1 0.9633\n",
      "EarlyStopping counter: 20 out of 100\n",
      "Epoch 42 | Train Loss 0.0317 | Train Acc 0.9950 | Train Micro f1 0.9950 | Train Macro f1 0.9950 | Val Loss 0.1107 | Val Micro f1 0.9533 | Val Macro f1 0.9535\n",
      "EarlyStopping counter: 21 out of 100\n",
      "Epoch 43 | Train Loss 0.0324 | Train Acc 0.9950 | Train Micro f1 0.9950 | Train Macro f1 0.9950 | Val Loss 0.1242 | Val Micro f1 0.9467 | Val Macro f1 0.9468\n",
      "EarlyStopping counter: 22 out of 100\n",
      "Epoch 44 | Train Loss 0.0425 | Train Acc 0.9917 | Train Micro f1 0.9917 | Train Macro f1 0.9917 | Val Loss 0.1170 | Val Micro f1 0.9533 | Val Macro f1 0.9535\n",
      "EarlyStopping counter: 23 out of 100\n",
      "Epoch 45 | Train Loss 0.0341 | Train Acc 0.9950 | Train Micro f1 0.9950 | Train Macro f1 0.9950 | Val Loss 0.1074 | Val Micro f1 0.9567 | Val Macro f1 0.9566\n",
      "EarlyStopping counter: 24 out of 100\n",
      "Epoch 46 | Train Loss 0.0383 | Train Acc 0.9933 | Train Micro f1 0.9933 | Train Macro f1 0.9933 | Val Loss 0.1020 | Val Micro f1 0.9600 | Val Macro f1 0.9599\n",
      "EarlyStopping counter: 25 out of 100\n",
      "Epoch 47 | Train Loss 0.0401 | Train Acc 0.9950 | Train Micro f1 0.9950 | Train Macro f1 0.9950 | Val Loss 0.0992 | Val Micro f1 0.9667 | Val Macro f1 0.9666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 26 out of 100\n",
      "Epoch 48 | Train Loss 0.0455 | Train Acc 0.9883 | Train Micro f1 0.9883 | Train Macro f1 0.9883 | Val Loss 0.0999 | Val Micro f1 0.9700 | Val Macro f1 0.9700\n",
      "EarlyStopping counter: 27 out of 100\n",
      "Epoch 49 | Train Loss 0.0363 | Train Acc 0.9883 | Train Micro f1 0.9883 | Train Macro f1 0.9883 | Val Loss 0.1042 | Val Micro f1 0.9633 | Val Macro f1 0.9633\n",
      "EarlyStopping counter: 28 out of 100\n",
      "Epoch 50 | Train Loss 0.0450 | Train Acc 0.9867 | Train Micro f1 0.9867 | Train Macro f1 0.9867 | Val Loss 0.1143 | Val Micro f1 0.9667 | Val Macro f1 0.9666\n",
      "EarlyStopping counter: 29 out of 100\n",
      "Epoch 51 | Train Loss 0.0359 | Train Acc 0.9933 | Train Micro f1 0.9933 | Train Macro f1 0.9933 | Val Loss 0.1286 | Val Micro f1 0.9533 | Val Macro f1 0.9533\n",
      "EarlyStopping counter: 30 out of 100\n",
      "Epoch 52 | Train Loss 0.0317 | Train Acc 0.9917 | Train Micro f1 0.9917 | Train Macro f1 0.9917 | Val Loss 0.1286 | Val Micro f1 0.9533 | Val Macro f1 0.9533\n",
      "EarlyStopping counter: 31 out of 100\n",
      "Epoch 53 | Train Loss 0.0331 | Train Acc 0.9933 | Train Micro f1 0.9933 | Train Macro f1 0.9933 | Val Loss 0.1189 | Val Micro f1 0.9533 | Val Macro f1 0.9533\n",
      "EarlyStopping counter: 32 out of 100\n",
      "Epoch 54 | Train Loss 0.0302 | Train Acc 0.9933 | Train Micro f1 0.9933 | Train Macro f1 0.9933 | Val Loss 0.1104 | Val Micro f1 0.9600 | Val Macro f1 0.9599\n",
      "EarlyStopping counter: 33 out of 100\n",
      "Epoch 55 | Train Loss 0.0355 | Train Acc 0.9950 | Train Micro f1 0.9950 | Train Macro f1 0.9950 | Val Loss 0.1066 | Val Micro f1 0.9633 | Val Macro f1 0.9633\n",
      "EarlyStopping counter: 34 out of 100\n",
      "Epoch 56 | Train Loss 0.0364 | Train Acc 0.9950 | Train Micro f1 0.9950 | Train Macro f1 0.9950 | Val Loss 0.1063 | Val Micro f1 0.9700 | Val Macro f1 0.9700\n",
      "EarlyStopping counter: 35 out of 100\n",
      "Epoch 57 | Train Loss 0.0344 | Train Acc 0.9933 | Train Micro f1 0.9933 | Train Macro f1 0.9933 | Val Loss 0.1179 | Val Micro f1 0.9533 | Val Macro f1 0.9534\n",
      "EarlyStopping counter: 36 out of 100\n",
      "Epoch 58 | Train Loss 0.0405 | Train Acc 0.9883 | Train Micro f1 0.9883 | Train Macro f1 0.9883 | Val Loss 0.1362 | Val Micro f1 0.9500 | Val Macro f1 0.9501\n",
      "EarlyStopping counter: 37 out of 100\n",
      "Epoch 59 | Train Loss 0.0332 | Train Acc 0.9967 | Train Micro f1 0.9967 | Train Macro f1 0.9967 | Val Loss 0.1316 | Val Micro f1 0.9500 | Val Macro f1 0.9500\n",
      "EarlyStopping counter: 38 out of 100\n",
      "Epoch 60 | Train Loss 0.0466 | Train Acc 0.9900 | Train Micro f1 0.9900 | Train Macro f1 0.9900 | Val Loss 0.1106 | Val Micro f1 0.9567 | Val Macro f1 0.9566\n",
      "EarlyStopping counter: 39 out of 100\n",
      "Epoch 61 | Train Loss 0.0375 | Train Acc 0.9917 | Train Micro f1 0.9917 | Train Macro f1 0.9917 | Val Loss 0.0965 | Val Micro f1 0.9700 | Val Macro f1 0.9699\n",
      "EarlyStopping counter: 40 out of 100\n",
      "Epoch 62 | Train Loss 0.0396 | Train Acc 0.9900 | Train Micro f1 0.9900 | Train Macro f1 0.9900 | Val Loss 0.0948 | Val Micro f1 0.9667 | Val Macro f1 0.9667\n",
      "EarlyStopping counter: 41 out of 100\n",
      "Epoch 63 | Train Loss 0.0298 | Train Acc 0.9983 | Train Micro f1 0.9983 | Train Macro f1 0.9983 | Val Loss 0.0967 | Val Micro f1 0.9633 | Val Macro f1 0.9634\n",
      "EarlyStopping counter: 42 out of 100\n",
      "Epoch 64 | Train Loss 0.0416 | Train Acc 0.9933 | Train Micro f1 0.9933 | Train Macro f1 0.9933 | Val Loss 0.1027 | Val Micro f1 0.9600 | Val Macro f1 0.9599\n",
      "EarlyStopping counter: 43 out of 100\n",
      "Epoch 65 | Train Loss 0.0320 | Train Acc 0.9917 | Train Micro f1 0.9917 | Train Macro f1 0.9917 | Val Loss 0.1258 | Val Micro f1 0.9533 | Val Macro f1 0.9533\n",
      "EarlyStopping counter: 44 out of 100\n",
      "Epoch 66 | Train Loss 0.0332 | Train Acc 0.9917 | Train Micro f1 0.9917 | Train Macro f1 0.9917 | Val Loss 0.1567 | Val Micro f1 0.9500 | Val Macro f1 0.9500\n",
      "EarlyStopping counter: 45 out of 100\n",
      "Epoch 67 | Train Loss 0.0394 | Train Acc 0.9900 | Train Micro f1 0.9900 | Train Macro f1 0.9900 | Val Loss 0.1527 | Val Micro f1 0.9500 | Val Macro f1 0.9500\n",
      "EarlyStopping counter: 46 out of 100\n",
      "Epoch 68 | Train Loss 0.0428 | Train Acc 0.9900 | Train Micro f1 0.9900 | Train Macro f1 0.9900 | Val Loss 0.1210 | Val Micro f1 0.9500 | Val Macro f1 0.9500\n",
      "EarlyStopping counter: 47 out of 100\n",
      "Epoch 69 | Train Loss 0.0261 | Train Acc 0.9983 | Train Micro f1 0.9983 | Train Macro f1 0.9983 | Val Loss 0.1001 | Val Micro f1 0.9633 | Val Macro f1 0.9633\n",
      "EarlyStopping counter: 48 out of 100\n",
      "Epoch 70 | Train Loss 0.0311 | Train Acc 0.9967 | Train Micro f1 0.9967 | Train Macro f1 0.9967 | Val Loss 0.0948 | Val Micro f1 0.9667 | Val Macro f1 0.9667\n",
      "EarlyStopping counter: 49 out of 100\n",
      "Epoch 71 | Train Loss 0.0350 | Train Acc 0.9950 | Train Micro f1 0.9950 | Train Macro f1 0.9950 | Val Loss 0.0950 | Val Micro f1 0.9667 | Val Macro f1 0.9667\n",
      "EarlyStopping counter: 50 out of 100\n",
      "Epoch 72 | Train Loss 0.0367 | Train Acc 0.9900 | Train Micro f1 0.9900 | Train Macro f1 0.9900 | Val Loss 0.1034 | Val Micro f1 0.9567 | Val Macro f1 0.9567\n",
      "EarlyStopping counter: 51 out of 100\n",
      "Epoch 73 | Train Loss 0.0313 | Train Acc 0.9933 | Train Micro f1 0.9933 | Train Macro f1 0.9933 | Val Loss 0.1120 | Val Micro f1 0.9567 | Val Macro f1 0.9567\n",
      "EarlyStopping counter: 52 out of 100\n",
      "Epoch 74 | Train Loss 0.0269 | Train Acc 0.9967 | Train Micro f1 0.9967 | Train Macro f1 0.9967 | Val Loss 0.1200 | Val Micro f1 0.9533 | Val Macro f1 0.9534\n",
      "EarlyStopping counter: 53 out of 100\n",
      "Epoch 75 | Train Loss 0.0285 | Train Acc 0.9950 | Train Micro f1 0.9950 | Train Macro f1 0.9950 | Val Loss 0.1179 | Val Micro f1 0.9500 | Val Macro f1 0.9500\n",
      "EarlyStopping counter: 54 out of 100\n",
      "Epoch 76 | Train Loss 0.0312 | Train Acc 0.9967 | Train Micro f1 0.9967 | Train Macro f1 0.9967 | Val Loss 0.1061 | Val Micro f1 0.9567 | Val Macro f1 0.9567\n",
      "EarlyStopping counter: 55 out of 100\n",
      "Epoch 77 | Train Loss 0.0329 | Train Acc 0.9917 | Train Micro f1 0.9917 | Train Macro f1 0.9917 | Val Loss 0.0994 | Val Micro f1 0.9600 | Val Macro f1 0.9600\n",
      "EarlyStopping counter: 56 out of 100\n",
      "Epoch 78 | Train Loss 0.0236 | Train Acc 0.9967 | Train Micro f1 0.9967 | Train Macro f1 0.9967 | Val Loss 0.0974 | Val Micro f1 0.9633 | Val Macro f1 0.9634\n",
      "EarlyStopping counter: 57 out of 100\n",
      "Epoch 79 | Train Loss 0.0342 | Train Acc 0.9917 | Train Micro f1 0.9917 | Train Macro f1 0.9917 | Val Loss 0.0993 | Val Micro f1 0.9567 | Val Macro f1 0.9567\n",
      "EarlyStopping counter: 58 out of 100\n",
      "Epoch 80 | Train Loss 0.0320 | Train Acc 0.9933 | Train Micro f1 0.9933 | Train Macro f1 0.9933 | Val Loss 0.0972 | Val Micro f1 0.9567 | Val Macro f1 0.9567\n",
      "EarlyStopping counter: 59 out of 100\n",
      "Epoch 81 | Train Loss 0.0311 | Train Acc 0.9900 | Train Micro f1 0.9900 | Train Macro f1 0.9900 | Val Loss 0.1000 | Val Micro f1 0.9567 | Val Macro f1 0.9567\n",
      "EarlyStopping counter: 60 out of 100\n",
      "Epoch 82 | Train Loss 0.0289 | Train Acc 0.9950 | Train Micro f1 0.9950 | Train Macro f1 0.9950 | Val Loss 0.1077 | Val Micro f1 0.9567 | Val Macro f1 0.9566\n",
      "EarlyStopping counter: 61 out of 100\n",
      "Epoch 83 | Train Loss 0.0239 | Train Acc 0.9950 | Train Micro f1 0.9950 | Train Macro f1 0.9950 | Val Loss 0.1107 | Val Micro f1 0.9567 | Val Macro f1 0.9566\n",
      "EarlyStopping counter: 62 out of 100\n",
      "Epoch 84 | Train Loss 0.0308 | Train Acc 0.9933 | Train Micro f1 0.9933 | Train Macro f1 0.9933 | Val Loss 0.1165 | Val Micro f1 0.9567 | Val Macro f1 0.9566\n",
      "EarlyStopping counter: 63 out of 100\n",
      "Epoch 85 | Train Loss 0.0332 | Train Acc 0.9933 | Train Micro f1 0.9933 | Train Macro f1 0.9933 | Val Loss 0.1072 | Val Micro f1 0.9533 | Val Macro f1 0.9533\n",
      "EarlyStopping counter: 64 out of 100\n",
      "Epoch 86 | Train Loss 0.0243 | Train Acc 0.9983 | Train Micro f1 0.9983 | Train Macro f1 0.9983 | Val Loss 0.0996 | Val Micro f1 0.9567 | Val Macro f1 0.9566\n",
      "EarlyStopping counter: 65 out of 100\n",
      "Epoch 87 | Train Loss 0.0271 | Train Acc 0.9950 | Train Micro f1 0.9950 | Train Macro f1 0.9950 | Val Loss 0.0955 | Val Micro f1 0.9667 | Val Macro f1 0.9666\n",
      "EarlyStopping counter: 66 out of 100\n",
      "Epoch 88 | Train Loss 0.0415 | Train Acc 0.9867 | Train Micro f1 0.9867 | Train Macro f1 0.9867 | Val Loss 0.0993 | Val Micro f1 0.9567 | Val Macro f1 0.9567\n",
      "EarlyStopping counter: 67 out of 100\n",
      "Epoch 89 | Train Loss 0.0269 | Train Acc 0.9967 | Train Micro f1 0.9967 | Train Macro f1 0.9967 | Val Loss 0.1156 | Val Micro f1 0.9567 | Val Macro f1 0.9567\n",
      "EarlyStopping counter: 68 out of 100\n",
      "Epoch 90 | Train Loss 0.0280 | Train Acc 0.9967 | Train Micro f1 0.9967 | Train Macro f1 0.9967 | Val Loss 0.1223 | Val Micro f1 0.9567 | Val Macro f1 0.9567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EarlyStopping counter: 69 out of 100\n",
      "Epoch 91 | Train Loss 0.0344 | Train Acc 0.9933 | Train Micro f1 0.9933 | Train Macro f1 0.9933 | Val Loss 0.1145 | Val Micro f1 0.9567 | Val Macro f1 0.9567\n",
      "EarlyStopping counter: 70 out of 100\n",
      "Epoch 92 | Train Loss 0.0310 | Train Acc 0.9933 | Train Micro f1 0.9933 | Train Macro f1 0.9933 | Val Loss 0.0968 | Val Micro f1 0.9533 | Val Macro f1 0.9534\n",
      "EarlyStopping counter: 71 out of 100\n",
      "Epoch 93 | Train Loss 0.0297 | Train Acc 0.9933 | Train Micro f1 0.9933 | Train Macro f1 0.9933 | Val Loss 0.0921 | Val Micro f1 0.9700 | Val Macro f1 0.9700\n",
      "EarlyStopping counter: 72 out of 100\n",
      "Epoch 94 | Train Loss 0.0305 | Train Acc 0.9950 | Train Micro f1 0.9950 | Train Macro f1 0.9950 | Val Loss 0.0967 | Val Micro f1 0.9667 | Val Macro f1 0.9666\n",
      "EarlyStopping counter: 73 out of 100\n",
      "Epoch 95 | Train Loss 0.0329 | Train Acc 0.9933 | Train Micro f1 0.9933 | Train Macro f1 0.9933 | Val Loss 0.1062 | Val Micro f1 0.9533 | Val Macro f1 0.9533\n",
      "EarlyStopping counter: 74 out of 100\n",
      "Epoch 96 | Train Loss 0.0332 | Train Acc 0.9867 | Train Micro f1 0.9867 | Train Macro f1 0.9867 | Val Loss 0.1191 | Val Micro f1 0.9533 | Val Macro f1 0.9534\n",
      "EarlyStopping counter: 75 out of 100\n",
      "Epoch 97 | Train Loss 0.0233 | Train Acc 0.9950 | Train Micro f1 0.9950 | Train Macro f1 0.9950 | Val Loss 0.1302 | Val Micro f1 0.9567 | Val Macro f1 0.9566\n",
      "EarlyStopping counter: 76 out of 100\n",
      "Epoch 98 | Train Loss 0.0459 | Train Acc 0.9900 | Train Micro f1 0.9900 | Train Macro f1 0.9900 | Val Loss 0.1156 | Val Micro f1 0.9567 | Val Macro f1 0.9566\n",
      "EarlyStopping counter: 77 out of 100\n",
      "Epoch 99 | Train Loss 0.0277 | Train Acc 0.9933 | Train Micro f1 0.9933 | Train Macro f1 0.9933 | Val Loss 0.1004 | Val Micro f1 0.9633 | Val Macro f1 0.9632\n",
      "EarlyStopping counter: 78 out of 100\n",
      "Epoch 100 | Train Loss 0.0232 | Train Acc 0.9967 | Train Micro f1 0.9967 | Train Macro f1 0.9967 | Val Loss 0.0959 | Val Micro f1 0.9700 | Val Macro f1 0.9699\n",
      "EarlyStopping counter: 79 out of 100\n",
      "Epoch 101 | Train Loss 0.0429 | Train Acc 0.9883 | Train Micro f1 0.9883 | Train Macro f1 0.9884 | Val Loss 0.1001 | Val Micro f1 0.9633 | Val Macro f1 0.9633\n",
      "EarlyStopping counter: 80 out of 100\n",
      "Epoch 102 | Train Loss 0.0222 | Train Acc 0.9933 | Train Micro f1 0.9933 | Train Macro f1 0.9933 | Val Loss 0.1081 | Val Micro f1 0.9567 | Val Macro f1 0.9566\n",
      "EarlyStopping counter: 81 out of 100\n",
      "Epoch 103 | Train Loss 0.0357 | Train Acc 0.9850 | Train Micro f1 0.9850 | Train Macro f1 0.9850 | Val Loss 0.1136 | Val Micro f1 0.9533 | Val Macro f1 0.9534\n",
      "EarlyStopping counter: 82 out of 100\n",
      "Epoch 104 | Train Loss 0.0292 | Train Acc 0.9917 | Train Micro f1 0.9917 | Train Macro f1 0.9917 | Val Loss 0.1114 | Val Micro f1 0.9533 | Val Macro f1 0.9534\n",
      "EarlyStopping counter: 83 out of 100\n",
      "Epoch 105 | Train Loss 0.0299 | Train Acc 0.9933 | Train Micro f1 0.9933 | Train Macro f1 0.9933 | Val Loss 0.1032 | Val Micro f1 0.9600 | Val Macro f1 0.9600\n",
      "EarlyStopping counter: 84 out of 100\n",
      "Epoch 106 | Train Loss 0.0181 | Train Acc 0.9983 | Train Micro f1 0.9983 | Train Macro f1 0.9983 | Val Loss 0.0955 | Val Micro f1 0.9600 | Val Macro f1 0.9601\n",
      "EarlyStopping counter: 85 out of 100\n",
      "Epoch 107 | Train Loss 0.0238 | Train Acc 0.9933 | Train Micro f1 0.9933 | Train Macro f1 0.9933 | Val Loss 0.0932 | Val Micro f1 0.9600 | Val Macro f1 0.9601\n",
      "EarlyStopping counter: 86 out of 100\n",
      "Epoch 108 | Train Loss 0.0268 | Train Acc 0.9967 | Train Micro f1 0.9967 | Train Macro f1 0.9967 | Val Loss 0.0939 | Val Micro f1 0.9600 | Val Macro f1 0.9601\n",
      "EarlyStopping counter: 87 out of 100\n",
      "Epoch 109 | Train Loss 0.0235 | Train Acc 0.9967 | Train Micro f1 0.9967 | Train Macro f1 0.9967 | Val Loss 0.1006 | Val Micro f1 0.9600 | Val Macro f1 0.9599\n",
      "EarlyStopping counter: 88 out of 100\n",
      "Epoch 110 | Train Loss 0.0261 | Train Acc 0.9933 | Train Micro f1 0.9933 | Train Macro f1 0.9933 | Val Loss 0.1184 | Val Micro f1 0.9600 | Val Macro f1 0.9599\n",
      "EarlyStopping counter: 89 out of 100\n",
      "Epoch 111 | Train Loss 0.0224 | Train Acc 1.0000 | Train Micro f1 1.0000 | Train Macro f1 1.0000 | Val Loss 0.1249 | Val Micro f1 0.9600 | Val Macro f1 0.9599\n",
      "EarlyStopping counter: 90 out of 100\n",
      "Epoch 112 | Train Loss 0.0345 | Train Acc 0.9967 | Train Micro f1 0.9967 | Train Macro f1 0.9967 | Val Loss 0.1103 | Val Micro f1 0.9567 | Val Macro f1 0.9566\n",
      "EarlyStopping counter: 91 out of 100\n",
      "Epoch 113 | Train Loss 0.0292 | Train Acc 0.9900 | Train Micro f1 0.9900 | Train Macro f1 0.9900 | Val Loss 0.0959 | Val Micro f1 0.9567 | Val Macro f1 0.9567\n",
      "EarlyStopping counter: 92 out of 100\n",
      "Epoch 114 | Train Loss 0.0272 | Train Acc 0.9967 | Train Micro f1 0.9967 | Train Macro f1 0.9967 | Val Loss 0.0916 | Val Micro f1 0.9600 | Val Macro f1 0.9600\n",
      "EarlyStopping counter: 93 out of 100\n",
      "Epoch 115 | Train Loss 0.0287 | Train Acc 0.9967 | Train Micro f1 0.9967 | Train Macro f1 0.9967 | Val Loss 0.0969 | Val Micro f1 0.9633 | Val Macro f1 0.9633\n",
      "EarlyStopping counter: 94 out of 100\n",
      "Epoch 116 | Train Loss 0.0270 | Train Acc 0.9950 | Train Micro f1 0.9950 | Train Macro f1 0.9950 | Val Loss 0.1109 | Val Micro f1 0.9567 | Val Macro f1 0.9566\n",
      "EarlyStopping counter: 95 out of 100\n",
      "Epoch 117 | Train Loss 0.0285 | Train Acc 0.9950 | Train Micro f1 0.9950 | Train Macro f1 0.9950 | Val Loss 0.1225 | Val Micro f1 0.9600 | Val Macro f1 0.9599\n",
      "EarlyStopping counter: 96 out of 100\n",
      "Epoch 118 | Train Loss 0.0435 | Train Acc 0.9867 | Train Micro f1 0.9867 | Train Macro f1 0.9867 | Val Loss 0.1117 | Val Micro f1 0.9533 | Val Macro f1 0.9533\n",
      "EarlyStopping counter: 97 out of 100\n",
      "Epoch 119 | Train Loss 0.0229 | Train Acc 0.9983 | Train Micro f1 0.9983 | Train Macro f1 0.9983 | Val Loss 0.0954 | Val Micro f1 0.9667 | Val Macro f1 0.9666\n",
      "EarlyStopping counter: 98 out of 100\n",
      "Epoch 120 | Train Loss 0.0258 | Train Acc 0.9933 | Train Micro f1 0.9933 | Train Macro f1 0.9933 | Val Loss 0.0906 | Val Micro f1 0.9633 | Val Macro f1 0.9634\n",
      "EarlyStopping counter: 99 out of 100\n",
      "Epoch 121 | Train Loss 0.0339 | Train Acc 0.9933 | Train Micro f1 0.9933 | Train Macro f1 0.9933 | Val Loss 0.0953 | Val Micro f1 0.9667 | Val Macro f1 0.9667\n",
      "EarlyStopping counter: 100 out of 100\n",
      "Epoch 122 | Train Loss 0.0221 | Train Acc 0.9983 | Train Micro f1 0.9983 | Train Macro f1 0.9983 | Val Loss 0.1171 | Val Micro f1 0.9500 | Val Macro f1 0.9499\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(args['num_epochs']):\n",
    "    model.train()\n",
    "    logits = model(g, features)\n",
    "    loss = loss_fcn(logits[train_mask], labels[train_mask])\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    train_acc, train_micro_f1, train_macro_f1 = score(logits[train_mask], labels[train_mask])\n",
    "    val_loss, val_acc, val_micro_f1, val_macro_f1 = evaluate(model, g, features, labels, val_mask, loss_fcn)\n",
    "    early_stop = stopper.step(val_loss.data.item(), val_acc, model)\n",
    "    \n",
    "    print('Epoch {:d} | Train Loss {:.4f} | Train Acc {:.4f} | Train Micro f1 {:.4f} | Train Macro f1 {:.4f} | '\n",
    "              'Val Loss {:.4f} | Val Micro f1 {:.4f} | Val Macro f1 {:.4f}'.format(epoch + 1, loss.item(),\n",
    "                 train_acc, train_micro_f1, train_macro_f1, val_loss.item(), val_micro_f1, val_macro_f1))\n",
    "\n",
    "    if early_stop:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopper.load_checkpoint(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc, test_micro_f1, test_macro_f1 = evaluate(model, g, features, labels, test_mask, loss_fcn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss 0.3348 | Test Acc 0.8847 | Test Micro f1 0.8847 | Test Macro f1 0.8849\n"
     ]
    }
   ],
   "source": [
    "print('Test loss {:.4f} | Test Acc {:.4f} | Test Micro f1 {:.4f} | Test Macro f1 {:.4f}'.format(test_loss.item(), test_acc, test_micro_f1, test_macro_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
