{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import pyhocon\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataCenter(object):\n",
    "\t\"\"\"docstring for DataCenter\"\"\"\n",
    "\tdef __init__(self, config):\n",
    "\t\tsuper(DataCenter, self).__init__()\n",
    "\t\tself.config = config\n",
    "\t\t\n",
    "\tdef load_dataSet(self, dataSet='cora'):\n",
    "\t\tif dataSet == 'cora':\n",
    "\t\t\tcora_content_file = self.config['file_path.cora_content']\n",
    "\t\t\tcora_cite_file = self.config['file_path.cora_cite']\n",
    "\n",
    "\t\t\tfeat_data = []\n",
    "\t\t\tlabels = [] # label sequence of node\n",
    "\t\t\tnode_map = {} # map node to Node_ID\n",
    "\t\t\tlabel_map = {} # map label to Label_ID\n",
    "\t\t\twith open(cora_content_file) as fp:\n",
    "\t\t\t\tfor i,line in enumerate(fp):\n",
    "\t\t\t\t\tinfo = line.strip().split()\n",
    "\t\t\t\t\tfeat_data.append([float(x) for x in info[1:-1]])\n",
    "\t\t\t\t\tnode_map[info[0]] = i  # 节点的index\n",
    "\t\t\t\t\tif not info[-1] in label_map:\n",
    "\t\t\t\t\t\tlabel_map[info[-1]] = len(label_map)\n",
    "\t\t\t\t\tlabels.append(label_map[info[-1]])\n",
    "\t\t\tfeat_data = np.asarray(feat_data)\n",
    "\t\t\tlabels = np.asarray(labels, dtype=np.int64)\n",
    "\t\t\t\n",
    "\t\t\tadj_lists = defaultdict(set)\n",
    "\t\t\twith open(cora_cite_file) as fp:\n",
    "\t\t\t\tfor i,line in enumerate(fp):\n",
    "\t\t\t\t\tinfo = line.strip().split()\n",
    "\t\t\t\t\tassert len(info) == 2\n",
    "\t\t\t\t\tpaper1 = node_map[info[0]]\n",
    "\t\t\t\t\tpaper2 = node_map[info[1]]\n",
    "\t\t\t\t\tadj_lists[paper1].add(paper2)\n",
    "\t\t\t\t\tadj_lists[paper2].add(paper1)\n",
    "\n",
    "\t\t\tassert len(feat_data) == len(labels) == len(adj_lists)\n",
    "\t\t\ttest_indexs, val_indexs, train_indexs = self._split_data(feat_data.shape[0])  # 选择data中训练，验证，测试集\n",
    "\n",
    "\t\t\tsetattr(self, dataSet+'_test', test_indexs)\n",
    "\t\t\tsetattr(self, dataSet+'_val', val_indexs)\n",
    "\t\t\tsetattr(self, dataSet+'_train', train_indexs)\n",
    "\n",
    "\t\t\tsetattr(self, dataSet+'_feats', feat_data)\n",
    "\t\t\tsetattr(self, dataSet+'_labels', labels)\n",
    "\t\t\tsetattr(self, dataSet+'_adj_lists', adj_lists)\n",
    "\n",
    "\t\telif dataSet == 'pubmed':\n",
    "\t\t\tpubmed_content_file = self.config['file_path.pubmed_paper']\n",
    "\t\t\tpubmed_cite_file = self.config['file_path.pubmed_cites']\n",
    "\n",
    "\t\t\tfeat_data = []\n",
    "\t\t\tlabels = [] # label sequence of node\n",
    "\t\t\tnode_map = {} # map node to Node_ID\n",
    "\t\t\twith open(pubmed_content_file) as fp:\n",
    "\t\t\t\tfp.readline()\n",
    "\t\t\t\tfeat_map = {entry.split(\":\")[1]:i-1 for i,entry in enumerate(fp.readline().split(\"\\t\"))}\n",
    "\t\t\t\tfor i, line in enumerate(fp):\n",
    "\t\t\t\t\tinfo = line.split(\"\\t\")\n",
    "\t\t\t\t\tnode_map[info[0]] = i\n",
    "\t\t\t\t\tlabels.append(int(info[1].split(\"=\")[1])-1)\n",
    "\t\t\t\t\ttmp_list = np.zeros(len(feat_map)-2)\n",
    "\t\t\t\t\tfor word_info in info[2:-1]:\n",
    "\t\t\t\t\t\tword_info = word_info.split(\"=\")\n",
    "\t\t\t\t\t\ttmp_list[feat_map[word_info[0]]] = float(word_info[1])\n",
    "\t\t\t\t\tfeat_data.append(tmp_list)\n",
    "\t\t\t\n",
    "\t\t\tfeat_data = np.asarray(feat_data)\n",
    "\t\t\tlabels = np.asarray(labels, dtype=np.int64)\n",
    "\t\t\t\n",
    "\t\t\tadj_lists = defaultdict(set)\n",
    "\t\t\twith open(pubmed_cite_file) as fp:\n",
    "\t\t\t\tfp.readline()\n",
    "\t\t\t\tfp.readline()\n",
    "\t\t\t\tfor line in fp:\n",
    "\t\t\t\t\tinfo = line.strip().split(\"\\t\")\n",
    "\t\t\t\t\tpaper1 = node_map[info[1].split(\":\")[1]]\n",
    "\t\t\t\t\tpaper2 = node_map[info[-1].split(\":\")[1]]\n",
    "\t\t\t\t\tadj_lists[paper1].add(paper2)\n",
    "\t\t\t\t\tadj_lists[paper2].add(paper1)\n",
    "\t\t\t\n",
    "\t\t\tassert len(feat_data) == len(labels) == len(adj_lists)\n",
    "\t\t\ttest_indexs, val_indexs, train_indexs = self._split_data(feat_data.shape[0])\n",
    "\n",
    "\t\t\tsetattr(self, dataSet+'_test', test_indexs)\n",
    "\t\t\tsetattr(self, dataSet+'_val', val_indexs)\n",
    "\t\t\tsetattr(self, dataSet+'_train', train_indexs)\n",
    "\n",
    "\t\t\tsetattr(self, dataSet+'_feats', feat_data)\n",
    "\t\t\tsetattr(self, dataSet+'_labels', labels)\n",
    "\t\t\tsetattr(self, dataSet+'_adj_lists', adj_lists)\n",
    "\n",
    "\n",
    "\tdef _split_data(self, num_nodes, test_split = 3, val_split = 6):\n",
    "\t\trand_indices = np.random.permutation(num_nodes)\n",
    "\n",
    "\t\ttest_size = num_nodes // test_split\n",
    "\t\tval_size = num_nodes // val_split\n",
    "\t\ttrain_size = num_nodes - (test_size + val_size)\n",
    "\n",
    "\t\ttest_indexs = rand_indices[:test_size]\n",
    "\t\tval_indexs = rand_indices[test_size:(test_size+val_size)]\n",
    "\t\ttrain_indexs = rand_indices[(test_size+val_size):]\n",
    "\t\t\n",
    "\t\treturn test_indexs, val_indexs, train_indexs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataCenter, ds, graphSage, classification, device, max_vali_f1, name, cur_epoch):\n",
    "\ttest_nodes = getattr(dataCenter, ds+'_test')\n",
    "\tval_nodes = getattr(dataCenter, ds+'_val')\n",
    "\tlabels = getattr(dataCenter, ds+'_labels')\n",
    "\n",
    "\tmodels = [graphSage, classification]\n",
    "\n",
    "\tparams = []\n",
    "\tfor model in models:\n",
    "\t\tfor param in model.parameters():\n",
    "\t\t\tif param.requires_grad:\n",
    "\t\t\t\tparam.requires_grad = False\n",
    "\t\t\t\tparams.append(param)\n",
    "\n",
    "\tembs = graphSage(val_nodes)\n",
    "\tlogists = classification(embs)\n",
    "\t_, predicts = torch.max(logists, 1)\n",
    "\tlabels_val = labels[val_nodes]\n",
    "\tassert len(labels_val) == len(predicts)\n",
    "\tcomps = zip(labels_val, predicts.data)\n",
    "\n",
    "\tvali_f1 = f1_score(labels_val, predicts.cpu().data, average=\"micro\")\n",
    "\tprint(\"Validation F1:\", vali_f1)\n",
    "\n",
    "\tif vali_f1 > max_vali_f1:\n",
    "\t\tmax_vali_f1 = vali_f1\n",
    "\t\tembs = graphSage(test_nodes)\n",
    "\t\tlogists = classification(embs)\n",
    "\t\t_, predicts = torch.max(logists, 1)\n",
    "\t\tlabels_test = labels[test_nodes]\n",
    "\t\tassert len(labels_test) == len(predicts)\n",
    "\t\tcomps = zip(labels_test, predicts.data)\n",
    "\n",
    "\t\ttest_f1 = f1_score(labels_test, predicts.cpu().data, average=\"micro\")\n",
    "\t\tprint(\"Test F1:\", test_f1)\n",
    "\n",
    "\t\tfor param in params:\n",
    "\t\t\tparam.requires_grad = True\n",
    "\n",
    "\t\ttorch.save(models, './model_best_{}_ep{}_{:.4f}.torch'.format(name, cur_epoch, test_f1))\n",
    "\n",
    "\tfor param in params:\n",
    "\t\tparam.requires_grad = True\n",
    "\n",
    "\treturn max_vali_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gnn_embeddings(gnn_model, dataCenter, ds):\n",
    "    print('Loading embeddings from trained GraphSAGE model.')\n",
    "    features = np.zeros((len(getattr(dataCenter, ds+'_labels')), gnn_model.out_size))\n",
    "    nodes = np.arange(len(getattr(dataCenter, ds+'_labels'))).tolist()\n",
    "    b_sz = 500\n",
    "    batches = math.ceil(len(nodes) / b_sz)\n",
    "    embs = []\n",
    "    for index in range(batches):\n",
    "        nodes_batch = nodes[index*b_sz:(index+1)*b_sz]\n",
    "        embs_batch = gnn_model(nodes_batch)\n",
    "        assert len(embs_batch) == len(nodes_batch)\n",
    "        embs.append(embs_batch)\n",
    "        # if ((index+1)*b_sz) % 10000 == 0:\n",
    "        #     print(f'Dealed Nodes [{(index+1)*b_sz}/{len(nodes)}]')\n",
    "\n",
    "    assert len(embs) == batches\n",
    "    embs = torch.cat(embs, 0)\n",
    "    assert len(embs) == len(nodes)\n",
    "    print('Embeddings loaded.')\n",
    "    return embs.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classification(dataCenter, graphSage, classification, ds, device, max_vali_f1, name, epochs=800):\n",
    "\tprint('Training Classification ...')\n",
    "\tc_optimizer = torch.optim.SGD(classification.parameters(), lr=0.5)\n",
    "\t# train classification, detached from the current graph\n",
    "\t#classification.init_params()\n",
    "\tb_sz = 50\n",
    "\ttrain_nodes = getattr(dataCenter, ds+'_train')\n",
    "\tlabels = getattr(dataCenter, ds+'_labels')\n",
    "\tfeatures = get_gnn_embeddings(graphSage, dataCenter, ds)\n",
    "\tfor epoch in range(epochs):\n",
    "\t\ttrain_nodes = shuffle(train_nodes)\n",
    "\t\tbatches = math.ceil(len(train_nodes) / b_sz)\n",
    "\t\tvisited_nodes = set()\n",
    "\t\tfor index in range(batches):\n",
    "\t\t\tnodes_batch = train_nodes[index*b_sz:(index+1)*b_sz]\n",
    "\t\t\tvisited_nodes |= set(nodes_batch)\n",
    "\t\t\tlabels_batch = labels[nodes_batch]\n",
    "\t\t\tembs_batch = features[nodes_batch]\n",
    "\n",
    "\t\t\tlogists = classification(embs_batch)\n",
    "\t\t\tloss = -torch.sum(logists[range(logists.size(0)), labels_batch], 0)\n",
    "\t\t\tloss /= len(nodes_batch)\n",
    "\t\t\t# print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Dealed Nodes [{}/{}] '.format(epoch+1, epochs, index, batches, loss.item(), len(visited_nodes), len(train_nodes)))\n",
    "\n",
    "\t\t\tloss.backward()\n",
    "\t\t\t\n",
    "\t\t\tnn.utils.clip_grad_norm_(classification.parameters(), 5)\n",
    "\t\t\tc_optimizer.step()\n",
    "\t\t\tc_optimizer.zero_grad()\n",
    "\n",
    "\t\tmax_vali_f1 = evaluate(dataCenter, ds, graphSage, classification, device, max_vali_f1, name, epoch)\n",
    "\treturn classification, max_vali_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_model(dataCenter, ds, graphSage, classification, unsupervised_loss, b_sz, unsup_loss, device, learn_method):\n",
    "\ttest_nodes = getattr(dataCenter, ds+'_test')\n",
    "\tval_nodes = getattr(dataCenter, ds+'_val')\n",
    "\ttrain_nodes = getattr(dataCenter, ds+'_train')\n",
    "\tlabels = getattr(dataCenter, ds+'_labels')\n",
    "\n",
    "\tif unsup_loss == 'margin':\n",
    "\t\tnum_neg = 6\n",
    "\telif unsup_loss == 'normal':\n",
    "\t\tnum_neg = 100\n",
    "\telse:\n",
    "\t\tprint(\"unsup_loss can be only 'margin' or 'normal'.\")\n",
    "\t\tsys.exit(1)\n",
    "\n",
    "\ttrain_nodes = shuffle(train_nodes)\n",
    "\n",
    "\tmodels = [graphSage, classification]\n",
    "\tparams = []\n",
    "\tfor model in models:\n",
    "\t\tfor param in model.parameters():\n",
    "\t\t\tif param.requires_grad:\n",
    "\t\t\t\tparams.append(param)\n",
    "\n",
    "\toptimizer = torch.optim.SGD(params, lr=0.7)\n",
    "\toptimizer.zero_grad()\n",
    "\tfor model in models:\n",
    "\t\tmodel.zero_grad()\n",
    "\n",
    "\tbatches = math.ceil(len(train_nodes) / b_sz)\n",
    "\n",
    "\tvisited_nodes = set()\n",
    "\tfor index in range(batches):\n",
    "\t\tnodes_batch = train_nodes[index*b_sz:(index+1)*b_sz]  # batch训练的节点\n",
    "\n",
    "\t\t# extend nodes batch for unspervised learning\n",
    "\t\t# no conflicts with supervised learning\n",
    "\t\tnodes_batch = np.asarray(list(unsupervised_loss.extend_nodes(nodes_batch, num_neg=num_neg)))\n",
    "\t\tvisited_nodes |= set(nodes_batch)\n",
    "\n",
    "\t\t# get ground-truth for the nodes batch\n",
    "\t\tlabels_batch = labels[nodes_batch]\n",
    "\n",
    "\t\t# feed nodes batch to the graphSAGE\n",
    "\t\t# returning the nodes embeddings。 得到GraphSAGE后的ebmedding向量\n",
    "\t\tembs_batch = graphSage(nodes_batch)  # 跳到models的GraphSge\n",
    "\n",
    "\t\tif learn_method == 'sup':\n",
    "\t\t\t# superivsed learning\n",
    "\t\t\tlogists = classification(embs_batch)\n",
    "\t\t\tloss_sup = -torch.sum(logists[range(logists.size(0)), labels_batch], 0)\n",
    "\t\t\tloss_sup /= len(nodes_batch)\n",
    "\t\t\tloss = loss_sup\n",
    "\t\telif learn_method == 'plus_unsup':\n",
    "\t\t\t# superivsed learning\n",
    "\t\t\tlogists = classification(embs_batch)\n",
    "\t\t\tloss_sup = -torch.sum(logists[range(logists.size(0)), labels_batch], 0)\n",
    "\t\t\tloss_sup /= len(nodes_batch)\n",
    "\t\t\t# unsuperivsed learning\n",
    "\t\t\tif unsup_loss == 'margin':\n",
    "\t\t\t\tloss_net = unsupervised_loss.get_loss_margin(embs_batch, nodes_batch)\n",
    "\t\t\telif unsup_loss == 'normal':\n",
    "\t\t\t\tloss_net = unsupervised_loss.get_loss_sage(embs_batch, nodes_batch)\n",
    "\t\t\tloss = loss_sup + loss_net\n",
    "\t\telse:\n",
    "\t\t\tif unsup_loss == 'margin':\n",
    "\t\t\t\tloss_net = unsupervised_loss.get_loss_margin(embs_batch, nodes_batch)\n",
    "\t\t\telif unsup_loss == 'normal':\n",
    "\t\t\t\tloss_net = unsupervised_loss.get_loss_sage(embs_batch, nodes_batch)\n",
    "\t\t\tloss = loss_net\n",
    "\n",
    "\t\tprint('Step [{}/{}], Loss: {:.4f}, Dealed Nodes [{}/{}] '.format(index+1, batches, loss.item(), len(visited_nodes), len(train_nodes)))\n",
    "\t\tloss.backward()\n",
    "\t\tfor model in models:\n",
    "\t\t\tnn.utils.clip_grad_norm_(model.parameters(), 5)  # 梯度的二范数和不超过5（平方和开根号）\n",
    "\t\toptimizer.step()\n",
    "\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\tfor model in models:\n",
    "\t\t\tmodel.zero_grad()\n",
    "\n",
    "\treturn graphSage, classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classification(nn.Module):\n",
    "\tdef __init__(self, emb_size, num_classes):\n",
    "\t\tsuper(Classification, self).__init__()\n",
    "\n",
    "\t\t#self.weight = nn.Parameter(torch.FloatTensor(emb_size, num_classes)) 最终的输出 (128, num_classes)\n",
    "\t\tself.layer = nn.Sequential(\n",
    "\t\t\t\t\t\t\t\tnn.Linear(emb_size, num_classes)\t  \n",
    "\t\t\t\t\t\t\t\t#nn.ReLU()\n",
    "\t\t\t\t\t\t\t)\n",
    "\t\tself.init_params()\n",
    "\n",
    "\tdef init_params(self):\n",
    "\t\tfor param in self.parameters():\n",
    "\t\t\tif len(param.size()) == 2:\n",
    "\t\t\t\tnn.init.xavier_uniform_(param)\n",
    "\n",
    "\tdef forward(self, embeds):\n",
    "\t\tlogists = torch.log_softmax(self.layer(embeds), 1)\n",
    "\t\treturn logists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnsupervisedLoss(object):\n",
    "\t\"\"\"docstring for UnsupervisedLoss\"\"\"\n",
    "\tdef __init__(self, adj_lists, train_nodes, device):\n",
    "\t\tsuper(UnsupervisedLoss, self).__init__()\n",
    "\t\tself.Q = 10\n",
    "\t\tself.N_WALKS = 6\n",
    "\t\tself.WALK_LEN = 1\n",
    "\t\tself.N_WALK_LEN = 5\n",
    "\t\tself.MARGIN = 3\n",
    "\t\tself.adj_lists = adj_lists\n",
    "\t\tself.train_nodes = train_nodes\n",
    "\t\tself.device = device\n",
    "\n",
    "\t\tself.target_nodes = None\n",
    "\t\tself.positive_pairs = []\n",
    "\t\tself.negtive_pairs = []\n",
    "\t\tself.node_positive_pairs = {}\n",
    "\t\tself.node_negtive_pairs = {}\n",
    "\t\tself.unique_nodes_batch = []\n",
    "\n",
    "\tdef get_loss_sage(self, embeddings, nodes):\n",
    "\t\tassert len(embeddings) == len(self.unique_nodes_batch)\n",
    "\t\tassert False not in [nodes[i]==self.unique_nodes_batch[i] for i in range(len(nodes))]\n",
    "\t\tnode2index = {n:i for i,n in enumerate(self.unique_nodes_batch)}\n",
    "\n",
    "\t\tnodes_score = []\n",
    "\t\tassert len(self.node_positive_pairs) == len(self.node_negtive_pairs)\n",
    "\t\tfor node in self.node_positive_pairs:\n",
    "\t\t\tpps = self.node_positive_pairs[node]\n",
    "\t\t\tnps = self.node_negtive_pairs[node]\n",
    "\t\t\tif len(pps) == 0 or len(nps) == 0:\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t# Q * Exception(negative score)\n",
    "\t\t\tindexs = [list(x) for x in zip(*nps)]\n",
    "\t\t\tnode_indexs = [node2index[x] for x in indexs[0]]\n",
    "\t\t\tneighb_indexs = [node2index[x] for x in indexs[1]]\n",
    "\t\t\tneg_score = F.cosine_similarity(embeddings[node_indexs], embeddings[neighb_indexs])\n",
    "\t\t\tneg_score = self.Q*torch.mean(torch.log(torch.sigmoid(-neg_score)), 0)\n",
    "\t\t\t#print(neg_score)\n",
    "\n",
    "\t\t\t# multiple positive score\n",
    "\t\t\tindexs = [list(x) for x in zip(*pps)]\n",
    "\t\t\tnode_indexs = [node2index[x] for x in indexs[0]]\n",
    "\t\t\tneighb_indexs = [node2index[x] for x in indexs[1]]\n",
    "\t\t\tpos_score = F.cosine_similarity(embeddings[node_indexs], embeddings[neighb_indexs])\n",
    "\t\t\tpos_score = torch.log(torch.sigmoid(pos_score))\n",
    "\t\t\t#print(pos_score)\n",
    "\n",
    "\t\t\tnodes_score.append(torch.mean(- pos_score - neg_score).view(1,-1))\n",
    "\t\t\t\t\n",
    "\t\tloss = torch.mean(torch.cat(nodes_score, 0))\n",
    "\t\t\n",
    "\t\treturn loss\n",
    "\n",
    "\tdef get_loss_margin(self, embeddings, nodes):\n",
    "\t\tassert len(embeddings) == len(self.unique_nodes_batch)\n",
    "\t\tassert False not in [nodes[i]==self.unique_nodes_batch[i] for i in range(len(nodes))]\n",
    "\t\tnode2index = {n:i for i,n in enumerate(self.unique_nodes_batch)}\n",
    "\n",
    "\t\tnodes_score = []\n",
    "\t\tassert len(self.node_positive_pairs) == len(self.node_negtive_pairs)\n",
    "\t\tfor node in self.node_positive_pairs:\n",
    "\t\t\tpps = self.node_positive_pairs[node]\n",
    "\t\t\tnps = self.node_negtive_pairs[node]\n",
    "\t\t\tif len(pps) == 0 or len(nps) == 0:\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\t\tindexs = [list(x) for x in zip(*pps)]\n",
    "\t\t\tnode_indexs = [node2index[x] for x in indexs[0]]\n",
    "\t\t\tneighb_indexs = [node2index[x] for x in indexs[1]]\n",
    "\t\t\tpos_score = F.cosine_similarity(embeddings[node_indexs], embeddings[neighb_indexs])\n",
    "\t\t\tpos_score, _ = torch.min(torch.log(torch.sigmoid(pos_score)), 0)\n",
    "\n",
    "\t\t\tindexs = [list(x) for x in zip(*nps)]\n",
    "\t\t\tnode_indexs = [node2index[x] for x in indexs[0]]\n",
    "\t\t\tneighb_indexs = [node2index[x] for x in indexs[1]]\n",
    "\t\t\tneg_score = F.cosine_similarity(embeddings[node_indexs], embeddings[neighb_indexs])\n",
    "\t\t\tneg_score, _ = torch.max(torch.log(torch.sigmoid(neg_score)), 0)\n",
    "\n",
    "\t\t\tnodes_score.append(torch.max(torch.tensor(0.0).to(self.device), neg_score-pos_score+self.MARGIN).view(1,-1))\n",
    "\t\t\t# nodes_score.append((-pos_score - neg_score).view(1,-1))\n",
    "\n",
    "\t\tloss = torch.mean(torch.cat(nodes_score, 0),0)\n",
    "\n",
    "\t\t# loss = -torch.log(torch.sigmoid(pos_score))-4*torch.log(torch.sigmoid(-neg_score))\n",
    "\t\t\n",
    "\t\treturn loss\n",
    "\n",
    "\n",
    "\tdef extend_nodes(self, nodes, num_neg=6):\n",
    "\t\tself.positive_pairs = []\n",
    "\t\tself.node_positive_pairs = {}\n",
    "\t\tself.negtive_pairs = []\n",
    "\t\tself.node_negtive_pairs = {}\n",
    "\n",
    "\t\tself.target_nodes = nodes\n",
    "\t\tself.get_positive_nodes(nodes)\n",
    "\t\t# print(self.positive_pairs)\n",
    "\t\tself.get_negtive_nodes(nodes, num_neg)\n",
    "\t\t# print(self.negtive_pairs)\n",
    "\t\tself.unique_nodes_batch = list(set([i for x in self.positive_pairs for i in x]) | set([i for x in self.negtive_pairs for i in x]))\n",
    "\t\tassert set(self.target_nodes) < set(self.unique_nodes_batch)\n",
    "\t\treturn self.unique_nodes_batch\n",
    "\n",
    "\tdef get_positive_nodes(self, nodes):\n",
    "\t\treturn self._run_random_walks(nodes)\n",
    "\n",
    "\tdef get_negtive_nodes(self, nodes, num_neg):\n",
    "\t\tfor node in nodes:\n",
    "\t\t\tneighbors = set([node])\n",
    "\t\t\tfrontier = set([node])\n",
    "\t\t\tfor i in range(self.N_WALK_LEN):\n",
    "\t\t\t\tcurrent = set()\n",
    "\t\t\t\tfor outer in frontier:\n",
    "\t\t\t\t\tcurrent |= self.adj_lists[int(outer)]\n",
    "\t\t\t\tfrontier = current - neighbors\n",
    "\t\t\t\tneighbors |= current\n",
    "\t\t\tfar_nodes = set(self.train_nodes) - neighbors\n",
    "\t\t\tneg_samples = random.sample(far_nodes, num_neg) if num_neg < len(far_nodes) else far_nodes\n",
    "\t\t\tself.negtive_pairs.extend([(node, neg_node) for neg_node in neg_samples])\n",
    "\t\t\tself.node_negtive_pairs[node] = [(node, neg_node) for neg_node in neg_samples]\n",
    "\t\treturn self.negtive_pairs\n",
    "\n",
    "\tdef _run_random_walks(self, nodes):\n",
    "\t\tfor node in nodes:\n",
    "\t\t\tif len(self.adj_lists[int(node)]) == 0:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tcur_pairs = []\n",
    "\t\t\tfor i in range(self.N_WALKS):\n",
    "\t\t\t\tcurr_node = node\n",
    "\t\t\t\tfor j in range(self.WALK_LEN):\n",
    "\t\t\t\t\tneighs = self.adj_lists[int(curr_node)]\n",
    "\t\t\t\t\tnext_node = random.choice(list(neighs))\n",
    "\t\t\t\t\t# self co-occurrences are useless\n",
    "\t\t\t\t\tif next_node != node and next_node in self.train_nodes:\n",
    "\t\t\t\t\t\tself.positive_pairs.append((node,next_node))\n",
    "\t\t\t\t\t\tcur_pairs.append((node,next_node))\n",
    "\t\t\t\t\tcurr_node = next_node\n",
    "\n",
    "\t\t\tself.node_positive_pairs[node] = cur_pairs\n",
    "\t\treturn self.positive_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SageLayer(nn.Module):\n",
    "\t\"\"\"\n",
    "\tEncodes a node's using 'convolutional' GraphSage approach\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, input_size, out_size, gcn=False): \n",
    "\t\tsuper(SageLayer, self).__init__()\n",
    "\n",
    "\t\tself.input_size = input_size\n",
    "\t\tself.out_size = out_size\n",
    "\n",
    "\n",
    "\t\tself.gcn = gcn\n",
    "\t\tself.weight = nn.Parameter(torch.FloatTensor(out_size, self.input_size if self.gcn else 2 * self.input_size)) # 创建weight\n",
    "\n",
    "\t\tself.init_params()  # 初始化参数\n",
    "\n",
    "\tdef init_params(self):\n",
    "\t\tfor param in self.parameters():\n",
    "\t\t\tnn.init.xavier_uniform_(param)\n",
    "\n",
    "\tdef forward(self, self_feats, aggregate_feats, neighs=None):\n",
    "\t\t\"\"\"\n",
    "\t\tGenerates embeddings for a batch of nodes.\n",
    "\n",
    "\t\tnodes\t -- list of nodes\n",
    "\t\t\"\"\"\n",
    "\t\tif not self.gcn:\n",
    "\t\t\tcombined = torch.cat([self_feats, aggregate_feats], dim=1)   # concat自己信息和邻居信息\n",
    "\t\telse:\n",
    "\t\t\tcombined = aggregate_feats\n",
    "\t\tcombined = F.relu(self.weight.mm(combined.t())).t()\n",
    "\t\treturn combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSage(nn.Module):\n",
    "\t\"\"\"docstring for GraphSage\"\"\"\n",
    "\tdef __init__(self, num_layers, input_size, out_size, raw_features, adj_lists, device, gcn=False, agg_func='MEAN'):\n",
    "\t\tsuper(GraphSage, self).__init__()\n",
    "\n",
    "\t\tself.input_size = input_size\n",
    "\t\tself.out_size = out_size\n",
    "\t\tself.num_layers = num_layers\n",
    "\t\tself.gcn = gcn\n",
    "\t\tself.device = device\n",
    "\t\tself.agg_func = agg_func\n",
    "\n",
    "\t\tself.raw_features = raw_features  # 点的特征\n",
    "\t\tself.adj_lists = adj_lists  # 边的连接\n",
    "\n",
    "\t\tfor index in range(1, num_layers+1):\n",
    "\t\t\tlayer_size = out_size if index != 1 else input_size\n",
    "\t\t\tsetattr(self, 'sage_layer'+str(index), SageLayer(layer_size, out_size, gcn=self.gcn))\n",
    "\n",
    "\tdef forward(self, nodes_batch):\n",
    "\t\t\"\"\"\n",
    "\t\tGenerates embeddings for a batch of nodes.\n",
    "\t\tnodes_batch\t-- batch of nodes to learn the embeddings.    《minbatch 过程，涉及到的所有节点》\n",
    "\t\t\"\"\"\n",
    "\t\tlower_layer_nodes = list(nodes_batch)\n",
    "\t\tnodes_batch_layers = [(lower_layer_nodes,)]  # 第一次放入的节点，batch节点\n",
    "\t\t# self.dc.logger.info('get_unique_neighs.')\n",
    "\t\tfor i in range(self.num_layers):  # 每层的Sage\n",
    "\t\t\tlower_samp_neighs, lower_layer_nodes_dict, lower_layer_nodes= self._get_unique_neighs_list(lower_layer_nodes)  # 获得neighbors。 聚合自己和邻居节点，点的dict，涉及到的所有节点\n",
    "\t\t\tnodes_batch_layers.insert(0, (lower_layer_nodes, lower_samp_neighs, lower_layer_nodes_dict))  # 聚合自己和邻居节点，点的dict，涉及到的所有节点\n",
    "\t\t\t# insert,0 从最外层开始聚合\n",
    "\t\tassert len(nodes_batch_layers) == self.num_layers + 1\n",
    "\n",
    "\t\tpre_hidden_embs = self.raw_features\n",
    "\t\tfor index in range(1, self.num_layers+1):\n",
    "\t\t\tnb = nodes_batch_layers[index][0]   # 聚合自己和周围的节点\n",
    "\t\t\tpre_neighs = nodes_batch_layers[index-1]  # 这层节点的上层邻居的所有信息。聚合自己和邻居节点，点的dict，涉及到的所有节点\n",
    "\t\t\t# self.dc.logger.info('aggregate_feats.') aggrefate_feats=>输出GraphSAGE聚合后的信息\n",
    "\t\t\taggregate_feats = self.aggregate(nb, pre_hidden_embs, pre_neighs)  # 聚合函数。nb-这一层的节点， pre_hidden_embs-feature，pre_neighs-上一层节点\n",
    "\t\t\tsage_layer = getattr(self, 'sage_layer'+str(index))\n",
    "\t\t\tif index > 1:\n",
    "\t\t\t\tnb = self._nodes_map(nb, pre_hidden_embs, pre_neighs)   # 第一层的batch节点，没有进行转换\n",
    "\t\t\t# self.dc.logger.info('sage_layer.')\n",
    "\t\t\tcur_hidden_embs = sage_layer(self_feats=pre_hidden_embs[nb],\n",
    "\t\t\t\t\t\t\t\t\t\taggregate_feats=aggregate_feats)  # 进入SageLayer。weight*concat(node,neighbors)\n",
    "\t\t\tpre_hidden_embs = cur_hidden_embs\n",
    "\n",
    "\t\treturn pre_hidden_embs\n",
    "\n",
    "\tdef _nodes_map(self, nodes, hidden_embs, neighs):\n",
    "\t\tlayer_nodes, samp_neighs, layer_nodes_dict = neighs\n",
    "\t\tassert len(samp_neighs) == len(nodes)\n",
    "\t\tindex = [layer_nodes_dict[x] for x in nodes]  # 记录将上一层的节点编号。\n",
    "\t\treturn index\n",
    "\n",
    "\tdef _get_unique_neighs_list(self, nodes, num_sample=10):\n",
    "\t\t_set = set\n",
    "\t\tto_neighs = [self.adj_lists[int(node)] for node in nodes]    # self.adj_lists边矩阵，获取节点的邻居\n",
    "\t\tif not num_sample is None:  # 对邻居节点进行采样，如果大于邻居数据，则进行采样\n",
    "\t\t\t_sample = random.sample\n",
    "\t\t\tsamp_neighs = [_set(_sample(to_neigh, num_sample)) if len(to_neigh) >= num_sample else to_neigh for to_neigh in to_neighs]\n",
    "\t\telse:\n",
    "\t\t\tsamp_neighs = to_neighs\n",
    "\t\tsamp_neighs = [samp_neigh | set([nodes[i]]) for i, samp_neigh in enumerate(samp_neighs)]  # 聚合本身节点和邻居节点\n",
    "\t\t_unique_nodes_list = list(set.union(*samp_neighs))  # 这个batch涉及到的所有节点\n",
    "\t\ti = list(range(len(_unique_nodes_list)))\n",
    "\t\tunique_nodes = dict(list(zip(_unique_nodes_list, i)))  # 字典编号\n",
    "\t\treturn samp_neighs, unique_nodes, _unique_nodes_list   # 聚合自己和邻居节点，点的dict，涉及到的所有节点\n",
    "\n",
    "\tdef aggregate(self, nodes, pre_hidden_embs, pre_neighs, num_sample=10):\n",
    "\t\tunique_nodes_list, samp_neighs, unique_nodes = pre_neighs   # 聚合自己和邻居节点，涉及到的所有节点，点的dict\n",
    "\n",
    "\t\tassert len(nodes) == len(samp_neighs)\n",
    "\t\tindicator = [(nodes[i] in samp_neighs[i]) for i in range(len(samp_neighs))]  # 都是True，因为上文中，将nodes加入到neighs中了\n",
    "\t\tassert (False not in indicator)\n",
    "\t\tif not self.gcn:\n",
    "\t\t\tsamp_neighs = [(samp_neighs[i]-set([nodes[i]])) for i in range(len(samp_neighs))]  # 在把中心节点去掉\n",
    "\t\t# self.dc.logger.info('2')\n",
    "\t\tif len(pre_hidden_embs) == len(unique_nodes):  # 如果涉及到所有节点，保留原矩阵。如果不涉及所有节点，保留部分矩阵。\n",
    "\t\t\tembed_matrix = pre_hidden_embs\n",
    "\t\telse:\n",
    "\t\t\tembed_matrix = pre_hidden_embs[torch.LongTensor(unique_nodes_list)]\n",
    "\t\t# self.dc.logger.info('3')  将对应到的边，构建邻接矩阵\n",
    "\t\tmask = torch.zeros(len(samp_neighs), len(unique_nodes))   # 本层节点数量，涉及到上层节点数量\n",
    "\t\tcolumn_indices = [unique_nodes[n] for samp_neigh in samp_neighs for n in samp_neigh]  # 构建邻接矩阵\n",
    "\t\trow_indices = [i for i in range(len(samp_neighs)) for j in range(len(samp_neighs[i]))]\n",
    "\t\tmask[row_indices, column_indices] = 1   # 加上上两个步骤，都是构建邻接矩阵;\n",
    "\t\t# self.dc.logger.info('4')\n",
    "\t\t# mask - 邻接矩阵\n",
    "\t\tif self.agg_func == 'MEAN':\n",
    "\t\t\tnum_neigh = mask.sum(1, keepdim=True)    # 按行求和，保持和输入一个维度\n",
    "\t\t\tmask = mask.div(num_neigh).to(embed_matrix.device)  # 归一化操作\n",
    "\t\t\taggregate_feats = mask.mm(embed_matrix)   # 矩阵相乘，相当于聚合周围邻接信息求和\n",
    "\n",
    "\t\telif self.agg_func == 'MAX':\n",
    "\t\t\t# print(mask)\n",
    "\t\t\tindexs = [x.nonzero() for x in mask==1]\n",
    "\t\t\taggregate_feats = []\n",
    "\t\t\t# self.dc.logger.info('5')\n",
    "\t\t\tfor feat in [embed_matrix[x.squeeze()] for x in indexs]:\n",
    "\t\t\t\tif len(feat.size()) == 1:\n",
    "\t\t\t\t\taggregate_feats.append(feat.view(1, -1))\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\taggregate_feats.append(torch.max(feat,0)[0].view(1, -1))\n",
    "\t\t\taggregate_feats = torch.cat(aggregate_feats, 0)\n",
    "\n",
    "\t\t# self.dc.logger.info('6')\n",
    "\t\t\n",
    "\t\treturn aggregate_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='pytorch version of GraphSAGE')\n",
    "\n",
    "parser.add_argument('--dataSet', type=str, default='cora')\n",
    "parser.add_argument('--agg_func', type=str, default='MEAN')\n",
    "parser.add_argument('--epochs', type=int, default=50)\n",
    "parser.add_argument('--b_sz', type=int, default=20)\n",
    "parser.add_argument('--seed', type=int, default=824)\n",
    "parser.add_argument('--cuda', action='store_true', help='use CUDA')\n",
    "parser.add_argument('--gcn', action='store_true')\n",
    "parser.add_argument('--learn_method', type=str, default='sup')\n",
    "parser.add_argument('--unsup_loss', type=str, default='normal')\n",
    "parser.add_argument('--max_vali_f1', type=float, default=0)\n",
    "parser.add_argument('--name', type=str, default='debug')\n",
    "\n",
    "# attention\n",
    "parser.add_argument('--config', type=str, default=r'C:\\Users\\sss\\Desktop\\graphSAGEpytorch\\src/experiments.conf')  #/src\n",
    "\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(agg_func='MEAN', b_sz=20, config='C:\\\\Users\\\\sss\\\\Desktop\\\\graphSAGEpytorch\\\\src/experiments.conf', cuda=False, dataSet='cora', epochs=50, gcn=False, learn_method='sup', max_vali_f1=0, name='debug', seed=824, unsup_loss='normal')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: You have a CUDA device, so you should probably run with --cuda\n",
      "DEVICE: cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "\tif not args.cuda:\n",
    "\t\tprint(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n",
    "\telse:\n",
    "\t\tdevice_id = torch.cuda.current_device()\n",
    "\t\tprint('using device', device_id, torch.cuda.get_device_name(device_id))\n",
    "\n",
    "device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "print('DEVICE:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config file\n",
    "config = pyhocon.ConfigFactory.parse_file(args.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "ds = args.dataSet\n",
    "dataCenter = DataCenter(config)\n",
    "dataCenter.load_dataSet(ds)  # 读取数据\n",
    "features = torch.FloatTensor(getattr(dataCenter, ds+'_feats')).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphSage(\n",
       "  (sage_layer1): SageLayer()\n",
       "  (sage_layer2): SageLayer()\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graphSage = GraphSage(config['setting.num_layers'], features.size(1), config['setting.hidden_emb_size'], features, getattr(dataCenter, ds+'_adj_lists'), device, gcn=args.gcn, agg_func=args.agg_func)\n",
    "graphSage.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classification(\n",
       "  (layer): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=7, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_labels = len(set(getattr(dataCenter, ds+'_labels')))  # label的数量\n",
    "classification = Classification(config['setting.hidden_emb_size'], num_labels)\n",
    "classification.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsupervised_loss = UnsupervisedLoss(getattr(dataCenter, ds+'_adj_lists'), getattr(dataCenter, ds+'_train'), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphSage with Supervised Learning\n"
     ]
    }
   ],
   "source": [
    "if args.learn_method == 'sup':\n",
    "    print('GraphSage with Supervised Learning')\n",
    "elif args.learn_method == 'plus_unsup':\n",
    "    print('GraphSage with Supervised Learning plus Net Unsupervised Learning')\n",
    "else:\n",
    "    print('GraphSage with Net Unsupervised Learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------EPOCH 0-----------------------\n",
      "Step [1/68], Loss: 1.9455, Dealed Nodes [1022/1355] \n",
      "Step [2/68], Loss: 1.8527, Dealed Nodes [1228/1355] \n",
      "Step [3/68], Loss: 1.7817, Dealed Nodes [1312/1355] \n",
      "Step [4/68], Loss: 1.7097, Dealed Nodes [1335/1355] \n",
      "Step [5/68], Loss: 1.6273, Dealed Nodes [1343/1355] \n",
      "Step [6/68], Loss: 1.5432, Dealed Nodes [1347/1355] \n",
      "Step [7/68], Loss: 1.4228, Dealed Nodes [1350/1355] \n",
      "Step [8/68], Loss: 1.2860, Dealed Nodes [1353/1355] \n",
      "Step [9/68], Loss: 1.1731, Dealed Nodes [1353/1355] \n",
      "Step [10/68], Loss: 1.0483, Dealed Nodes [1353/1355] \n",
      "Step [11/68], Loss: 0.9332, Dealed Nodes [1353/1355] \n",
      "Step [12/68], Loss: 0.8344, Dealed Nodes [1354/1355] \n",
      "Step [13/68], Loss: 0.8589, Dealed Nodes [1355/1355] \n",
      "Step [14/68], Loss: 1.4226, Dealed Nodes [1355/1355] \n",
      "Step [15/68], Loss: 1.7055, Dealed Nodes [1355/1355] \n",
      "Step [16/68], Loss: 0.8923, Dealed Nodes [1355/1355] \n",
      "Step [17/68], Loss: 0.6651, Dealed Nodes [1355/1355] \n",
      "Step [18/68], Loss: 0.6100, Dealed Nodes [1355/1355] \n",
      "Step [19/68], Loss: 0.5092, Dealed Nodes [1355/1355] \n",
      "Step [20/68], Loss: 0.4711, Dealed Nodes [1355/1355] \n",
      "Step [21/68], Loss: 0.4063, Dealed Nodes [1355/1355] \n",
      "Step [22/68], Loss: 0.3867, Dealed Nodes [1355/1355] \n",
      "Step [23/68], Loss: 0.3597, Dealed Nodes [1355/1355] \n",
      "Step [24/68], Loss: 0.3363, Dealed Nodes [1355/1355] \n",
      "Step [25/68], Loss: 0.3437, Dealed Nodes [1355/1355] \n",
      "Step [26/68], Loss: 0.3509, Dealed Nodes [1355/1355] \n",
      "Step [27/68], Loss: 0.3566, Dealed Nodes [1355/1355] \n",
      "Step [28/68], Loss: 0.3657, Dealed Nodes [1355/1355] \n",
      "Step [29/68], Loss: 0.3501, Dealed Nodes [1355/1355] \n",
      "Step [30/68], Loss: 0.2815, Dealed Nodes [1355/1355] \n",
      "Step [31/68], Loss: 0.2471, Dealed Nodes [1355/1355] \n",
      "Step [32/68], Loss: 0.2139, Dealed Nodes [1355/1355] \n",
      "Step [33/68], Loss: 0.1928, Dealed Nodes [1355/1355] \n",
      "Step [34/68], Loss: 0.1763, Dealed Nodes [1355/1355] \n",
      "Step [35/68], Loss: 0.1778, Dealed Nodes [1355/1355] \n",
      "Step [36/68], Loss: 0.1658, Dealed Nodes [1355/1355] \n",
      "Step [37/68], Loss: 0.1559, Dealed Nodes [1355/1355] \n",
      "Step [38/68], Loss: 0.1462, Dealed Nodes [1355/1355] \n",
      "Step [39/68], Loss: 0.1461, Dealed Nodes [1355/1355] \n",
      "Step [40/68], Loss: 0.1347, Dealed Nodes [1355/1355] \n",
      "Step [41/68], Loss: 0.1330, Dealed Nodes [1355/1355] \n",
      "Step [42/68], Loss: 0.1264, Dealed Nodes [1355/1355] \n",
      "Step [43/68], Loss: 0.1167, Dealed Nodes [1355/1355] \n",
      "Step [44/68], Loss: 0.1035, Dealed Nodes [1355/1355] \n",
      "Step [45/68], Loss: 0.1090, Dealed Nodes [1355/1355] \n",
      "Step [46/68], Loss: 0.0980, Dealed Nodes [1355/1355] \n",
      "Step [47/68], Loss: 0.0969, Dealed Nodes [1355/1355] \n",
      "Step [48/68], Loss: 0.1035, Dealed Nodes [1355/1355] \n",
      "Step [49/68], Loss: 0.0795, Dealed Nodes [1355/1355] \n",
      "Step [50/68], Loss: 0.0822, Dealed Nodes [1355/1355] \n",
      "Step [51/68], Loss: 0.0770, Dealed Nodes [1355/1355] \n",
      "Step [52/68], Loss: 0.0728, Dealed Nodes [1355/1355] \n",
      "Step [53/68], Loss: 0.0728, Dealed Nodes [1355/1355] \n",
      "Step [54/68], Loss: 0.0631, Dealed Nodes [1355/1355] \n",
      "Step [55/68], Loss: 0.0533, Dealed Nodes [1355/1355] \n",
      "Step [56/68], Loss: 0.0593, Dealed Nodes [1355/1355] \n",
      "Step [57/68], Loss: 0.0532, Dealed Nodes [1355/1355] \n",
      "Step [58/68], Loss: 0.0522, Dealed Nodes [1355/1355] \n",
      "Step [59/68], Loss: 0.0510, Dealed Nodes [1355/1355] \n",
      "Step [60/68], Loss: 0.0417, Dealed Nodes [1355/1355] \n",
      "Step [61/68], Loss: 0.0490, Dealed Nodes [1355/1355] \n",
      "Step [62/68], Loss: 0.0457, Dealed Nodes [1355/1355] \n",
      "Step [63/68], Loss: 0.0440, Dealed Nodes [1355/1355] \n",
      "Step [64/68], Loss: 0.0398, Dealed Nodes [1355/1355] \n",
      "Step [65/68], Loss: 0.0426, Dealed Nodes [1355/1355] \n",
      "Step [66/68], Loss: 0.0405, Dealed Nodes [1355/1355] \n",
      "Step [67/68], Loss: 0.0390, Dealed Nodes [1355/1355] \n",
      "Step [68/68], Loss: 0.0344, Dealed Nodes [1355/1355] \n",
      "Validation F1: 0.893569844789357\n",
      "Test F1: 0.8758314855875832\n",
      "----------------------EPOCH 1-----------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sss\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\serialization.py:250: UserWarning: Couldn't retrieve source code for container of type GraphSage. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "c:\\users\\sss\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\serialization.py:250: UserWarning: Couldn't retrieve source code for container of type SageLayer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "c:\\users\\sss\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\serialization.py:250: UserWarning: Couldn't retrieve source code for container of type Classification. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [1/68], Loss: 0.0364, Dealed Nodes [1034/1355] \n",
      "Step [2/68], Loss: 0.0313, Dealed Nodes [1254/1355] \n",
      "Step [3/68], Loss: 0.0343, Dealed Nodes [1319/1355] \n",
      "Step [4/68], Loss: 0.0304, Dealed Nodes [1344/1355] \n",
      "Step [5/68], Loss: 0.0326, Dealed Nodes [1350/1355] \n",
      "Step [6/68], Loss: 0.0303, Dealed Nodes [1352/1355] \n",
      "Step [7/68], Loss: 0.0287, Dealed Nodes [1352/1355] \n",
      "Step [8/68], Loss: 0.0290, Dealed Nodes [1353/1355] \n",
      "Step [9/68], Loss: 0.0262, Dealed Nodes [1353/1355] \n",
      "Step [10/68], Loss: 0.0257, Dealed Nodes [1354/1355] \n",
      "Step [11/68], Loss: 0.0273, Dealed Nodes [1355/1355] \n",
      "Step [12/68], Loss: 0.0255, Dealed Nodes [1355/1355] \n",
      "Step [13/68], Loss: 0.0251, Dealed Nodes [1355/1355] \n",
      "Step [14/68], Loss: 0.0251, Dealed Nodes [1355/1355] \n",
      "Step [15/68], Loss: 0.0233, Dealed Nodes [1355/1355] \n",
      "Step [16/68], Loss: 0.0213, Dealed Nodes [1355/1355] \n",
      "Step [17/68], Loss: 0.0229, Dealed Nodes [1355/1355] \n",
      "Step [18/68], Loss: 0.0219, Dealed Nodes [1355/1355] \n",
      "Step [19/68], Loss: 0.0220, Dealed Nodes [1355/1355] \n",
      "Step [20/68], Loss: 0.0216, Dealed Nodes [1355/1355] \n",
      "Step [21/68], Loss: 0.0209, Dealed Nodes [1355/1355] \n",
      "Step [22/68], Loss: 0.0214, Dealed Nodes [1355/1355] \n",
      "Step [23/68], Loss: 0.0205, Dealed Nodes [1355/1355] \n",
      "Step [24/68], Loss: 0.0169, Dealed Nodes [1355/1355] \n",
      "Step [25/68], Loss: 0.0177, Dealed Nodes [1355/1355] \n",
      "Step [26/68], Loss: 0.0176, Dealed Nodes [1355/1355] \n",
      "Step [27/68], Loss: 0.0168, Dealed Nodes [1355/1355] \n",
      "Step [28/68], Loss: 0.0161, Dealed Nodes [1355/1355] \n",
      "Step [29/68], Loss: 0.0160, Dealed Nodes [1355/1355] \n",
      "Step [30/68], Loss: 0.0156, Dealed Nodes [1355/1355] \n",
      "Step [31/68], Loss: 0.0160, Dealed Nodes [1355/1355] \n",
      "Step [32/68], Loss: 0.0148, Dealed Nodes [1355/1355] \n",
      "Step [33/68], Loss: 0.0162, Dealed Nodes [1355/1355] \n",
      "Step [34/68], Loss: 0.0153, Dealed Nodes [1355/1355] \n",
      "Step [35/68], Loss: 0.0156, Dealed Nodes [1355/1355] \n",
      "Step [36/68], Loss: 0.0142, Dealed Nodes [1355/1355] \n",
      "Step [37/68], Loss: 0.0148, Dealed Nodes [1355/1355] \n",
      "Step [38/68], Loss: 0.0145, Dealed Nodes [1355/1355] \n",
      "Step [39/68], Loss: 0.0141, Dealed Nodes [1355/1355] \n",
      "Step [40/68], Loss: 0.0134, Dealed Nodes [1355/1355] \n",
      "Step [41/68], Loss: 0.0122, Dealed Nodes [1355/1355] \n",
      "Step [42/68], Loss: 0.0137, Dealed Nodes [1355/1355] \n",
      "Step [43/68], Loss: 0.0123, Dealed Nodes [1355/1355] \n",
      "Step [44/68], Loss: 0.0126, Dealed Nodes [1355/1355] \n",
      "Step [45/68], Loss: 0.0129, Dealed Nodes [1355/1355] \n",
      "Step [46/68], Loss: 0.0120, Dealed Nodes [1355/1355] \n",
      "Step [47/68], Loss: 0.0127, Dealed Nodes [1355/1355] \n",
      "Step [48/68], Loss: 0.0118, Dealed Nodes [1355/1355] \n",
      "Step [49/68], Loss: 0.0119, Dealed Nodes [1355/1355] \n",
      "Step [50/68], Loss: 0.0132, Dealed Nodes [1355/1355] \n",
      "Step [51/68], Loss: 0.0115, Dealed Nodes [1355/1355] \n",
      "Step [52/68], Loss: 0.0116, Dealed Nodes [1355/1355] \n",
      "Step [53/68], Loss: 0.0103, Dealed Nodes [1355/1355] \n",
      "Step [54/68], Loss: 0.0109, Dealed Nodes [1355/1355] \n",
      "Step [55/68], Loss: 0.0104, Dealed Nodes [1355/1355] \n",
      "Step [56/68], Loss: 0.0113, Dealed Nodes [1355/1355] \n",
      "Step [57/68], Loss: 0.0101, Dealed Nodes [1355/1355] \n",
      "Step [58/68], Loss: 0.0098, Dealed Nodes [1355/1355] \n",
      "Step [59/68], Loss: 0.0099, Dealed Nodes [1355/1355] \n",
      "Step [60/68], Loss: 0.0101, Dealed Nodes [1355/1355] \n",
      "Step [61/68], Loss: 0.0089, Dealed Nodes [1355/1355] \n",
      "Step [62/68], Loss: 0.0098, Dealed Nodes [1355/1355] \n",
      "Step [63/68], Loss: 0.0102, Dealed Nodes [1355/1355] \n",
      "Step [64/68], Loss: 0.0085, Dealed Nodes [1355/1355] \n",
      "Step [65/68], Loss: 0.0083, Dealed Nodes [1355/1355] \n",
      "Step [66/68], Loss: 0.0082, Dealed Nodes [1355/1355] \n",
      "Step [67/68], Loss: 0.0088, Dealed Nodes [1355/1355] \n",
      "Step [68/68], Loss: 0.0078, Dealed Nodes [1355/1355] \n",
      "Validation F1: 0.88470066518847\n",
      "----------------------EPOCH 2-----------------------\n",
      "Step [1/68], Loss: 0.0087, Dealed Nodes [1037/1355] \n",
      "Step [2/68], Loss: 0.0091, Dealed Nodes [1248/1355] \n",
      "Step [3/68], Loss: 0.0079, Dealed Nodes [1313/1355] \n",
      "Step [4/68], Loss: 0.0083, Dealed Nodes [1340/1355] \n",
      "Step [5/68], Loss: 0.0076, Dealed Nodes [1349/1355] \n",
      "Step [6/68], Loss: 0.0080, Dealed Nodes [1351/1355] \n",
      "Step [7/68], Loss: 0.0076, Dealed Nodes [1353/1355] \n",
      "Step [8/68], Loss: 0.0078, Dealed Nodes [1354/1355] \n",
      "Step [9/68], Loss: 0.0077, Dealed Nodes [1354/1355] \n",
      "Step [10/68], Loss: 0.0072, Dealed Nodes [1354/1355] \n",
      "Step [11/68], Loss: 0.0076, Dealed Nodes [1355/1355] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-6c3ae07a0ab0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'----------------------EPOCH %d-----------------------'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m# 返回的是模型\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mgraphSage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassification\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapply_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataCenter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraphSage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassification\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munsupervised_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb_sz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsup_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn_method\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn_method\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'unsup'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mclassification\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_vali_f1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_classification\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataCenter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraphSage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassification\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_vali_f1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-dfd0cb927541>\u001b[0m in \u001b[0;36mapply_model\u001b[1;34m(dataCenter, ds, graphSage, classification, unsupervised_loss, b_sz, unsup_loss, device, learn_method)\u001b[0m\n\u001b[0;32m     43\u001b[0m                 \u001b[1;31m# feed nodes batch to the graphSAGE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m                 \u001b[1;31m# returning the nodes embeddings。 得到GraphSAGE后的ebmedding向量\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m                 \u001b[0membs_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraphSage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnodes_batch\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 跳到models的GraphSge\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlearn_method\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'sup'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sss\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-4211e6cfdc3c>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, nodes_batch)\u001b[0m\n\u001b[0;32m     43\u001b[0m                         \u001b[1;31m# self.dc.logger.info('sage_layer.')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \t\t\tcur_hidden_embs = sage_layer(self_feats=pre_hidden_embs[nb],\n\u001b[1;32m---> 45\u001b[1;33m \t\t\t\t\t\t\t\t\t\taggregate_feats=aggregate_feats)  # 进入SageLayer。weight*concat(node,neighbors)\n\u001b[0m\u001b[0;32m     46\u001b[0m                         \u001b[0mpre_hidden_embs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcur_hidden_embs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sss\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-f052b845ad20>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, self_feats, aggregate_feats, neighs)\u001b[0m\n\u001b[0;32m     26\u001b[0m \t\t\"\"\"\n\u001b[0;32m     27\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgcn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                         \u001b[0mcombined\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself_feats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maggregate_feats\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# concat自己信息和邻居信息\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m                         \u001b[0mcombined\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maggregate_feats\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(args.epochs):\n",
    "    print('----------------------EPOCH %d-----------------------' % epoch)\n",
    "    # 返回的是模型\n",
    "    graphSage, classification = apply_model(dataCenter, ds, graphSage, classification, unsupervised_loss, args.b_sz, args.unsup_loss, device, args.learn_method)\n",
    "    if (epoch+1) % 2 == 0 and args.learn_method == 'unsup':\n",
    "        classification, args.max_vali_f1 = train_classification(dataCenter, graphSage, classification, ds, device, args.max_vali_f1, args.name)\n",
    "    if args.learn_method != 'unsup':\n",
    "        args.max_vali_f1 = evaluate(dataCenter, ds, graphSage, classification, device, args.max_vali_f1, args.name, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
