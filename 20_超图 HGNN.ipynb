{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy.io as scio\n",
    "import time\n",
    "import copy\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import pprint as pp\n",
    "import math\n",
    "\n",
    "\n",
    "import yaml\n",
    "import os.path as osp\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dir(folder, mk_dir=True):\n",
    "    if not osp.exists(folder):\n",
    "        if mk_dir:\n",
    "            print(f'making direction {folder}!')\n",
    "            os.mkdir(folder)\n",
    "        else:\n",
    "            raise Exception(f'Not exist direction {folder}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dirs(cfg):\n",
    "    check_dir(cfg['data_root'], mk_dir=False)\n",
    "\n",
    "    check_dir(cfg['result_root'])\n",
    "    check_dir(cfg['ckpt_folder'])\n",
    "    check_dir(cfg['result_sub_folder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_config(dir=r'C:\\Users\\sss\\Desktop\\HGNN-master\\config/config.yaml'):\n",
    "    # add direction join function when parse the yaml file\n",
    "    def join(loader, node):\n",
    "        seq = loader.construct_sequence(node)\n",
    "        return os.path.sep.join(seq)\n",
    "\n",
    "    # add string concatenation function when parse the yaml file\n",
    "    def concat(loader, node):\n",
    "        seq = loader.construct_sequence(node)\n",
    "        seq = [str(tmp) for tmp in seq]\n",
    "        return ''.join(seq)\n",
    "\n",
    "    yaml.add_constructor('!join', join)\n",
    "    yaml.add_constructor('!concat', concat)\n",
    "    with open(dir, 'r') as f:\n",
    "        cfg = yaml.load(f)\n",
    "\n",
    "    check_dirs(cfg)\n",
    "\n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making direction C:\\Users\\sss\\Desktop\\HGNN-master\\result/hgnn\\hypergraph_NTU2012!\n"
     ]
    }
   ],
   "source": [
    "cfg = get_config(r'C:\\Users\\sss\\Desktop\\HGNN-master\\config/config.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize data\n",
    "data_dir = cfg['modelnet40_ft'] if cfg['on_dataset'] == 'ModelNet40' else cfg['ntu2012_ft']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ft(data_dir, feature_name='GVCNN'):\n",
    "    data = scio.loadmat(data_dir)\n",
    "    lbls = data['Y'].astype(np.long)\n",
    "    if lbls.min() == 1:\n",
    "        lbls = lbls - 1\n",
    "    idx = data['indices'].item()\n",
    "\n",
    "    if feature_name == 'MVCNN':\n",
    "        fts = data['X'][0].item().astype(np.float32)\n",
    "    elif feature_name == 'GVCNN':\n",
    "        fts = data['X'][1].item().astype(np.float32)\n",
    "    else:\n",
    "        print(f'wrong feature name{feature_name}!')\n",
    "        raise IOError\n",
    "\n",
    "    idx_train = np.where(idx == 1)[0]\n",
    "    idx_test = np.where(idx == 0)[0]\n",
    "    return fts, lbls, idx_train, idx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Eu_dis(x):\n",
    "    \"\"\"\n",
    "    Calculate the distance among each raw of x\n",
    "    :param x: N X D\n",
    "                N: the object number\n",
    "                D: Dimension of the feature\n",
    "    :return: N X N distance matrix\n",
    "    \"\"\"\n",
    "    x = np.mat(x)\n",
    "    aa = np.sum(np.multiply(x, x), 1)\n",
    "    ab = x * x.T\n",
    "    dist_mat = aa + aa.T - 2 * ab\n",
    "    dist_mat[dist_mat < 0] = 0\n",
    "    dist_mat = np.sqrt(dist_mat)\n",
    "    dist_mat = np.maximum(dist_mat, dist_mat.T)\n",
    "    return dist_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_concat(*F_list, normal_col=False):\n",
    "    \"\"\"\n",
    "    Concatenate multiple modality feature. If the dimension of a feature matrix is more than two,\n",
    "    the function will reduce it into two dimension(using the last dimension as the feature dimension,\n",
    "    the other dimension will be fused as the object dimension)\n",
    "    :param F_list: Feature matrix list\n",
    "    :param normal_col: normalize each column of the feature\n",
    "    :return: Fused feature matrix\n",
    "    \"\"\"\n",
    "    features = None\n",
    "    for f in F_list:\n",
    "        if f is not None and f != []:\n",
    "            # deal with the dimension that more than two\n",
    "            if len(f.shape) > 2:\n",
    "                f = f.reshape(-1, f.shape[-1])\n",
    "            # normal each column\n",
    "            if normal_col:\n",
    "                f_max = np.max(np.abs(f), axis=0)\n",
    "                f = f / f_max\n",
    "            # facing the first feature matrix appended to fused feature matrix\n",
    "            if features is None:\n",
    "                features = f\n",
    "            else:\n",
    "                features = np.hstack((features, f))\n",
    "    if normal_col:\n",
    "        features_max = np.max(np.abs(features), axis=0)\n",
    "        features = features / features_max\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperedge_concat(*H_list):\n",
    "    \"\"\"\n",
    "    Concatenate hyperedge group in H_list\n",
    "    :param H_list: Hyperedge groups which contain two or more hypergraph incidence matrix\n",
    "    :return: Fused hypergraph incidence matrix\n",
    "    \"\"\"\n",
    "    H = None\n",
    "    for h in H_list:\n",
    "        if h is not None and h != []:\n",
    "            # for the first H appended to fused hypergraph incidence matrix\n",
    "            if H is None:\n",
    "                H = h\n",
    "            else:\n",
    "                if type(h) != list:\n",
    "                    H = np.hstack((H, h))\n",
    "                else:\n",
    "                    tmp = []\n",
    "                    for a, b in zip(H, h):\n",
    "                        tmp.append(np.hstack((a, b)))\n",
    "                    H = tmp\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_G_from_H(H, variable_weight=False):\n",
    "    \"\"\"\n",
    "    calculate G from hypgraph incidence matrix H\n",
    "    :param H: hypergraph incidence matrix H\n",
    "    :param variable_weight: whether the weight of hyperedge is variable\n",
    "    :return: G\n",
    "    \"\"\"\n",
    "    if type(H) != list:\n",
    "        return _generate_G_from_H(H, variable_weight)\n",
    "    else:\n",
    "        G = []\n",
    "        for sub_H in H:\n",
    "            G.append(generate_G_from_H(sub_H, variable_weight))\n",
    "        return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _generate_G_from_H(H, variable_weight=False):\n",
    "    \"\"\"\n",
    "    calculate G from hypgraph incidence matrix H\n",
    "    :param H: hypergraph incidence matrix H\n",
    "    :param variable_weight: whether the weight of hyperedge is variable\n",
    "    :return: G\n",
    "    \"\"\"\n",
    "    H = np.array(H)\n",
    "    n_edge = H.shape[1]\n",
    "    # the weight of the hyperedge\n",
    "    W = np.ones(n_edge)\n",
    "    # the degree of the node\n",
    "    DV = np.sum(H * W, axis=1)\n",
    "    # the degree of the hyperedge\n",
    "    DE = np.sum(H, axis=0)\n",
    "\n",
    "    invDE = np.mat(np.diag(np.power(DE, -1)))\n",
    "    DV2 = np.mat(np.diag(np.power(DV, -0.5)))\n",
    "    W = np.mat(np.diag(W))\n",
    "    H = np.mat(H)\n",
    "    HT = H.T\n",
    "\n",
    "    if variable_weight:\n",
    "        DV2_H = DV2 * H\n",
    "        invDE_HT_DV2 = invDE * HT * DV2\n",
    "        return DV2_H, W, invDE_HT_DV2\n",
    "    else:\n",
    "        G = DV2 * H * W * invDE * HT * DV2\n",
    "        return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_H_with_KNN_from_distance(dis_mat, k_neig, is_probH=True, m_prob=1):\n",
    "    \"\"\"\n",
    "    construct hypregraph incidence matrix from hypergraph node distance matrix\n",
    "    :param dis_mat: node distance matrix\n",
    "    :param k_neig: K nearest neighbor\n",
    "    :param is_probH: prob Vertex-Edge matrix or binary\n",
    "    :param m_prob: prob\n",
    "    :return: N_object X N_hyperedge\n",
    "    \"\"\"\n",
    "    n_obj = dis_mat.shape[0]\n",
    "    # construct hyperedge from the central feature space of each node\n",
    "    n_edge = n_obj\n",
    "    H = np.zeros((n_obj, n_edge))\n",
    "    for center_idx in range(n_obj):\n",
    "        dis_mat[center_idx, center_idx] = 0\n",
    "        dis_vec = dis_mat[center_idx]\n",
    "        nearest_idx = np.array(np.argsort(dis_vec)).squeeze()\n",
    "        avg_dis = np.average(dis_vec)\n",
    "        if not np.any(nearest_idx[:k_neig] == center_idx):\n",
    "            nearest_idx[k_neig - 1] = center_idx\n",
    "\n",
    "        for node_idx in nearest_idx[:k_neig]:\n",
    "            if is_probH:\n",
    "                H[node_idx, center_idx] = np.exp(-dis_vec[0, node_idx] ** 2 / (m_prob * avg_dis) ** 2)\n",
    "            else:\n",
    "                H[node_idx, center_idx] = 1.0\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_H_with_KNN(X, K_neigs=[10], split_diff_scale=False, is_probH=True, m_prob=1):\n",
    "    \"\"\"\n",
    "    init multi-scale hypergraph Vertex-Edge matrix from original node feature matrix\n",
    "    :param X: N_object x feature_number\n",
    "    :param K_neigs: the number of neighbor expansion\n",
    "    :param split_diff_scale: whether split hyperedge group at different neighbor scale\n",
    "    :param is_probH: prob Vertex-Edge matrix or binary\n",
    "    :param m_prob: prob\n",
    "    :return: N_object x N_hyperedge\n",
    "    \"\"\"\n",
    "    if len(X.shape) != 2:\n",
    "        X = X.reshape(-1, X.shape[-1])\n",
    "\n",
    "    if type(K_neigs) == int:\n",
    "        K_neigs = [K_neigs]\n",
    "\n",
    "    dis_mat = Eu_dis(X)\n",
    "    H = []\n",
    "    for k_neig in K_neigs:\n",
    "        H_tmp = construct_H_with_KNN_from_distance(dis_mat, k_neig, is_probH, m_prob)\n",
    "        if not split_diff_scale:\n",
    "            H = hyperedge_concat(H, H_tmp)\n",
    "        else:\n",
    "            H.append(H_tmp)\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_feature_construct_H(data_dir, m_prob=1, K_neigs=[10], is_probH=True, split_diff_scale=False, use_mvcnn_feature=False, use_gvcnn_feature=True, use_mvcnn_feature_for_structure=False, use_gvcnn_feature_for_structure=True):\n",
    "    \"\"\"\n",
    "    :param data_dir: directory of feature data\n",
    "    :param m_prob: parameter in hypergraph incidence matrix construction\n",
    "    :param K_neigs: the number of neighbor expansion\n",
    "    :param is_probH: probability Vertex-Edge matrix or binary\n",
    "    :param use_mvcnn_feature:\n",
    "    :param use_gvcnn_feature:\n",
    "    :param use_mvcnn_feature_for_structure:\n",
    "    :param use_gvcnn_feature_for_structure:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # init feature\n",
    "    if use_mvcnn_feature or use_mvcnn_feature_for_structure:\n",
    "        mvcnn_ft, lbls, idx_train, idx_test = load_ft(data_dir, feature_name='MVCNN')\n",
    "    if use_gvcnn_feature or use_gvcnn_feature_for_structure:\n",
    "        gvcnn_ft, lbls, idx_train, idx_test = load_ft(data_dir, feature_name='GVCNN')\n",
    "    if 'mvcnn_ft' not in dir() and 'gvcnn_ft' not in dir():\n",
    "        raise Exception('None feature initialized')\n",
    "\n",
    "    # construct feature matrix\n",
    "    fts = None\n",
    "    if use_mvcnn_feature:\n",
    "        fts = feature_concat(fts, mvcnn_ft)\n",
    "    if use_gvcnn_feature:\n",
    "        fts = feature_concat(fts, gvcnn_ft)\n",
    "    if fts is None:\n",
    "        raise Exception(f'None feature used for model!')\n",
    "\n",
    "    # construct hypergraph incidence matrix\n",
    "    print('Constructing hypergraph incidence matrix! \\n(It may take several minutes! Please wait patiently!)')\n",
    "    H = None\n",
    "    if use_mvcnn_feature_for_structure:\n",
    "        tmp = construct_H_with_KNN(mvcnn_ft, K_neigs=K_neigs,\n",
    "                                        split_diff_scale=split_diff_scale,\n",
    "                                        is_probH=is_probH, m_prob=m_prob)\n",
    "        H = hyperedge_concat(H, tmp)\n",
    "    if use_gvcnn_feature_for_structure:\n",
    "        tmp = construct_H_with_KNN(gvcnn_ft, K_neigs=K_neigs,\n",
    "                                        split_diff_scale=split_diff_scale,\n",
    "                                        is_probH=is_probH, m_prob=m_prob)\n",
    "        H = hyperedge_concat(H, tmp)\n",
    "    if H is None:\n",
    "        raise Exception('None feature to construct hypergraph incidence matrix!')\n",
    "\n",
    "    return fts, lbls, idx_train, idx_test, H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sss\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing hypergraph incidence matrix! \n",
      "(It may take several minutes! Please wait patiently!)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sss\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:9: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "fts, lbls, idx_train, idx_test, H = load_feature_construct_H(\n",
    "    data_dir,\n",
    "    m_prob=cfg['m_prob'],\n",
    "    K_neigs=cfg['K_neigs'],\n",
    "    is_probH=cfg['is_probH'],\n",
    "    use_mvcnn_feature=cfg['use_mvcnn_feature'],\n",
    "    use_gvcnn_feature=cfg['use_gvcnn_feature'],\n",
    "    use_mvcnn_feature_for_structure=cfg['use_mvcnn_feature_for_structure'],\n",
    "    use_gvcnn_feature_for_structure=cfg['use_gvcnn_feature_for_structure']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = generate_G_from_H(H)\n",
    "n_class = int(lbls.max()) + 1\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "if device == \"cuda\":\n",
    "    fts = torch.Tensor(fts).to(device)\n",
    "    lbls = torch.Tensor(lbls).squeeze().long().to(device)\n",
    "    G = torch.Tensor(G).to(device)\n",
    "    idx_train = torch.Tensor(idx_train).long().to(device)\n",
    "    idx_test = torch.Tensor(idx_test).long().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HGNN_fc(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(HGNN_fc, self).__init__()\n",
    "        self.fc = nn.Linear(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HGNN_embedding(nn.Module):\n",
    "    def __init__(self, in_ch, n_hid, dropout=0.5):\n",
    "        super(HGNN_embedding, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.hgc1 = HGNN_conv(in_ch, n_hid)\n",
    "        self.hgc2 = HGNN_conv(n_hid, n_hid)\n",
    "\n",
    "    def forward(self, x, G):\n",
    "        x = F.relu(self.hgc1(x, G))\n",
    "        x = F.dropout(x, self.dropout)\n",
    "        x = F.relu(self.hgc2(x, G))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HGNN_classifier(nn.Module):\n",
    "    def __init__(self, n_hid, n_class):\n",
    "        super(HGNN_classifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(n_hid, n_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HGNN_conv(nn.Module):\n",
    "    def __init__(self, in_ft, out_ft, bias=True):\n",
    "        super(HGNN_conv, self).__init__()\n",
    "\n",
    "        self.weight = Parameter(torch.Tensor(in_ft, out_ft))\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.Tensor(out_ft))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, G: torch.Tensor):\n",
    "        x = x.matmul(self.weight)\n",
    "        if self.bias is not None:\n",
    "            x = x + self.bias\n",
    "        x = G.matmul(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HGNN(nn.Module):\n",
    "    def __init__(self, in_ch, n_class, n_hid, dropout=0.5):\n",
    "        super(HGNN, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.hgc1 = HGNN_conv(in_ch, n_hid)\n",
    "        self.hgc2 = HGNN_conv(n_hid, n_class)\n",
    "\n",
    "    def forward(self, x, G):\n",
    "        x = F.relu(self.hgc1(x, G))\n",
    "        x = F.dropout(x, self.dropout)\n",
    "        x = self.hgc2(x, G)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25, print_freq=500):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        if epoch % print_freq == 0:\n",
    "            print('-' * 10)\n",
    "            print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            idx = idx_train if phase == 'train' else idx_test\n",
    "\n",
    "            # Iterate over data.\n",
    "            optimizer.zero_grad()\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                outputs = model(fts, G)\n",
    "                loss = criterion(outputs[idx], lbls[idx])\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "            # statistics\n",
    "            running_loss += loss.item() * fts.size(0)\n",
    "            running_corrects += torch.sum(preds[idx] == lbls.data[idx])\n",
    "\n",
    "            epoch_loss = running_loss / len(idx)\n",
    "            epoch_acc = running_corrects.double() / len(idx)\n",
    "\n",
    "            if epoch % print_freq == 0:\n",
    "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        if epoch % print_freq == 0:\n",
    "            print(f'Best val Acc: {best_acc:4f}')\n",
    "            print('-' * 20)\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'\\nTraining complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 主函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _main():\n",
    "    print(f\"Classification on {cfg['on_dataset']} dataset!!! class number: {n_class}\")\n",
    "    print(f\"use MVCNN feature: {cfg['use_mvcnn_feature']}\")\n",
    "    print(f\"use GVCNN feature: {cfg['use_gvcnn_feature']}\")\n",
    "    print(f\"use MVCNN feature for structure: {cfg['use_mvcnn_feature_for_structure']}\")\n",
    "    print(f\"use GVCNN feature for structure: {cfg['use_gvcnn_feature_for_structure']}\")\n",
    "    print('Configuration -> Start')\n",
    "    # pp.pprint(cfg)\n",
    "    print('Configuration -> End')\n",
    "    print(\"**\" * 20)\n",
    "\n",
    "    model_ft = HGNN(\n",
    "        in_ch=fts.shape[1],\n",
    "        n_class=n_class,\n",
    "        n_hid=cfg['n_hid'],\n",
    "        dropout=cfg['drop_out']\n",
    "    )\n",
    "    model_ft = model_ft.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model_ft.parameters(), lr=cfg['lr'], weight_decay=cfg['weight_decay'])\n",
    "    # optimizer = optim.SGD(model_ft.parameters(), lr=0.01, weight_decay=cfg['weight_decay)\n",
    "    schedular = optim.lr_scheduler.MultiStepLR(\n",
    "        optimizer,\n",
    "        milestones=cfg['milestones'],\n",
    "        gamma=cfg['gamma']\n",
    "    )\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    model_ft = train_model(model_ft, criterion, optimizer, schedular, cfg['max_epoch'], print_freq=cfg['print_freq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification on NTU2012 dataset!!! class number: 67\n",
      "use MVCNN feature: False\n",
      "use GVCNN feature: True\n",
      "use MVCNN feature for structure: True\n",
      "use GVCNN feature for structure: True\n",
      "Configuration -> Start\n",
      "{'K_neigs': [10],\n",
      " 'ckpt_folder': 'C:\\\\Users\\\\sss\\\\Desktop\\\\HGNN-master\\\\result/hgnn\\\\ckpt',\n",
      " 'data_root': 'C:\\\\Users\\\\sss\\\\Desktop\\\\HGNN-master\\\\data/features',\n",
      " 'decay_rate': 0.7,\n",
      " 'decay_step': 200,\n",
      " 'drop_out': 0.5,\n",
      " 'gamma': 0.9,\n",
      " 'graph_type': 'hypergraph',\n",
      " 'is_probH': True,\n",
      " 'lr': 0.001,\n",
      " 'm_prob': 1.0,\n",
      " 'max_epoch': 600,\n",
      " 'milestones': [100],\n",
      " 'modelnet40_ft': 'C:\\\\Users\\\\sss\\\\Desktop\\\\HGNN-master\\\\data/features\\\\ModelNet40_mvcnn_gvcnn.mat',\n",
      " 'n_hid': 128,\n",
      " 'ntu2012_ft': 'C:\\\\Users\\\\sss\\\\Desktop\\\\HGNN-master\\\\data/features\\\\NTU2012_mvcnn_gvcnn.mat',\n",
      " 'on_dataset': 'NTU2012',\n",
      " 'print_freq': 50,\n",
      " 'result_root': 'C:\\\\Users\\\\sss\\\\Desktop\\\\HGNN-master\\\\result/hgnn',\n",
      " 'result_sub_folder': 'C:\\\\Users\\\\sss\\\\Desktop\\\\HGNN-master\\\\result/hgnn\\\\hypergraph_NTU2012',\n",
      " 'use_gvcnn_feature': True,\n",
      " 'use_gvcnn_feature_for_structure': True,\n",
      " 'use_mvcnn_feature': False,\n",
      " 'use_mvcnn_feature_for_structure': True,\n",
      " 'weight_decay': 0.0005}\n",
      "Configuration -> End\n",
      "----------\n",
      "Epoch 0/599\n",
      "train Loss: 5.2185 Acc: 0.0201\n",
      "val Loss: 21.7326 Acc: 0.0992\n",
      "Best val Acc: 0.099196\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sss\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "Epoch 50/599\n",
      "train Loss: 0.7057 Acc: 0.8993\n",
      "val Loss: 5.2324 Acc: 0.7694\n",
      "Best val Acc: 0.782842\n",
      "--------------------\n",
      "----------\n",
      "Epoch 100/599\n",
      "train Loss: 0.3850 Acc: 0.9347\n",
      "val Loss: 4.3641 Acc: 0.8231\n",
      "Best val Acc: 0.823056\n",
      "--------------------\n",
      "----------\n",
      "Epoch 150/599\n",
      "train Loss: 0.2834 Acc: 0.9469\n",
      "val Loss: 4.4748 Acc: 0.8043\n",
      "Best val Acc: 0.828418\n",
      "--------------------\n",
      "----------\n",
      "Epoch 200/599\n",
      "train Loss: 0.2504 Acc: 0.9536\n",
      "val Loss: 4.5410 Acc: 0.8231\n",
      "Best val Acc: 0.833780\n",
      "--------------------\n",
      "----------\n",
      "Epoch 250/599\n",
      "train Loss: 0.2120 Acc: 0.9628\n",
      "val Loss: 4.6265 Acc: 0.8204\n",
      "Best val Acc: 0.833780\n",
      "--------------------\n",
      "----------\n",
      "Epoch 300/599\n",
      "train Loss: 0.1960 Acc: 0.9640\n",
      "val Loss: 4.5542 Acc: 0.8123\n",
      "Best val Acc: 0.839142\n",
      "--------------------\n",
      "----------\n",
      "Epoch 350/599\n",
      "train Loss: 0.1817 Acc: 0.9591\n",
      "val Loss: 4.8348 Acc: 0.8204\n",
      "Best val Acc: 0.839142\n",
      "--------------------\n",
      "----------\n",
      "Epoch 400/599\n",
      "train Loss: 0.1635 Acc: 0.9683\n",
      "val Loss: 4.8651 Acc: 0.8177\n",
      "Best val Acc: 0.839142\n",
      "--------------------\n",
      "----------\n",
      "Epoch 450/599\n",
      "train Loss: 0.1598 Acc: 0.9701\n",
      "val Loss: 5.0475 Acc: 0.8123\n",
      "Best val Acc: 0.839142\n",
      "--------------------\n",
      "----------\n",
      "Epoch 500/599\n",
      "train Loss: 0.1505 Acc: 0.9738\n",
      "val Loss: 4.9320 Acc: 0.8150\n",
      "Best val Acc: 0.839142\n",
      "--------------------\n",
      "----------\n",
      "Epoch 550/599\n",
      "train Loss: 0.1374 Acc: 0.9725\n",
      "val Loss: 4.9621 Acc: 0.8177\n",
      "Best val Acc: 0.839142\n",
      "--------------------\n",
      "\n",
      "Training complete in 1m 17s\n",
      "Best val Acc: 0.839142\n"
     ]
    }
   ],
   "source": [
    "_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
