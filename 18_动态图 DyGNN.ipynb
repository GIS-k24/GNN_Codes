{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import choice\n",
    "\n",
    "import random\n",
    "import time\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "\n",
    "from scipy.stats import rankdata\n",
    "from scipy.sparse import lil_matrix, find"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decayer(nn.Module):\n",
    "    def __init__(self, device, w, decay_method='exp'):\n",
    "        super(Decayer,self).__init__()\n",
    "        self.decay_method = decay_method\n",
    "        self.linear = nn.Linear(1,1,False).to(device)\n",
    "        self.w = w\n",
    "\n",
    "    def exponetial_decay(self, delta_t):\n",
    "        return torch.exp(-self.w*delta_t)\n",
    "    \n",
    "    def log_decay(self, delta_t):\n",
    "        return 1/torch.log(2.7183 + self.w*delta_t)\n",
    "    \n",
    "    def rev_decay(self, delta_t):\n",
    "        return 1/(1 + self.w*delta_t)\n",
    "\n",
    "    def forward(self,delta_t):\n",
    "        if self.decay_method == 'exp':\n",
    "            return self.exponetial_decay(delta_t)\n",
    "        elif self.decay_method == 'log':\n",
    "            return self.log_decay(delta_t)\n",
    "        elif self.decay_method == 'rev':\n",
    "            return self.rev_decay(delta_t)\n",
    "        else:\n",
    "            return self.exponetial_decay(delta_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Combiner(nn.Module):\n",
    "    def __init__(self, input_size, output_size,act, bias = True ):\n",
    "        super(Combiner,self).__init__()\n",
    "        self.h2o = nn.Linear(input_size,output_size,bias)\n",
    "        self.l2o = nn.Linear(input_size,output_size,bias)\n",
    "        if act == 'tanh':\n",
    "            self.act = nn.Tanh()\n",
    "        elif act == 'sigmoid':\n",
    "            self.act = nn.Sigmoid()\n",
    "        else:\n",
    "            self.act = nn.ReLU() \n",
    "\n",
    "    def forward(self, head_info, tail_info):\n",
    "        node_output = self.h2o(head_info) + self.l2o(tail_info)\n",
    "        node_output_tanh = self.act(node_output)\n",
    "        return node_output_tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Edge_updater_nn(nn.Module):\n",
    "    def __init__(self, node_input_size, output_size , act = 'tanh',relation_input_size = None, bias = True):\n",
    "        super(Edge_updater_nn,self).__init__()\n",
    "        self.h2o = nn.Linear(node_input_size,output_size,bias)\n",
    "        self.l2o = nn.Linear(node_input_size,output_size,bias)\n",
    "        if relation_input_size is not None:\n",
    "            self.r2o = nn.Linear(relation_input_size,output_size,bias)\n",
    "        if act == 'tanh':\n",
    "            self.act = nn.Tanh()\n",
    "        elif act == 'sigmoid':\n",
    "            self.act = nn.Sigmoid()\n",
    "        else:\n",
    "            self.act = nn.ReLU() \n",
    "\n",
    "    def forward(self, head_node, tail_node, relation=None):\n",
    "\n",
    "        if relation is None:\n",
    "            edge_output = self.h2o(head_node) + self.l2o(tail_node)\n",
    "        else:\n",
    "            edge_output = self.h2o(head_node) + self.l2o(tail_node) + self.r2o(relation)\n",
    "        edge_output_act = self.act(edge_output)\n",
    "        return edge_output_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TLSTM(nn.Module):\n",
    "    def __init__(self,input_size, hidden_size,  bias = True):\n",
    "        super(TLSTM,self).__init__()\n",
    "        self.i2h = nn.Linear(input_size, 4*hidden_size, bias)\n",
    "        self.h2h = nn.Linear(hidden_size, 4*hidden_size, bias)\n",
    "        self.c2s = nn.Sequential(nn.Linear(hidden_size, hidden_size, bias), nn.Tanh())\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self,input, cell, hidden, transed_delta_t):\n",
    "        cell_short = self.c2s(cell)\n",
    "        cell_new = cell - cell_short + cell_short* transed_delta_t \n",
    "        gates = self.i2h(input) + self.h2h(hidden)\n",
    "        ingate, forgate, cellgate, outgate = gates.chunk(4,1)\n",
    "        ingate = self.sigmoid(ingate)\n",
    "        forgate = self.sigmoid(forgate)\n",
    "        cellgate = self.tanh(cellgate)\n",
    "        outgate = self.sigmoid(outgate)\n",
    "        cell_output = forgate*cell_new + ingate*cellgate\n",
    "        hidden_output = outgate*self.tanh(cell_output) \n",
    "        \n",
    "        return cell_output, hidden_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, embedding_dims):\n",
    "        super(Attention,self).__init__()\n",
    "        self.bilinear = nn.Bilinear(embedding_dims,embedding_dims,1)\n",
    "        self.softmax = nn.Softmax(0)\n",
    "\n",
    "    def forward(self,node1, node2):\n",
    "        return self.softmax( self.bilinear(node1, node2).view(-1,1) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DyGNN(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dims, edge_output_size, device, w, is_att=False, transfer=False, nor=0, if_no_time=0, threhold=None, second_order=False, if_updated=0, drop_p=0, num_negative=5, act='tanh', if_propagation=1, decay_method='exp', weight=None, relation_size=None, bias=True):\n",
    "        super(DyGNN, self).__init__()\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.nor = nor\n",
    "        \n",
    "        # self.weight = weight.to(device)\n",
    "        self.device = device\n",
    "        self.transfer = transfer\n",
    "        self.if_propagation = if_propagation\n",
    "        self.if_no_time = if_no_time\n",
    "        self.second_order = second_order\n",
    "        \n",
    "        # self.cuda = cuda\n",
    "        self.combiner = Combiner(embedding_dims, embedding_dims, act).to(device)\n",
    "        self.decay_method = decay_method\n",
    "        self.if_updated = if_updated\n",
    "        self.threhold = threhold\n",
    "        \n",
    "        print('Only propagate to relevance nodes below time interval: ', threhold)\n",
    "        \n",
    "        # self.tanh = nn.Tanh().to(device)\n",
    "        if act == 'tanh':\n",
    "            self.act = nn.Tanh().to(device)\n",
    "        elif act == 'sigmoid':\n",
    "            self.act = nn.Sigmoid().to(device)\n",
    "        else:\n",
    "            self.act = nn.ReLU().to(device) \n",
    "            \n",
    "        self.decayer = Decayer(device, w, decay_method)\n",
    "        self.edge_updater_head = Edge_updater_nn(embedding_dims, edge_output_size,act, relation_size).to(device)\n",
    "        self.edge_updater_tail = Edge_updater_nn(embedding_dims, edge_output_size,act, relation_size).to(device)\n",
    "\n",
    "        if if_no_time:\n",
    "            self.node_updater_head = nn.LSTMCell(edge_output_size, embedding_dims, bias).to(device)\n",
    "            self.node_updater_tail = nn.LSTMCell(edge_output_size, embedding_dims, bias).to(device) \n",
    "        else:\n",
    "            self.node_updater_head = TLSTM(edge_output_size, embedding_dims).to(device)\n",
    "            self.node_updater_tail = TLSTM(edge_output_size, embedding_dims).to(device)\t\n",
    "\n",
    "        self.tran_head_edge_head = nn.Linear(edge_output_size, embedding_dims, bias).to(device)\n",
    "        self.tran_head_edge_tail = nn.Linear(edge_output_size, embedding_dims, bias).to(device)\t\n",
    "        self.tran_tail_edge_head = nn.Linear(edge_output_size, embedding_dims, bias).to(device)\n",
    "        self.tran_tail_edge_tail = nn.Linear(edge_output_size, embedding_dims, bias).to(device)\n",
    "        \n",
    "        self.is_att = is_att\n",
    "        if self.is_att:\n",
    "            self.attention = Attention(embedding_dims).to(device)\n",
    "\n",
    "        self.num_negative = num_negative\n",
    "\n",
    "        self.recent_timestamp = torch.zeros((num_embeddings, 1), dtype = torch.float, requires_grad = False).to(device)\n",
    "\n",
    "        self.interaction_timestamp = lil_matrix((num_embeddings,num_embeddings),dtype = np.float32)\n",
    "        \n",
    "        self.cell_head = nn.Embedding(num_embeddings, embedding_dims, weight).to(device)\n",
    "        self.cell_head.weight.requires_grad = False\n",
    "        self.cell_tail = nn.Embedding(num_embeddings, embedding_dims, weight).to(device)\n",
    "        self.cell_tail.weight.requires_grad = False\n",
    "        \n",
    "        self.hidden_head = nn.Embedding(num_embeddings, embedding_dims, weight).to(device)\n",
    "        self.hidden_head.weight.requires_grad = False\n",
    "        self.hidden_tail = nn.Embedding(num_embeddings, embedding_dims, weight).to(device)\n",
    "        self.hidden_tail.weight.requires_grad = False\n",
    "        \n",
    "        self.node_representations = nn.Embedding(num_embeddings, embedding_dims, weight).to(device)\n",
    "        self.node_representations.weight.requires_grad = False\n",
    "\n",
    "        if transfer:\n",
    "            self.transfer2head = nn.Linear(embedding_dims, embedding_dims, False).to(device)\n",
    "            self.transfer2tail = nn.Linear(embedding_dims, embedding_dims, False).to(device)\n",
    "            if drop_p>=0:\n",
    "                self.dropout = nn.Dropout(p=drop_p).to(device)\n",
    "                \n",
    "        self.cell_head_copy = nn.Embedding.from_pretrained(self.cell_head.weight.clone()).to(device)\n",
    "        self.cell_tail_copy = nn.Embedding.from_pretrained(self.cell_tail.weight.clone()).to(device)\n",
    "        self.hidden_head_copy = nn.Embedding.from_pretrained(self.hidden_head.weight.clone()).to(device)\n",
    "        self.hidden_tail_copy = nn.Embedding.from_pretrained(self.hidden_tail.weight.clone()).to(device)\n",
    "        self.node_representations_copy = nn.Embedding.from_pretrained(self.node_representations.weight.clone()).to(device)\n",
    "\n",
    "        # if cuda:\n",
    "        #     self.cell_head = self.cell_head.cuda()\n",
    "        #     self.cell_tail = self.cell_tail.cuda()\n",
    "        #     self.node_representations = self.node_representations.cuda()\n",
    "        #     self.recent_timestamp = self.recent_timestamp.cuda()\n",
    "        #     self.tran_head_edge_head.cuda()\n",
    "        #     self.tran_head_edge_head.cuda()\n",
    "        #     self.tran_tail_edge_head.cuda()\n",
    "        #     self.tran_tail_edge_tail.cuda()\n",
    "        \n",
    "    def reset_time(self):\n",
    "        self.recent_timestamp = torch.zeros((self.num_embeddings, 1), dtype = torch.float, requires_grad = False).to(self.device)\n",
    "        self.interaction_timestamp = lil_matrix((self.num_embeddings,self.num_embeddings),dtype = np.float32)\n",
    "    \n",
    "    def reset_reps(self):\n",
    "        self.cell_head = nn.Embedding.from_pretrained(self.cell_head_copy.weight.clone()).to(self.device)\n",
    "        self.cell_tail = nn.Embedding.from_pretrained(self.cell_tail_copy.weight.clone()).to(self.device)\n",
    "        self.hidden_head = nn.Embedding.from_pretrained(self.hidden_head_copy.weight.clone()).to(self.device)\n",
    "        self.hidden_tail = nn.Embedding.from_pretrained(self.hidden_tail_copy.weight.clone()).to(self.device)\n",
    "        self.node_representations = nn.Embedding.from_pretrained(self.node_representations_copy.weight.clone()).to(self.device)\n",
    "        \n",
    "    def link_pred_with_update(self,test_data):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def forward(self, interactions):\n",
    "        test_time = False\n",
    "        \n",
    "        all_head_nodes = set()\n",
    "        all_tail_nodes = set()\n",
    "        \n",
    "        steps = len(interactions[:,0])\n",
    "\n",
    "        node2timetsamp = dict()\n",
    "        node2cell_head = dict()\n",
    "        node2cell_tail = dict()\n",
    "        node2hidden_head = dict()\n",
    "        node2hidden_tail = dict()\n",
    "        node2rep = dict()\n",
    "        \n",
    "        output_rep_head = []\n",
    "        output_rep_tail = []\n",
    "        tail_neg_list = []\n",
    "        head_neg_list = []\n",
    "        \n",
    "        if test_time:\n",
    "            old_time = time.time()\n",
    "        for i in range(steps):\n",
    "            i_condi = i%200 == 1\n",
    "            if test_time and i_condi:\n",
    "                time1 = time.time() \n",
    "                print('----------------------------------------------------')\n",
    "                print(i,'1 step time', str(time1 - old_time) )\n",
    "                old_time = time1\n",
    "                \n",
    "            head_index = int(interactions[i,0])\n",
    "            tail_index = int(interactions[i,1])\n",
    "            all_head_nodes.add(head_index)\n",
    "            all_tail_nodes.add(tail_index)\n",
    "            \n",
    "            head_inx_lt = torch.LongTensor([head_index]).to(self.device)\n",
    "            tail_inx_lt = torch.LongTensor([tail_index]).to(self.device)\n",
    "            \n",
    "            timestamp = interactions[i,2]\n",
    "            current_t = torch.FloatTensor([timestamp]).view(-1,1).to(self.device)\n",
    "\n",
    "            head_prev_t = self.recent_timestamp[head_index]\n",
    "            tail_prev_t = self.recent_timestamp[tail_index]\n",
    "\n",
    "            if test_time and i_condi:\n",
    "                time2 = time.time()\n",
    "                print('test_point2', str(time2-time1))\n",
    "\n",
    "            if head_index in node2rep:\n",
    "                head_node_rep = node2rep[head_index]\n",
    "            else:\n",
    "                head_node_rep = self.node_representations(head_inx_lt)\n",
    "\n",
    "            if tail_index in node2rep:\n",
    "                tail_node_rep = node2rep[tail_index]\n",
    "            else:\n",
    "                tail_node_rep = self.node_representations(tail_inx_lt)\n",
    "                \n",
    "                \n",
    "            if head_index in node2hidden_head:\n",
    "                head_node_cell_head = node2cell_head[head_index]\n",
    "                head_node_hidden_head = node2hidden_head[head_index]\n",
    "            else:\n",
    "                head_node_cell_head = self.cell_head(head_inx_lt)\n",
    "                head_node_hidden_head = self.hidden_head(head_inx_lt)\n",
    "            if head_index in node2hidden_tail:\n",
    "                head_node_hidden_tail = node2hidden_tail[head_index]\n",
    "            else:\n",
    "                head_node_hidden_tail = self.hidden_tail(head_inx_lt)\n",
    "                \n",
    "            \n",
    "            if tail_index in node2hidden_tail:\n",
    "                tail_node_cell_tail = node2cell_tail[tail_index]\n",
    "                tail_node_hidden_tail = node2hidden_tail[tail_index]\n",
    "            else:\n",
    "                tail_node_cell_tail = self.cell_tail(tail_inx_lt)\n",
    "                tail_node_hidden_tail = self.hidden_tail(tail_inx_lt)\n",
    "              \n",
    "            \n",
    "            if tail_index in node2hidden_head:\n",
    "                tail_node_hidden_head = node2hidden_head[tail_index]\n",
    "            else:\n",
    "                tail_node_hidden_head = self.hidden_head(tail_inx_lt)\n",
    "                \n",
    "            \n",
    "            if test_time and i_condi:\n",
    "                time3 = time.time()\n",
    "                print('prepare rep time', str(time3-time2))\n",
    "\n",
    "            head_delta_t = current_t - head_prev_t\n",
    "            tail_delta_t = current_t - tail_prev_t\n",
    "\n",
    "            with torch.no_grad():\n",
    "                self.recent_timestamp[[head_index, tail_index]] = current_t\n",
    "\n",
    "            transed_head_delta_t = self.decayer(head_delta_t)\n",
    "            transed_tail_delta_t = self.decayer(tail_delta_t)\n",
    "\n",
    "            edge_info_head = self.edge_updater_head(head_node_rep, tail_node_rep)\n",
    "            edge_info_tail = self.edge_updater_tail(head_node_rep, tail_node_rep)\n",
    "            \n",
    "            \n",
    "            if self.if_no_time:\n",
    "                updated_head_node_hidden_head,updated_head_node_cell_head  = self.node_updater_head(edge_info_head, ( head_node_hidden_head, head_node_cell_head ))\n",
    "            else:\n",
    "                updated_head_node_cell_head, updated_head_node_hidden_head = self.node_updater_head(edge_info_head, head_node_cell_head, head_node_hidden_head , transed_head_delta_t)\n",
    "\n",
    "            updated_head_node_rep = self.combiner(updated_head_node_hidden_head, head_node_hidden_tail)\n",
    "\n",
    "            node2cell_head[head_index] = updated_head_node_cell_head\n",
    "            node2hidden_head[head_index] = updated_head_node_hidden_head\n",
    "            node2rep[head_index] = updated_head_node_rep\n",
    "            \n",
    "            \n",
    "            if self.if_updated:\n",
    "                output_rep_head.append(updated_head_node_rep)\n",
    "            else:\n",
    "                output_rep_head.append(head_node_rep)\n",
    "\n",
    "            if self.if_no_time:\n",
    "                updated_tail_node_hidden_tail, updated_tail_node_cell_tail, = self.node_updater_tail(edge_info_tail, (tail_node_hidden_tail, tail_node_cell_tail))\n",
    "            else:\n",
    "                updated_tail_node_cell_tail, updated_tail_node_hidden_tail = self.node_updater_tail(edge_info_tail, tail_node_cell_tail, tail_node_hidden_tail, transed_tail_delta_t)\n",
    "            updated_tail_node_rep = self.combiner(tail_node_hidden_head, updated_tail_node_hidden_tail)\n",
    "\n",
    "            node2cell_tail[tail_index] = updated_tail_node_cell_tail\n",
    "            node2hidden_tail[tail_index] = updated_tail_node_hidden_tail\n",
    "            node2rep[tail_index] = updated_tail_node_rep\n",
    "            \n",
    "            \n",
    "            if self.if_updated:\n",
    "                output_rep_tail.append(updated_tail_node_rep)\n",
    "            else:\n",
    "                output_rep_tail.append(tail_node_rep)\n",
    "\n",
    "            if test_time and i_condi:\n",
    "                time4 = time.time()\n",
    "                print('update reps', str(time4-time3))\n",
    "\n",
    "\n",
    "            if self.if_propagation:\n",
    "                head_node_head_neighbors, head_node_tail_neighbors = self.propagation(head_index, current_t, edge_info_head, 'head', node2cell_head, node2hidden_head, node2cell_tail, node2hidden_tail, node2rep, self.threhold, self.second_order)\n",
    "                tail_node_head_neighbors, tail_node_tail_neighbors = self.propagation(tail_index, current_t, edge_info_tail, 'tail', node2cell_head, node2hidden_head, node2cell_tail, node2hidden_tail, node2rep, self.threhold, self.second_order)\n",
    "            else:\n",
    "                head_node_head_neighbors, head_node_tail_neighbors, n_i_1, n_i_2 = self.get_neighbors(head_index,current_t, self.threhold)\n",
    "                tail_node_head_neighbors, tail_node_tail_neighbors, n_i_1, n_i_2 = self.get_neighbors(tail_index, current_t, self.threhold)\n",
    "                head_node_head_neighbors = set(head_node_head_neighbors)\n",
    "                head_node_tail_neighbors = set(head_node_tail_neighbors)\n",
    "                tail_node_head_neighbors = set(tail_node_head_neighbors)\n",
    "                tail_node_tail_neighbors = set(tail_node_tail_neighbors)\n",
    "                \n",
    "            \n",
    "            if test_time and i_condi:\n",
    "                time5 = time.time()\n",
    "                if self.if_propagation:\n",
    "                    print('propagation time', str(time5-time4))\n",
    "                else:\n",
    "                    print('Get neighbors time', str(time5-time4))\n",
    "            all_head_nodes = all_head_nodes | head_node_head_neighbors | tail_node_head_neighbors\n",
    "            all_tail_nodes = all_tail_nodes | head_node_tail_neighbors | tail_node_tail_neighbors\n",
    "\n",
    "            ### generate negative samples ###\n",
    "            tail_candidates = all_tail_nodes - {head_index,tail_index} - head_node_tail_neighbors\n",
    "            if len(tail_candidates)==0:\n",
    "                tail_neg_samples = list(choice(range(self.num_embeddings), size=self.num_negative))\n",
    "\n",
    "            else:\n",
    "                tail_neg_samples = list(choice(list(tail_candidates), size = self.num_negative))\n",
    "\n",
    "            head_candidates = all_head_nodes - {tail_index,head_index}- tail_node_head_neighbors\n",
    "            if len(head_candidates) ==0:\n",
    "                head_neg_samples = list(choice(range(self.num_embeddings), size=self.num_negative))\n",
    "            else:\n",
    "                head_neg_samples = list(choice(list(head_candidates), size = self.num_negative))\n",
    "                \n",
    "            \n",
    "            if test_time and i_condi: \n",
    "                time6 = time.time()\n",
    "                print('get negative samples time', str(time6 - time5))\n",
    "\n",
    "            for i in tail_neg_samples:\n",
    "                if i in node2rep:\n",
    "                    tail_neg_list.append(node2rep[i])\n",
    "                else:\n",
    "                    i_lt = torch.LongTensor([i]).to(self.device)\n",
    "\n",
    "                    tail_neg_list.append(self.node_representations(i_lt))\n",
    "            for i in head_neg_samples:\n",
    "                if i in node2rep:\n",
    "                    head_neg_list.append(node2rep[i])\n",
    "                else:\n",
    "                    i_lt = torch.LongTensor([i]).to(self.device)\n",
    "\n",
    "                    head_neg_list.append(self.node_representations(i_lt))\n",
    "            if test_time and i_condi: \n",
    "                time7 = time.time()\n",
    "                print('Prepare neg reps time', str(time7 - time6))\n",
    "                \n",
    "        ###update interaction time###\n",
    "            self.interaction_timestamp[head_index, tail_index] = current_t[0,0]    \n",
    "        \n",
    "        ###### Prepare modifed cell, hidden and rep to write back to the memory ########\n",
    "        cell_head_inx = list(node2cell_head.keys())\n",
    "        output_cell_head = list(node2cell_head.values())\n",
    "\n",
    "        cell_tail_inx = list(node2cell_tail.keys())\n",
    "        output_cell_tail = list(node2cell_tail.values())\n",
    "\n",
    "\n",
    "        hidden_head_inx = list(node2hidden_head.keys())\n",
    "        output_hidden_head = list(node2hidden_head.values())\n",
    "\n",
    "        hidden_tail_inx = list(node2hidden_tail.keys())\n",
    "        output_hidden_tail = list(node2hidden_tail.values())\n",
    "\n",
    "        rep_inx = list(node2rep.keys())\n",
    "        output_rep = list(node2rep.values())\n",
    "           \n",
    "        \n",
    "        output_cell_head_tensor = torch.cat([*output_cell_head]).view(-1,self.embedding_dims)\n",
    "        output_hidden_head_tensor = torch.cat([*output_hidden_head]).view(-1,self.embedding_dims)\n",
    "        output_rep_head_tensor = torch.cat([*output_rep_head]).view(-1,self.embedding_dims)\n",
    "\n",
    "        output_cell_tail_tensor = torch.cat([*output_cell_tail]).view(-1,self.embedding_dims)\n",
    "        output_hidden_tail_tensor = torch.cat([*output_hidden_tail]).view(-1,self.embedding_dims)\n",
    "        output_rep_tail_tensor = torch.cat([*output_rep_tail]).view(-1,self.embedding_dims)\n",
    "\n",
    "        output_rep_tensor = torch.cat([*output_rep]).view(-1,self.embedding_dims)\n",
    "\n",
    "        tail_neg_tensors = torch.cat([*tail_neg_list]).view(-1,self.embedding_dims)\n",
    "        head_neg_tensors = torch.cat([*head_neg_list]).view(-1,self.embedding_dims)\n",
    "\n",
    "        if self.transfer:\n",
    "            output_rep_head_tensor = self.dropout(self.transfer2head(output_rep_head_tensor))\n",
    "            output_rep_tail_tensor = self.dropout(self.transfer2tail(output_rep_tail_tensor))\n",
    "\n",
    "            head_neg_tensors =self.dropout(self.transfer2head(head_neg_tensors))\n",
    "            tail_neg_tensors = self.dropout(self.transfer2tail(tail_neg_tensors))\n",
    "\n",
    "        if self.nor:\n",
    "            output_rep_head_tensor = nn.functional.normalize(output_rep_head_tensor)\n",
    "            output_rep_tail_tensor = nn.functional.normalize(output_rep_tail_tensor)\n",
    "\n",
    "            head_neg_tensors = nn.functional.normalize(head_neg_tensors)\n",
    "            tail_neg_tensors = nn.functional.normalize(tail_neg_tensors)\n",
    "\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.cell_head.weight[cell_head_inx,:] = output_cell_head_tensor\n",
    "            self.hidden_head.weight[hidden_head_inx,:] = output_hidden_head_tensor\n",
    "\n",
    "            self.cell_tail.weight[cell_tail_inx,:] = output_cell_tail_tensor\n",
    "            self.hidden_tail.weight[hidden_tail_inx,:] = output_hidden_tail_tensor\n",
    "\n",
    "            self.node_representations.weight[rep_inx,:] = output_rep_tensor\n",
    "\n",
    "        return output_rep_head_tensor, output_rep_tail_tensor, head_neg_tensors, tail_neg_tensors\n",
    "    \n",
    "    \n",
    "    def get_rep(self, nodes, rep_type, rep_dict):\n",
    "        if rep_type == 'node_rep':\n",
    "            rep = self.node_representations(torch.LongTensor(nodes).to(self.device))\n",
    "        elif rep_type == 'cell_head':\n",
    "            rep = self.cell_head(torch.LongTensor(nodes).to(self.device))\n",
    "        elif rep_type == 'cell_tail':\n",
    "            rep = self.cell_tail(torch.LongTensor(nodes).to(self.device))\n",
    "        elif rep_type == 'hidden_head':\n",
    "            rep = self.hidden_head(torch.LongTensor(nodes).to(self.device))    \n",
    "        else:\n",
    "            rep = self.hidden_tail(torch.LongTensor(nodes).to(self.device))     \n",
    "        for nei in nodes:\n",
    "            if nei in rep_dict:\n",
    "                rep[nodes.index(nei),:] = rep_dict[nei]    \n",
    "        return  rep  \n",
    "    \n",
    "    \n",
    "    def get_neighbors(self,node,current_t,threhold=None):\n",
    "        row_inx, col_inx, timestamps = find(self.interaction_timestamp) \n",
    "\n",
    "        head_inx = list(np.where(col_inx == node)[0])\n",
    "        head_neighbors = row_inx[head_inx]\n",
    "        head_timestamps = timestamps[head_inx]\n",
    "\n",
    "        tail_inx = list(np.where(row_inx == node)[0])\n",
    "        tail_neighbors = col_inx[tail_inx]\n",
    "        tail_timestamps = timestamps[tail_inx]\n",
    "        if threhold is not None:\n",
    "\n",
    "            head_inx_th = (current_t.item() -  head_timestamps ) <=threhold\n",
    "            head_neighbors = head_neighbors[head_inx_th]\n",
    "            head_timestamps = head_timestamps[head_inx_th]\n",
    "\n",
    "\n",
    "            tail_inx_th = (current_t.item() - tail_timestamps) <=threhold\n",
    "            tail_timestamps = tail_timestamps[tail_inx_th]\n",
    "            tail_neighbors = tail_neighbors[tail_inx_th]\n",
    "\n",
    "        return head_neighbors, tail_neighbors , head_timestamps, tail_timestamps\n",
    "    \n",
    "    \n",
    "    def get_att_score(self,node, neighbors, node2rep):\n",
    "        nei_reps = self.get_rep(neighbors, 'node_rep', node2rep)\n",
    "        node_rep  = self.get_rep([node], 'node_rep', node2rep)\n",
    "\n",
    "        node_reps = node_rep.repeat(len(neighbors),1)\n",
    "\n",
    "        return self.attention(node_reps, nei_reps)\n",
    "    \n",
    "    \n",
    "    def propagation(self, node, current_t, edge_info, node_type, node2cell_head, node2hidden_head, node2cell_tail, node2hidden_tail, node2rep, threhold = None, second_order=False):\n",
    "        head_neighbors, tail_neighbors, head_timestamps, tail_timestamps = self.get_neighbors(node, current_t,threhold)\n",
    "\n",
    "        head_neighbors = list(head_neighbors)\n",
    "        head_timestamps = list(head_timestamps)\n",
    "        if len(head_neighbors)>0:\n",
    "            if node_type == 'head':\n",
    "                head_nei_edge_info = self.tran_head_edge_head(edge_info)\n",
    "\n",
    "            else: \n",
    "                head_nei_edge_info = self.tran_tail_edge_head(edge_info)\n",
    "\n",
    "            head_delta_ts = current_t.repeat(len(head_timestamps),1) - torch.FloatTensor(head_timestamps).to(self.device).view(-1,1)\n",
    "            transed_head_delta_ts = self.decayer(head_delta_ts)\n",
    "\n",
    "            \n",
    "            head_nei_cell = self.get_rep(head_neighbors, 'cell_head',node2cell_head)\n",
    "            if self.if_no_time:\n",
    "                tran_head_nei_edge_info = head_nei_edge_info.repeat(len(head_neighbors),1)\n",
    "            else:\n",
    "                tran_head_nei_edge_info = head_nei_edge_info.repeat(len(head_neighbors),1) * transed_head_delta_ts\n",
    "\n",
    "\n",
    "            if self.is_att:\n",
    "                att_score_head = self.get_att_score(node, head_neighbors, node2rep)\n",
    "                tran_head_nei_edge_info = tran_head_nei_edge_info*att_score_head\n",
    "\n",
    "\n",
    "            head_nei_cell = head_nei_cell + tran_head_nei_edge_info\n",
    "            head_nei_hidden = self.act(head_nei_cell)\n",
    "            head_nei_tail_hidden = self.get_rep(head_neighbors, 'hidden_tail', node2hidden_tail)\n",
    "            head_nei_rep = self.combiner(head_nei_hidden, head_nei_tail_hidden)\n",
    "\n",
    "            for i, nei in enumerate(head_neighbors):\n",
    "                node2cell_head[nei] = head_nei_cell[i].view(-1,self.embedding_dims)\n",
    "                node2hidden_head[nei] = head_nei_hidden[i].view(-1,self.embedding_dims)\n",
    "                node2rep[nei] = head_nei_rep[i].view(-1,self.embedding_dims)\n",
    "\n",
    "            if second_order:\n",
    "                for  head_node_sec in head_neighbors:\n",
    "                    self.second_propagation(head_node_sec, current_t , tran_head_nei_edge_info[0,:], 'head', node2cell_head, node2hidden_head, node2cell_tail, node2hidden_tail, node2rep, threhold)\n",
    "\n",
    "        tail_neighbors = list(tail_neighbors)\n",
    "        tail_timestamps = list(tail_timestamps)\n",
    "        if len(tail_neighbors)>0:\n",
    "\n",
    "            if node_type == 'head':\n",
    "                tail_nei_edge_info = self.tran_head_edge_tail(edge_info)\n",
    "            else: \n",
    "                tail_nei_edge_info = self.tran_tail_edge_tail(edge_info)\n",
    "\n",
    "            tail_delta_ts = current_t.repeat(len(tail_timestamps),1) - torch.FloatTensor(tail_timestamps).to(self.device).view(-1,1) \n",
    "            transed_tail_delta_ts = self.decayer(tail_delta_ts)\n",
    "\n",
    "\n",
    "            tail_nei_cell = self.get_rep(tail_neighbors, 'cell_tail', node2cell_tail)\n",
    "            if self.if_no_time:\n",
    "                tran_tail_nei_edge_info = tail_nei_edge_info.repeat(len(tail_neighbors),1)\n",
    "            else:\n",
    "                tran_tail_nei_edge_info = tail_nei_edge_info.repeat(len(tail_neighbors),1) * transed_tail_delta_ts\n",
    "\n",
    "            if self.is_att:\n",
    "                att_score_tail = self.get_att_score(node, tail_neighbors, node2rep)\n",
    "                tran_head_nei_edge_info = tran_tail_nei_edge_info*att_score_tail\n",
    "\n",
    "            tail_nei_cell = tail_nei_cell + tran_tail_nei_edge_info\n",
    "            tail_nei_hidden = self.act(tail_nei_cell)\n",
    "            tail_nei_head_hidden = self.get_rep(tail_neighbors, 'hidden_head', node2hidden_head)\n",
    "            tail_nei_rep = self.combiner(tail_nei_head_hidden, tail_nei_hidden)\n",
    "\n",
    "            for i, nei in enumerate(tail_neighbors):\n",
    "                node2cell_tail[nei] = tail_nei_cell[i].view(-1,self.embedding_dims)\n",
    "                node2hidden_tail[nei] = tail_nei_hidden[i].view(-1,self.embedding_dims)\n",
    "                node2rep[nei]= tail_nei_rep[i].view(-1, self.embedding_dims)\n",
    "\n",
    "            if second_order:\n",
    "                for tail_node_sec in tail_neighbors:\n",
    "                    self.second_propagation(tail_node_sec, current_t, tran_tail_nei_edge_info[0,:], 'tail', node2cell_head, node2hidden_head, node2cell_tail, node2hidden_tail, node2rep, threhold)\n",
    "\n",
    "        return set(head_neighbors), set(tail_neighbors)\n",
    "\n",
    "    \n",
    "    def second_propagation(self, node, current_t, edge_info, node_type, node2cell_head, node2hidden_head, node2cell_tail, node2hidden_tail, node2rep, threhold = None):\n",
    "        head_neighbors, tail_neighbors, head_timestamps, tail_timestamps = self.get_neighbors(node,current_t, threhold)\n",
    "\n",
    "        head_neighbors = list(head_neighbors)\n",
    "        head_timestamps = list(head_timestamps)\n",
    "        if len(head_neighbors) > 0:\n",
    "            if node_type == 'head':\n",
    "                head_nei_edge_info = self.tran_head_edge_head(edge_info)\n",
    "            else: \n",
    "                head_nei_edge_info = self.tran_tail_edge_head(edge_info)\n",
    "\n",
    "            head_delta_ts = current_t.repeat(len(head_timestamps),1) - torch.FloatTensor(head_timestamps).to(self.device).view(-1,1)\n",
    "            transed_head_delta_ts = self.decayer(head_delta_ts)\n",
    "\n",
    "            head_nei_cell = self.get_rep(head_neighbors, 'cell_head',node2cell_head)\n",
    "            if self.if_no_time:\n",
    "                tran_head_nei_edge_info = head_nei_edge_info.repeat(len(head_neighbors),1)\n",
    "            else:\n",
    "                tran_head_nei_edge_info = head_nei_edge_info.repeat(len(head_neighbors),1) * transed_head_delta_ts\n",
    "\n",
    "            if self.is_att:\n",
    "                att_score_head = self.get_att_score(node, head_neighbors, node2rep)\n",
    "                tran_head_nei_edge_info = tran_head_nei_edge_info*att_score_head\n",
    "\n",
    "            head_nei_cell = head_nei_cell + tran_head_nei_edge_info\n",
    "            head_nei_hidden = self.act(head_nei_cell)\n",
    "            head_nei_tail_hidden = self.get_rep(head_neighbors, 'hidden_tail', node2hidden_tail)\n",
    "            head_nei_rep = self.combiner(head_nei_hidden, head_nei_tail_hidden)\n",
    "\n",
    "            for i, nei in enumerate(head_neighbors):\n",
    "                node2cell_head[nei] = head_nei_cell[i].view(-1,self.embedding_dims)\n",
    "                node2hidden_head[nei] = head_nei_hidden[i].view(-1,self.embedding_dims)\n",
    "                node2rep[nei] = head_nei_rep[i].view(-1,self.embedding_dims)\n",
    "\n",
    "        tail_neighbors = list(tail_neighbors)\n",
    "        tail_timestamps = list(tail_timestamps)\n",
    "        \n",
    "        if len(tail_neighbors) > 0:\n",
    "            if node_type == 'head':\n",
    "                tail_nei_edge_info = self.tran_head_edge_tail(edge_info)\n",
    "            else: \n",
    "                tail_nei_edge_info = self.tran_tail_edge_tail(edge_info)\n",
    "\n",
    "            tail_delta_ts = current_t.repeat(len(tail_timestamps),1) - torch.FloatTensor(tail_timestamps).to(self.device).view(-1,1) \n",
    "            transed_tail_delta_ts = self.decayer(tail_delta_ts)\n",
    "\n",
    "            \n",
    "            tail_nei_cell = self.get_rep(tail_neighbors, 'cell_tail', node2cell_tail)\n",
    "            if self.if_no_time:\n",
    "                tran_tail_nei_edge_info = tail_nei_edge_info.repeat(len(tail_neighbors),1)\n",
    "            else:\n",
    "                tran_tail_nei_edge_info = tail_nei_edge_info.repeat(len(tail_neighbors),1) * transed_tail_delta_ts\n",
    "\n",
    "            if self.is_att:\n",
    "                att_score_tail = self.get_att_score(node, tail_neighbors, node2rep)\n",
    "                tran_head_nei_edge_info = tran_tail_nei_edge_info*att_score_tail\n",
    "\n",
    "\n",
    "            tail_nei_cell = tail_nei_cell + tran_tail_nei_edge_info\n",
    "            tail_nei_hidden = self.act(tail_nei_cell)\n",
    "            tail_nei_head_hidden = self.get_rep(tail_neighbors, 'hidden_head', node2hidden_head)\n",
    "            tail_nei_rep = self.combiner(tail_nei_head_hidden, tail_nei_hidden)\n",
    "\n",
    "            for i, nei in enumerate(tail_neighbors):\n",
    "                node2cell_tail[nei] = tail_nei_cell[i].view(-1,self.embedding_dims)\n",
    "                node2hidden_tail[nei] = tail_nei_hidden[i].view(-1,self.embedding_dims)\n",
    "                node2rep[nei]= tail_nei_rep[i].view(-1,self.embedding_dims)\n",
    "                \n",
    "        return set(head_neighbors), set(tail_neighbors)\n",
    "\n",
    "    def loss(self, interactions):\n",
    "        output_rep_head_tensor, output_rep_tail_tensor, head_neg_tensors, tail_neg_tensors = self.forward(interactions)\n",
    "\n",
    "        head_pos_tensors = output_rep_head_tensor.clone().repeat(1,self.num_negative).view(-1,self.embedding_dims)\n",
    "        tail_pos_tensors = output_rep_tail_tensor.clone().repeat(1,self.num_negative).view(-1,self.embedding_dims)\n",
    "\n",
    "        num_pp = output_rep_head_tensor.size()[0]\n",
    "        labels_p = torch.FloatTensor([1]*num_pp).to(self.device)\n",
    "        labels_n = torch.FloatTensor([0]*num_pp*2*self.num_negative).to(self.device)\n",
    "\n",
    "        labels = torch.cat((labels_p,labels_n))\n",
    "\n",
    "        scores_p = torch.bmm(output_rep_head_tensor.view(num_pp,1,self.embedding_dims),output_rep_tail_tensor.view(num_pp,self.embedding_dims,1))\n",
    "        scores_n_1 = torch.bmm(head_neg_tensors.view(num_pp*self.num_negative,1,self.embedding_dims), tail_pos_tensors.view(num_pp*self.num_negative, self.embedding_dims,1))\n",
    "        scores_n_2 = torch.bmm(head_pos_tensors.view(num_pp*self.num_negative,1,self.embedding_dims), tail_neg_tensors.view(num_pp*self.num_negative, self.embedding_dims,1))\n",
    "\n",
    "        scores = torch.cat((scores_p,scores_n_1,scores_n_2)).view(num_pp*(1+2*self.num_negative))\n",
    "        bce_with_logits_loss = nn.BCEWithLogitsLoss()\n",
    "        loss = bce_with_logits_loss(scores,labels)\n",
    "\n",
    "        return loss        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Temporal_Dataset(Dataset):\n",
    "    def __init__(self, file_name, starting = 0,skip_rows=0, div =3600):\n",
    "        self.data = np.loadtxt(fname=file_name, skiprows=skip_rows)[:,[0,1,3]]\n",
    "        self.time = self.data[:,2]\n",
    "        self.trans_time = (self.time - self.time[0])/div\n",
    "        self.data[:,2] = self.trans_time\n",
    "        self.data[:, [0,1]] = self.data[:,[0,1]] - starting\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.time.shape[0]\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        sample = self.data[idx,:]\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description = 'Show description')\n",
    "    \n",
    "    parser.add_argument('-data', '--dataset', type = str, help = 'which dataset to run', default = 'uci')\n",
    "    parser.add_argument('-b', '--batch_size', type= int, help = 'batch_size', default = 200)\n",
    "    parser.add_argument('-l', '--learning_rate', type = float, help = 'learning_rate', default = 0.001)\n",
    "    parser.add_argument('-nn', '--num_negative', type = int, help = 'num_negative', default = 5)\n",
    "    parser.add_argument('-tr', '--train_ratio', type = float, help = 'train_ratio', default = 0.8)\n",
    "    parser.add_argument('-vr', '--valid_ratio', type = float, help = 'valid_ratio', default = 0.01)\n",
    "    parser.add_argument('-act', '--act', type = str, help = 'act function', default = 'tanh')\n",
    "    parser.add_argument('-trans', '--transfer', type = int, help = 'transfer to head, tail representations', default = 1)\n",
    "    parser.add_argument('-dp' , '--drop_p', type = float, help = 'dropout_rate', default = 0)\n",
    "    parser.add_argument('-ip', '--if_propagation', type = int, help = 'if_propagation', default=1)\n",
    "    parser.add_argument('-ia', '--is_att', type = int, help = 'use attention or not', default=1)\n",
    "    parser.add_argument('-w', '--w', type = float, help = 'w for decayer', default = 2)\n",
    "    parser.add_argument('-s', '--seed', type = int, help = 'random seed', default = 0)\n",
    "    parser.add_argument('-rp', '--reset_rep', type = int, help = 'whether reset rep', default = 1)\n",
    "    parser.add_argument('-dc', '--decay_method', type = str, help = 'decay_method', default = 'log')\n",
    "    parser.add_argument('-nor', '--nor', type = int , help = 'normalize or not', default = 0)\n",
    "    parser.add_argument('-iu', '--if_updated', type = int, help = 'use updated representation in loss', default = 0)\n",
    "    parser.add_argument('-wd', '--weight_decay', type = float, help = 'weight decay', default = 0.001)\n",
    "    parser.add_argument('-nt', '--if_no_time', type = int, help = 'if no time interval information', default = 0)\n",
    "    parser.add_argument('-th', '--threhold', type = float, help = 'the threhold to filter the neighbors, if None, do not filter', default = None)\n",
    "    parser.add_argument('-2hop', '--second_order', type = int, help = 'whether to use 2-hop prop', default = 0)\n",
    "    \n",
    "    args = parser.parse_args(args=[])\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_prediction(data, reps):\n",
    "    head_list = list(data[:,0])\n",
    "    tail_list = list(data[:,1])\n",
    "    head_reps = reps[head_list,:]\n",
    "    tail_reps = reps[tail_list,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(data, head_reps, tail_reps,device):\n",
    "    head_list = list(data[:,0])\n",
    "    tail_list = list(data[:,1])\n",
    "\n",
    "    head_tensors = head_reps(torch.LongTensor(head_list).to(device))\n",
    "    tail_tensors = tail_reps(torch.LongTensor(tail_list).to(device))\n",
    "    scores = torch.bmm(head_tensors.view(len(head_list),1,head_tensors.size()[1]),tail_tensors.view(len(head_list),head_tensors.size()[1],1)).view(len(head_list))\n",
    "    labels = torch.FloatTensor([1]*len(head_list)).to(device)\n",
    "    bce_with_logits_loss = nn.BCEWithLogitsLoss().to(device)\n",
    "    loss = bce_with_logits_loss(scores,labels)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank(node, true_candidate, node2candidate, node_reps, candidate_reps, device, pri = False):\n",
    "    node_tensor = node_reps(torch.LongTensor([node]).to(device)).view(-1,1)\n",
    "    candidates = list(node2candidate[node])\n",
    "\n",
    "    candidates.append(true_candidate)\n",
    "\n",
    "    length = len(candidates)\n",
    "\n",
    "    candidate_tensors = candidate_reps(torch.LongTensor(candidates).to(device))\n",
    "\n",
    "    scores = torch.mm(candidate_tensors, node_tensor)\n",
    "    negative_scores_numpy = -scores.view(1,-1).to('cpu').numpy()\n",
    "    rank = rankdata(negative_scores_numpy)[-1]\n",
    "\n",
    "    if pri:\n",
    "        print(node , true_candidate)\n",
    "        print(scores.view(-1))\n",
    "        print(rank, 'out of',length)\n",
    "\n",
    "    return rank, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_previous_links(data):\n",
    "    previous_links = set()\n",
    "    for i in range(len(data)):\n",
    "        head, tail, time = data[i]\n",
    "        previous_links.add((int(head), int(tail)))\n",
    "    return previous_links "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node2candidate(train_data, all_nodes, pri = False):\n",
    "    head_node2candidate = dict()\n",
    "    tail_node2candidate = dict()\n",
    "\n",
    "    pri = True\n",
    "    if pri:\n",
    "        start_time = time.time()\n",
    "        print('Start to build node2candidate')\n",
    "\n",
    "    for i in range(len(train_data)):\n",
    "\n",
    "        head, tail, not_in_use = train_data[i]\n",
    "        head = int(head)\n",
    "        tail = int(tail)\n",
    "        if head not in head_node2candidate:\n",
    "            head_node2candidate[head] = all_nodes\n",
    "\n",
    "        if tail not in tail_node2candidate:\n",
    "            tail_node2candidate[tail] = all_nodes\n",
    "\n",
    "    if pri: \n",
    "        end_time = time.time()\n",
    "        print('node2candidate built in' , str(end_time-start_time))\n",
    "        \n",
    "    return head_node2candidate, tail_node2candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranks(test_data,head_reps, tail_reps, device, head_node2candidate, tail_node2candidate, pri=False, previous_links = None, bo = False):\n",
    "    head_ranks = []\n",
    "    tail_ranks = []\n",
    "    head_lengths = []\n",
    "    tail_lengths = []\n",
    "\n",
    "    for interactioin in test_data:\n",
    "        head_node, tail_node , time = interactioin\n",
    "        head_node = int(head_node)\n",
    "        tail_node = int(tail_node)\n",
    "        if pri:\n",
    "            print('--------------', head_node, tail_node, '---------------')\n",
    "\n",
    "        if bo:\n",
    "            if previous_links is not None: \n",
    "                if head_node in head_node2candidate and tail_node in tail_node2candidate and tail_node in head_node2candidate and head_node in tail_node2candidate and (head_node, tail_node) not in previous_links:\n",
    "                    head_rank, head_length = rank(head_node, tail_node, head_node2candidate, head_reps, tail_reps, device,pri)\n",
    "                    head_ranks.append(head_rank)\n",
    "                    head_lengths.append(head_length)\n",
    "\n",
    "                    tail_rank, tail_length = rank(tail_node, head_node, tail_node2candidate, tail_reps, head_reps, device)\n",
    "                    tail_ranks.append(tail_rank)\n",
    "                    tail_lengths.append(tail_length)\n",
    "            else:\n",
    "                if head_node in head_node2candidate and tail_node in tail_node2candidate and tail_node in head_node2candidate and head_node in tail_node2candidate:\n",
    "                    head_rank, head_length = rank(head_node, tail_node, head_node2candidate, head_reps, tail_reps, device,pri)\n",
    "                    head_ranks.append(head_rank)\n",
    "                    head_lengths.append(head_length)\n",
    "\n",
    "                    tail_rank, tail_length = rank(tail_node, head_node, tail_node2candidate, tail_reps, head_reps, device)\n",
    "                    tail_ranks.append(tail_rank)\n",
    "                    tail_lengths.append(tail_length)\n",
    "        else:\n",
    "            if previous_links is not None: \n",
    "                if head_node in head_node2candidate and tail_node in tail_node2candidate and (head_node, tail_node) not in previous_links:\n",
    "                    head_rank, head_length = rank(head_node, tail_node, head_node2candidate, head_reps, tail_reps, device,pri)\n",
    "                    head_ranks.append(head_rank)\n",
    "                    head_lengths.append(head_length)\n",
    "\n",
    "                    tail_rank, tail_length = rank(tail_node, head_node, tail_node2candidate, tail_reps, head_reps, device)\n",
    "                    tail_ranks.append(tail_rank)\n",
    "                    tail_lengths.append(tail_length)\n",
    "            else:\n",
    "                if head_node in head_node2candidate and tail_node in tail_node2candidate:\n",
    "                    head_rank, head_length = rank(head_node, tail_node, head_node2candidate, head_reps, tail_reps, device,pri)\n",
    "                    head_ranks.append(head_rank)\n",
    "                    head_lengths.append(head_length)\n",
    "\n",
    "                    tail_rank, tail_length = rank(tail_node, head_node, tail_node2candidate, tail_reps, head_reps, device)\n",
    "                    tail_ranks.append(tail_rank)\n",
    "                    tail_lengths.append(tail_length)\n",
    "\n",
    "    return head_ranks, tail_ranks, head_lengths, tail_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, data, num_nodes, model_save_dir):\n",
    "    batch_size = args.batch_size\n",
    "    learning_rate = args.learning_rate\n",
    "    num_negative = args.num_negative\n",
    "    act = args.act\n",
    "    transfer = args.transfer\n",
    "    drop_p = args.drop_p\n",
    "    if_propagation = args.if_propagation\n",
    "    w = args.w\n",
    "    is_att = args.is_att\n",
    "    seed = args.seed\n",
    "    reset_rep = args.reset_rep\n",
    "    decay_method = args.decay_method\n",
    "    nor = args.nor\n",
    "    if_updated = args.if_updated\n",
    "    weight_decay = args.weight_decay\n",
    "    if_no_time = args.if_no_time\n",
    "    threhold = args.threhold\n",
    "    second_order = args.second_order\n",
    "    num_iter = 4\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "\n",
    "    train_ratio = args.train_ratio\n",
    "    valid_ratio = args.valid_ratio\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    train_data = data[0:int(len(data)*train_ratio)]\n",
    "    validation_data = data[int(len(data)*train_ratio):int(len(data)*(train_ratio+valid_ratio))]\n",
    "    test_data = data[int(len(data)*(train_ratio + valid_ratio)):len(data)]\n",
    "    print('Data length: ', len(data))\n",
    "    print('Train length: ', len(train_data))\n",
    "    sampler = SequentialSampler(train_data)\n",
    "    data_loader = DataLoader(train_data, batch_size, sampler = sampler)\n",
    "\n",
    "    all_nodes = set(range(num_nodes))\n",
    "    print('num_nodes',len(all_nodes))\n",
    "    head_node2candidate, tail_node2candidate = get_node2candidate(train_data, all_nodes)\n",
    "\n",
    "\n",
    "\n",
    "    model_save_dir = model_save_dir  + 'nt_' +str(if_no_time)+ '_wd_' + str(weight_decay) + '_up_' + str(if_updated) +'_w_' + str(w) +'_b_' + str(batch_size) + '_l_' + str(learning_rate) + '_tr_' + str(train_ratio) + '_nn_' +str(num_negative)+'_' + act + '_trans_' +str(transfer) + '_dr_p_' + str(drop_p) + '_prop_' + str(if_propagation) + '_att_' +str(is_att) + '_rp_' + str(reset_rep) + '_dcm_' + decay_method + '_nor_' + str(nor)\n",
    "    if threhold is not None:\n",
    "        model_save_dir = model_save_dir + '_th_' + str(threhold)\n",
    "\n",
    "    if second_order:\n",
    "        model_save_dir = model_save_dir + '_2hop'\n",
    "        \n",
    "    if not os.path.exists(model_save_dir):\n",
    "        os.makedirs(model_save_dir)\n",
    "\n",
    "    dyGnn = DyGNN(num_nodes,64,64,device, w,is_att ,transfer,nor,if_no_time, threhold,second_order, if_updated,drop_p, num_negative, act, if_propagation, decay_method )\n",
    "    dyGnn.train()\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad,dyGnn.parameters()),lr = learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "    old_head_rank = num_nodes/2\n",
    "    old_tail_rank = num_nodes/2\n",
    "\n",
    "    for epoch in range(num_iter):\n",
    "        print('epoch: ', epoch)\n",
    "        print('Resetting time...')\n",
    "        dyGnn.reset_time()\n",
    "        print('Time reset')\n",
    "        if reset_rep:\n",
    "\n",
    "            dyGnn.reset_reps()\n",
    "            print('reps reset')\n",
    "\n",
    "        x = int(5000/batch_size)\n",
    "        y = int(10000/batch_size)\n",
    "\n",
    "\n",
    "        for i, interactions in enumerate(data_loader):\n",
    "\n",
    "            # Compute and print loss.\n",
    "            loss = dyGnn.loss(interactions)\n",
    "            if i%x==0:\n",
    "                #dyGnn.reset_reps()\n",
    "                print(i,' train_loss: ', loss.item())\n",
    "\n",
    "                if transfer:\n",
    "                    head_reps = nn.Embedding.from_pretrained(dyGnn.transfer2head(dyGnn.node_representations.weight))\n",
    "                    tail_reps = nn.Embedding.from_pretrained(dyGnn.transfer2tail(dyGnn.node_representations.weight))\n",
    "                else:\n",
    "                    head_reps = dyGnn.node_representations\n",
    "                    tail_reps = dyGnn.node_representations\n",
    "\n",
    "                head_reps = nn.Embedding.from_pretrained(nn.functional.normalize(head_reps.weight))\n",
    "                tail_reps = nn.Embedding.from_pretrained(nn.functional.normalize(tail_reps.weight))\n",
    "\n",
    "\n",
    "            if i%y==-1:\n",
    "\n",
    "                if transfer:\n",
    "                    head_reps = nn.Embedding.from_pretrained(dyGnn.transfer2head(dyGnn.node_representations.weight))\n",
    "                    tail_reps = nn.Embedding.from_pretrained(dyGnn.transfer2tail(dyGnn.node_representations.weight))\n",
    "                else:\n",
    "                    head_reps = dyGnn.node_representations\n",
    "                    tail_reps = dyGnn.node_representations\n",
    "\n",
    "                head_reps = nn.Embedding.from_pretrained(nn.functional.normalize(head_reps.weight))\n",
    "                tail_reps = nn.Embedding.from_pretrained(nn.functional.normalize(tail_reps.weight))\n",
    "\n",
    "                head_ranks, tail_ranks, not_in_use, not_in_use2= get_ranks(validation_data,head_reps, tail_reps, device, head_node2candidate, tail_node2candidate)\n",
    "                head_ranks_numpy = np.asarray(head_ranks)\n",
    "                tail_ranks_numpy = np.asarray(tail_ranks)\n",
    "                print('head_rank mean: ', np.mean(head_ranks_numpy),' ; ', 'head_rank var: ', np.var(head_ranks_numpy))\n",
    "                print('tail_rank mean: ', np.mean(tail_ranks_numpy),' ; ', 'tail_rank var: ', np.var(tail_ranks_numpy))\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "        if transfer:\n",
    "            head_reps = nn.Embedding.from_pretrained(dyGnn.transfer2head(dyGnn.node_representations.weight))\n",
    "            tail_reps = nn.Embedding.from_pretrained(dyGnn.transfer2tail(dyGnn.node_representations.weight))\n",
    "        else:\n",
    "            head_reps = dyGnn.node_representations\n",
    "            tail_reps = dyGnn.node_representations\n",
    "        head_reps = nn.Embedding.from_pretrained(nn.functional.normalize(head_reps.weight))\n",
    "        tail_reps = nn.Embedding.from_pretrained(nn.functional.normalize(tail_reps.weight))\n",
    "\n",
    "        valid_loss = get_loss(validation_data, head_reps, tail_reps, device)\n",
    "        head_ranks, tail_ranks, head_lengths, tail_lengths = get_ranks(validation_data, head_reps, tail_reps, device, head_node2candidate, tail_node2candidate)\n",
    "        head_ranks_numpy = np.asarray(head_ranks)\n",
    "        tail_ranks_numpy = np.asarray(tail_ranks)\n",
    "        head_lengths_numpy = np.asarray(head_lengths)\n",
    "        tail_lengths_numpy = np.asarray(tail_lengths)\n",
    "\n",
    "        mean_head_rank = np.mean(head_ranks_numpy)\n",
    "        mean_tail_rank = np.mean(tail_ranks_numpy)\n",
    "\n",
    "\n",
    "        print('head_length mean: ', np.mean(head_lengths_numpy), ';', 'num_test: ', head_lengths_numpy.shape[0])\n",
    "        print('tail_lengths mean: ', np.mean(tail_lengths_numpy), ';', 'num_test: ', tail_lengths_numpy.shape[0])\n",
    "        print('head_rank mean: ', mean_head_rank,' ; ', 'head_rank var: ', np.var(head_ranks_numpy))\n",
    "        print('tail_rank mean: ', mean_tail_rank,' ; ', 'tail_rank var: ', np.var(tail_ranks_numpy))\n",
    "        print('reverse head_rank mean: ', np.mean(1/head_ranks_numpy))\n",
    "        print('reverse tail_rank mean: ', np.mean(1/tail_ranks_numpy))\n",
    "        print('head_rank HITS 100: ', (head_ranks_numpy<=100).sum())\n",
    "        print('tail_rank_HITS 100: ', (tail_ranks_numpy<=100).sum())\n",
    "        print('head_rank HITS 50: ', (head_ranks_numpy<=50).sum())\n",
    "        print('tail_rank_HITS 50: ', (tail_ranks_numpy<=50).sum())\n",
    "        print('head_rank HITS 20: ', (head_ranks_numpy<=20).sum())\n",
    "        print('tail_rank_HITS 20: ', (tail_ranks_numpy<=20).sum())\n",
    "\n",
    "        if mean_head_rank < old_head_rank or mean_tail_rank < old_tail_rank:\n",
    "            model_save_path = model_save_dir + '/' + 'model_after_epoch_' + str(epoch) + '.pt'\n",
    "            torch.save(dyGnn.state_dict(), model_save_path)\n",
    "            print('model saved in: ', model_save_path)\n",
    "\n",
    "            with open(model_save_dir + '/' + '0valid_results.txt','a') as f:\n",
    "                f.write('epoch: ' + str(epoch) + '\\n')\n",
    "                f.write('head_rank mean: ' + str(mean_head_rank) + ' ; ' +  'head_rank var: ' + str(np.var(head_ranks_numpy)) + '\\n')\n",
    "                f.write('tail_rank mean: ' + str(mean_tail_rank) + ' ; ' +  'tail_rank var: ' + str(np.var(tail_ranks_numpy)) + '\\n')\n",
    "                f.write('head_rank HITS 100: ' + str ( (head_ranks_numpy<=100).sum()) + '\\n')\n",
    "                f.write('tail_rank_HITS 100: ' + str ( (tail_ranks_numpy<=100).sum()) + '\\n')\n",
    "                f.write('head_rank HITS 50: ' + str( (head_ranks_numpy<=50).sum()) + '\\n')\n",
    "                f.write('tail_rank_HITS 50: ' + str( (tail_ranks_numpy<=50).sum()) + '\\n')\n",
    "                f.write('head_rank HITS 20: ' + str( (head_ranks_numpy<=20).sum()) + '\\n')\n",
    "                f.write('tail_rank_HITS 20: ' + str( (tail_ranks_numpy<=20).sum()) + '\\n')\n",
    "                f.write('============================================================================\\n')\n",
    "            old_head_rank = mean_head_rank + 200\n",
    "            old_tail_rank = mean_tail_rank + 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 主函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on UCI_message dataset\n",
      "Data length:  59835\n",
      "Train length:  47868\n",
      "num_nodes 1899\n",
      "Start to build node2candidate\n",
      "node2candidate built in 0.11668658256530762\n",
      "Only propagate to relevance nodes below time interval:  None\n",
      "epoch:  0\n",
      "Resetting time...\n",
      "Time reset\n",
      "reps reset\n",
      "0  train_loss:  0.7923987507820129\n",
      "25  train_loss:  0.30442488193511963\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-3630cd78d6e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mmodel_save_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_save_dir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'UCI/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Train on UCI_message dataset'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_nodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_save_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Please choose a dataset to run'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-64-5cd81accb241>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(args, data, num_nodes, model_save_dir)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[1;31m# Compute and print loss.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdyGnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minteractions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m                 \u001b[1;31m#dyGnn.reset_reps()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-804731f45d70>\u001b[0m in \u001b[0;36mloss\u001b[1;34m(self, interactions)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minteractions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 570\u001b[1;33m         \u001b[0moutput_rep_head_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_rep_tail_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhead_neg_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtail_neg_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minteractions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    571\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m         \u001b[0mhead_pos_tensors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput_rep_head_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_negative\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_dims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-804731f45d70>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, interactions)\u001b[0m\n\u001b[0;32m    247\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mif_propagation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m                 \u001b[0mhead_node_head_neighbors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhead_node_tail_neighbors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpropagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhead_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medge_info_head\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'head'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode2cell_head\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode2hidden_head\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode2cell_tail\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode2hidden_tail\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode2rep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrehold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msecond_order\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m                 \u001b[0mtail_node_head_neighbors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtail_node_tail_neighbors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpropagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtail_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medge_info_tail\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tail'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode2cell_head\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode2hidden_head\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode2cell_tail\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode2hidden_tail\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode2rep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrehold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msecond_order\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m                 \u001b[0mhead_node_head_neighbors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhead_node_tail_neighbors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_i_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_i_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_neighbors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhead_index\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcurrent_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mthrehold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-804731f45d70>\u001b[0m in \u001b[0;36mpropagation\u001b[1;34m(self, node, current_t, edge_info, node_type, node2cell_head, node2hidden_head, node2cell_tail, node2hidden_tail, node2rep, threhold, second_order)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpropagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0medge_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode2cell_head\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode2hidden_head\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode2cell_tail\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode2hidden_tail\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode2rep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthrehold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msecond_order\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 418\u001b[1;33m         \u001b[0mhead_neighbors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtail_neighbors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhead_timestamps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtail_timestamps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_neighbors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_t\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mthrehold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m         \u001b[0mhead_neighbors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhead_neighbors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-804731f45d70>\u001b[0m in \u001b[0;36mget_neighbors\u001b[1;34m(self, node, current_t, threhold)\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_neighbors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcurrent_t\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mthrehold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m         \u001b[0mrow_inx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol_inx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimestamps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minteraction_timestamp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m         \u001b[0mhead_inx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol_inx\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sss\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\scipy\\sparse\\extract.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(A)\u001b[0m\n\u001b[0;32m     34\u001b[0m     \"\"\"\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[0mA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcoo_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m     \u001b[0mA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum_duplicates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;31m# remove explicit zeros\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sss\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\scipy\\sparse\\coo.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[0;32m    168\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m                     \u001b[0mcoo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtocoo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcoo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcoo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sss\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36mtocoo\u001b[1;34m(self, copy)\u001b[0m\n\u001b[0;32m    904\u001b[0m         \u001b[0mthe\u001b[0m \u001b[0mresultant\u001b[0m \u001b[0mcoo_matrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    905\u001b[0m         \"\"\"\n\u001b[1;32m--> 906\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtocsr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtocoo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    907\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    908\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtolil\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sss\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36mtocoo\u001b[1;34m(self, copy)\u001b[0m\n\u001b[0;32m   1015\u001b[0m         \u001b[0mminor_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m         \u001b[0mmajor_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminor_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1017\u001b[1;33m         \u001b[0m_sparsetools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpandptr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmajor_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindptr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmajor_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1018\u001b[0m         \u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmajor_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminor_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1019\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "args = get_args()\n",
    "model_save_dir = r\"C:\\Users\\sss\\Desktop\\DyGNN-main\\saved_models/\"\n",
    "if args.dataset == \"uci\":\n",
    "    data = Temporal_Dataset(r'C:\\Users\\sss\\Desktop\\DyGNN-main\\Dataset/UCI_email_1899_59835/opsahl-ucsocial/out.opsahl-ucsocial', 1, 2)\n",
    "    # print(dir(data))\n",
    "    num_nodes = 1899\n",
    "    model_save_dir = model_save_dir + 'UCI/'\n",
    "    print('Train on UCI_message dataset')\n",
    "    train(args, data, num_nodes, model_save_dir)   \n",
    "else:\n",
    "    print('Please choose a dataset to run')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
