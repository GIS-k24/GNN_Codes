{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EvolveGCN\n",
    "- EvolveGCN - H: 节点有信息，考虑了节点的特征变化\n",
    "- EvolveGCN - O: 节点信息比较少，更关系图结构的变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "import math\n",
    "import time\n",
    "import pprint\n",
    "import random\n",
    "import tarfile\n",
    "import itertools\n",
    "import argparse\n",
    "import logging\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import datetime\n",
    "from sklearn.metrics import average_precision_score\n",
    "from scipy.sparse import coo_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_with_last_col(matrix,cols):\n",
    "    out = [matrix]\n",
    "    pad = [matrix[:,[-1]]] * (cols - matrix.size(1))\n",
    "    out.extend(pad)\n",
    "    return torch.cat(out,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_with_last_val(vect, k):\n",
    "    device = 'cuda' if vect.is_cuda else 'cpu'\n",
    "    pad = torch.ones(k - vect.size(0),\n",
    "                         dtype=torch.long,\n",
    "                         device = device) * vect[-1]\n",
    "    vect = torch.cat([vect,pad])\n",
    "    return vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_prepare_tensor(tensor,torch_size, ignore_batch_dim = True):\n",
    "    if ignore_batch_dim:\n",
    "        tensor = sp_ignore_batch_dim(tensor)\n",
    "    tensor = make_sparse_tensor(tensor,\n",
    "                                tensor_type = 'float',\n",
    "                                torch_size = torch_size)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sp_ignore_batch_dim(tensor_dict):\n",
    "    tensor_dict['idx'] = tensor_dict['idx'][0]\n",
    "    tensor_dict['vals'] = tensor_dict['vals'][0]\n",
    "    return tensor_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_by_time(time_vector,time_win_aggr):\n",
    "    time_vector = time_vector - time_vector.min()\n",
    "    time_vector = time_vector // time_win_aggr\n",
    "    return time_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_time(data,time_col):\n",
    "    _, sort = torch.sort(data[:,time_col])\n",
    "    data = data[sort]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sp_tensor(sp_tensor,size):\n",
    "    print(torch.sparse.FloatTensor(sp_tensor['idx'].t(),sp_tensor['vals'],torch.Size([size,size])).to_dense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_param(t):\n",
    "    stdv = 2. / math.sqrt(t.size(0))\n",
    "    t.data.uniform_(-stdv,stdv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sparse_tensor(adj,tensor_type,torch_size):\n",
    "    if len(torch_size) == 2:\n",
    "        tensor_size = torch.Size(torch_size)\n",
    "    elif len(torch_size) == 1:\n",
    "        tensor_size = torch.Size(torch_size*2)\n",
    "\n",
    "    if tensor_type == 'float':\n",
    "        test = torch.sparse.FloatTensor(adj['idx'].t(),\n",
    "                                      adj['vals'].type(torch.float),\n",
    "                                      tensor_size)\n",
    "        return torch.sparse.FloatTensor(adj['idx'].t(),\n",
    "                                      adj['vals'].type(torch.float),\n",
    "                                      tensor_size)\n",
    "    elif tensor_type == 'long':\n",
    "        return torch.sparse.LongTensor(adj['idx'].t(),\n",
    "                                      adj['vals'].type(torch.long),\n",
    "                                      tensor_size)\n",
    "    else:\n",
    "        raise NotImplementedError('only make floats or long sparse tensors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sp_to_dict(sp_tensor):\n",
    "    return {'idx': sp_tensor._indices().t(), 'vals': sp_tensor._values()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Namespace(object):\n",
    "    '''\n",
    "    helps referencing object in a dictionary as dict.key instead of dict['key']\n",
    "    '''\n",
    "    def __init__(self, adict):\n",
    "        self.__dict__.update(adict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(rank):\n",
    "    seed = int(time.time()) + rank\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_param_value(param, param_min, param_max, type='int'):\n",
    "    if str(param) is None or str(param).lower()=='none':\n",
    "        if type=='int':\n",
    "            return random.randrange(param_min, param_max+1)\n",
    "        elif type=='logscale':\n",
    "            interval=np.logspace(np.log10(param_min), np.log10(param_max), num=100)\n",
    "            return np.random.choice(interval,1)[0]\n",
    "        else:\n",
    "            return random.uniform(param_min, param_max)\n",
    "    else:\n",
    "        return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file):\n",
    "    with open(file) as file:\n",
    "        file = file.read().splitlines()\n",
    "    data = torch.tensor([[float(r) for r in row.split(',')] for row in file[1:]])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_tar(file, tar_archive, replace_unknow=False, starting_line=1, sep=',', type_fn=float, tensor_const=torch.DoubleTensor):\n",
    "    f = tar_archive.extractfile(file)\n",
    "    lines = f.read()  #\n",
    "    lines=lines.decode('utf-8')\n",
    "    if replace_unknow:\n",
    "        lines=lines.replace('unknow', '-1')\n",
    "        lines=lines.replace('-1n', '-1')\n",
    "\n",
    "    lines=lines.splitlines()\n",
    "\n",
    "    data = [[type_fn(r) for r in row.split(sep)] for row in lines[starting_line:]]\n",
    "    data = tensor_const(data)\n",
    "    # print (file,'data size', data.size())\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parser():\n",
    "    parser = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter)\n",
    "    parser.add_argument('--config_file', default=r'C:\\Users\\sss\\Desktop\\EvolveGCN-master\\experiments/parameters_example.yaml', type=argparse.FileType(mode='r'), help='optional, yaml file containing parameters to be used, overrides command line parameters')\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args(parser):    \n",
    "    \n",
    "    args = parser.parse_args(args=[])\n",
    "        \n",
    "    if args.config_file:\n",
    "        data = yaml.load(args.config_file)\n",
    "        delattr(args, 'config_file')\n",
    "        # print(data)\n",
    "        arg_dict = args.__dict__\n",
    "        for key, value in data.items():\n",
    "            arg_dict[key] = value\n",
    "\n",
    "    args.learning_rate = random_param_value(args.learning_rate, args.learning_rate_min, args.learning_rate_max, type='logscale')\n",
    "    # args.adj_mat_time_window = random_param_value(args.adj_mat_time_window, args.adj_mat_time_window_min, args.adj_mat_time_window_max, type='int')\n",
    "    args.num_hist_steps = random_param_value(args.num_hist_steps, args.num_hist_steps_min, args.num_hist_steps_max, type='int')\n",
    "    args.gcn_parameters['feats_per_node'] =random_param_value(args.gcn_parameters['feats_per_node'], args.gcn_parameters['feats_per_node_min'], args.gcn_parameters['feats_per_node_max'], type='int')\n",
    "    args.gcn_parameters['layer_1_feats'] =random_param_value(args.gcn_parameters['layer_1_feats'], args.gcn_parameters['layer_1_feats_min'], args.gcn_parameters['layer_1_feats_max'], type='int')\n",
    "    \n",
    "    if args.gcn_parameters['layer_2_feats_same_as_l1'] or args.gcn_parameters['layer_2_feats_same_as_l1'].lower()=='true':\n",
    "        args.gcn_parameters['layer_2_feats'] = args.gcn_parameters['layer_1_feats']\n",
    "    else:\n",
    "        args.gcn_parameters['layer_2_feats'] =random_param_value(args.gcn_parameters['layer_2_feats'], args.gcn_parameters['layer_1_feats_min'], args.gcn_parameters['layer_1_feats_max'], type='int')\n",
    "    args.gcn_parameters['lstm_l1_feats'] =random_param_value(args.gcn_parameters['lstm_l1_feats'], args.gcn_parameters['lstm_l1_feats_min'], args.gcn_parameters['lstm_l1_feats_max'], type='int')\n",
    "    \n",
    "    if args.gcn_parameters['lstm_l2_feats_same_as_l1'] or args.gcn_parameters['lstm_l2_feats_same_as_l1'].lower()=='true':\n",
    "        args.gcn_parameters['lstm_l2_feats'] = args.gcn_parameters['lstm_l1_feats']\n",
    "    else:\n",
    "        args.gcn_parameters['lstm_l2_feats'] =random_param_value(args.gcn_parameters['lstm_l2_feats'], args.gcn_parameters['lstm_l1_feats_min'], args.gcn_parameters['lstm_l1_feats_max'], type='int')\n",
    "    args.gcn_parameters['cls_feats'] =random_param_value(args.gcn_parameters['cls_feats'], args.gcn_parameters['cls_feats_min'], args.gcn_parameters['cls_feats_max'], type='int')\n",
    "\n",
    "    \n",
    "    \n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autonomous_Systems_Dataset():\n",
    "    def __init__(self,args):\n",
    "        args.aut_sys_args = Namespace(args.aut_sys_args)\n",
    "        tar_file = os.path.join(args.aut_sys_args.folder, args.aut_sys_args.tar_file)  \n",
    "        tar_archive = tarfile.open(tar_file, 'r:gz')\n",
    "        self.edges = self.load_edges(args,tar_archive)\n",
    "\n",
    "    def load_edges(self,args,tar_archive):\n",
    "        files = tar_archive.getnames()\n",
    "        cont_files2times = self.times_from_names(files)\n",
    "        edges = []\n",
    "        cols = Namespace({\n",
    "            'source': 0,\n",
    "            'target': 1,\n",
    "            'time': 2\n",
    "        })\n",
    "        \n",
    "        for file in files:\n",
    "            data = load_data_from_tar(file, tar_archive, starting_line=4, sep='\\t', type_fn = int, tensor_const = torch.LongTensor)\n",
    "            time_col = torch.zeros(data.size(0), 1, dtype=torch.long) + cont_files2times[file]\n",
    "            data = torch.cat([data, time_col], dim = 1)\n",
    "            data = torch.cat([data, data[:, [cols.target, cols.source, cols.time]]])\n",
    "            edges.append(data)\n",
    "\n",
    "        edges = torch.cat(edges)\n",
    "        _, edges[:,[cols.source,cols.target]] = edges[:,[cols.source,cols.target]].unique(return_inverse = True)\n",
    "\n",
    "        # use only first X time steps\n",
    "        indices = edges[:, cols.time] < args.aut_sys_args.steps_accounted\n",
    "        edges = edges[indices, :]\n",
    "            \n",
    "        # time aggregation\n",
    "        edges[:, cols.time] = aggregate_by_time(edges[:, cols.time], args.aut_sys_args.aggr_time)\n",
    "        self.num_nodes = int(edges[:, [cols.source,cols.target]].max() + 1)\n",
    "\n",
    "        ids = edges[:, cols.source] * self.num_nodes + edges[:, cols.target]\n",
    "        self.num_non_existing = float(self.num_nodes**2 - ids.unique().size(0))\n",
    "        self.max_time = edges[:,cols.time].max()\n",
    "        self.min_time = edges[:,cols.time].min()\n",
    "            \n",
    "        return {'idx': edges, 'vals': torch.ones(edges.size(0))}\n",
    "\n",
    "    def times_from_names(self,files):\n",
    "        files2times = {}\n",
    "        times2files = {}\n",
    "\n",
    "        base = datetime.strptime(\"19800101\", '%Y%m%d')\n",
    "        for file in files:\n",
    "            delta =  (datetime.strptime(file[2: -4], '%Y%m%d') - base).days\n",
    "\n",
    "            files2times[file] = delta\n",
    "            times2files[delta] = file\n",
    "            \n",
    "        cont_files2times = {}\n",
    "\n",
    "        sorted_times = sorted(files2times.values())\n",
    "        new_t = 0\n",
    "\n",
    "        for t in sorted_times:\n",
    "            file = times2files[t]\n",
    "            cont_files2times[file] = new_t\n",
    "            new_t += 1\n",
    "        return cont_files2times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bitcoin_dataset():\n",
    "    def __init__(self,args):\n",
    "        assert args.task in ['link_pred', 'edge_cls'], 'bitcoin only implements link_pred or edge_cls'\n",
    "        self.ecols = Namespace({\n",
    "            'FromNodeId': 0,\n",
    "            'ToNodeId': 1,\n",
    "            'Weight': 2,\n",
    "            'TimeStep': 3\n",
    "        })\n",
    "        args.bitcoin_args = Namespace(args.bitcoin_args)\n",
    "\n",
    "        # build edge data structure\n",
    "        edges = self.load_edges(args.bitcoin_args)\n",
    "\n",
    "        edges = self.make_contigous_node_ids(edges)\n",
    "        num_nodes = edges[:, [self.ecols.FromNodeId, self.ecols.ToNodeId]].unique().size(0)\n",
    "\n",
    "        timesteps = aggregate_by_time(edges[:, self.ecols.TimeStep], args.bitcoin_args.aggr_time)\n",
    "        self.max_time = timesteps.max()\n",
    "        self.min_time = timesteps.min()\n",
    "        edges[:,self.ecols.TimeStep] = timesteps\n",
    "\n",
    "        edges[:,self.ecols.Weight] = self.cluster_negs_and_positives(edges[:, self.ecols.Weight])\n",
    "\n",
    "\n",
    "        # add the reversed link to make the graph undirected\n",
    "        edges = torch.cat([edges,edges[:, [self.ecols.ToNodeId, self.ecols.FromNodeId, self.ecols.Weight, self.ecols.TimeStep]]])\n",
    "\n",
    "        # separate classes\n",
    "        sp_indices = edges[:, [self.ecols.FromNodeId, self.ecols.ToNodeId, self.ecols.TimeStep]].t()\n",
    "        sp_values = edges[:, self.ecols.Weight]\n",
    "\n",
    "        neg_mask = sp_values == -1\n",
    "\n",
    "        neg_sp_indices = sp_indices[:,neg_mask]\n",
    "        neg_sp_values = sp_values[neg_mask]\n",
    "        neg_sp_edges = torch.sparse.LongTensor(neg_sp_indices ,neg_sp_values, torch.Size([num_nodes, num_nodes, self.max_time + 1])).coalesce()\n",
    "\n",
    "        pos_mask = sp_values == 1\n",
    "\n",
    "        pos_sp_indices = sp_indices[:,pos_mask]\n",
    "        pos_sp_values = sp_values[pos_mask]\n",
    "\n",
    "        pos_sp_edges = torch.sparse.LongTensor(pos_sp_indices, pos_sp_values, torch.Size([num_nodes, num_nodes, self.max_time + 1])).coalesce()\n",
    "\n",
    "        #scale positive class to separate after adding\n",
    "        pos_sp_edges *= 1000\n",
    "\n",
    "        #we substract the neg_sp_edges to make the values positive\n",
    "        sp_edges = (pos_sp_edges - neg_sp_edges).coalesce()\n",
    "\n",
    "        #separating negs and positive edges per edge/timestamp\n",
    "        vals = sp_edges._values()\n",
    "        neg_vals = vals % 1000\n",
    "        pos_vals = vals // 1000\n",
    "        #We add the negative and positive scores and do majority voting\n",
    "        vals = pos_vals - neg_vals\n",
    "        #creating labels new_vals -> the label of the edges\n",
    "        new_vals = torch.zeros(vals.size(0),dtype=torch.long)\n",
    "        new_vals[vals>0] = 1\n",
    "        new_vals[vals<=0] = 0\n",
    "        indices_labels = torch.cat([sp_edges._indices().t(),new_vals.view(-1, 1)],dim=1)\n",
    "\n",
    "        #the weight of the edges (vals), is simply the number of edges between two entities at each time_step\n",
    "        vals = pos_vals + neg_vals\n",
    "\n",
    "\n",
    "        self.edges = {'idx': indices_labels, 'vals': vals}\n",
    "        self.num_nodes = num_nodes\n",
    "        self.num_classes = 2\n",
    "\n",
    "    def cluster_negs_and_positives(self,ratings):\n",
    "        pos_indices = ratings > 0\n",
    "        neg_indices = ratings <= 0\n",
    "        ratings[pos_indices] = 1\n",
    "        ratings[neg_indices] = -1\n",
    "        return ratings\n",
    "\n",
    "    def prepare_node_feats(self,node_feats):\n",
    "        node_feats = node_feats[0]\n",
    "        return node_feats\n",
    "\n",
    "    def edges_to_sp_dict(self,edges):\n",
    "        idx = edges[:, [self.ecols.FromNodeId, self.ecols.ToNodeId, self.ecols.TimeStep]]\n",
    "\n",
    "        vals = edges[:, self.ecols.Weight]\n",
    "        return {'idx': idx, 'vals': vals}\n",
    "\n",
    "    def get_num_nodes(self,edges):\n",
    "        all_ids = edges[:,[self.ecols.FromNodeId,self.ecols.ToNodeId]]\n",
    "        num_nodes = all_ids.max() + 1\n",
    "        return num_nodes\n",
    "\n",
    "    def load_edges(self,bitcoin_args):\n",
    "        file = os.path.join(bitcoin_args.folder,bitcoin_args.edges_file)\n",
    "        with open(file) as f:\n",
    "            lines = f.read().splitlines()\n",
    "        edges = [[float(r) for r in row.split(',')] for row in lines]\n",
    "        edges = torch.tensor(edges,dtype = torch.long)\n",
    "        return edges\n",
    "\n",
    "    def make_contigous_node_ids(self,edges):\n",
    "        new_edges = edges[:,[self.ecols.FromNodeId,self.ecols.ToNodeId]]\n",
    "        _, new_edges = new_edges.unique(return_inverse=True)\n",
    "        edges[:,[self.ecols.FromNodeId,self.ecols.ToNodeId]] = new_edges\n",
    "        return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Elliptic_Temporal_Dataset():\n",
    "    def __init__(self,args):\n",
    "        args.elliptic_args = Namespace(args.elliptic_args)\n",
    "        tar_file = os.path.join(args.elliptic_args.folder, args.elliptic_args.tar_file)\n",
    "        tar_archive = tarfile.open(tar_file, 'r:gz')\n",
    "\n",
    "        self.nodes_labels_times = self.load_node_labels(args.elliptic_args, tar_archive)\n",
    "        self.edges = self.load_transactions(args.elliptic_args, tar_archive)\n",
    "        self.nodes, self.nodes_feats = self.load_node_feats(args.elliptic_args, tar_archive)\n",
    "\n",
    "    def load_node_feats(self, elliptic_args, tar_archive):\n",
    "        data = load_data_from_tar(elliptic_args.feats_file, tar_archive, starting_line=0)\n",
    "        nodes = data\n",
    "        nodes_feats = nodes[:,1:]\n",
    "\n",
    "        self.num_nodes = len(nodes)\n",
    "        self.feats_per_node = data.size(1) - 1\n",
    "        \n",
    "        return nodes, nodes_feats.float()\n",
    "\n",
    "    def load_node_labels(self, elliptic_args, tar_archive):\n",
    "        labels = load_data_from_tar(elliptic_args.classes_file, tar_archive, replace_unknow=True).long()\n",
    "        times = load_data_from_tar(elliptic_args.times_file, tar_archive, replace_unknow=True).long()\n",
    "        lcols = Namespace({'nid': 0, 'label': 1})\n",
    "        tcols = Namespace({'nid':0, 'time':1})\n",
    "\n",
    "        nodes_labels_times =[]\n",
    "        for i in range(len(labels)):\n",
    "            label = labels[i, [lcols.label]].long()\n",
    "            if label >= 0:\n",
    "                nid=labels[i, [lcols.nid]].long()\n",
    "                time=times[nid, [tcols.time]].long()\n",
    "                nodes_labels_times.append([nid, label, time])\n",
    "        nodes_labels_times = torch.tensor(nodes_labels_times)\n",
    "\n",
    "        return nodes_labels_times\n",
    "\n",
    "\n",
    "    def load_transactions(self, elliptic_args, tar_archive):\n",
    "        data = load_data_from_tar(elliptic_args.edges_file, tar_archive, type_fn=float, tensor_const=torch.LongTensor)\n",
    "        tcols = Namespace({'source': 0, 'target': 1, 'time': 2})\n",
    "        data = torch.cat([data,data[:,[1,0,2]]])\n",
    "\n",
    "        self.max_time = data[:,tcols.time].max()\n",
    "        self.min_time = data[:,tcols.time].min()\n",
    "\n",
    "        return {'idx': data, 'vals': torch.ones(data.size(0))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Reddit_Dataset():\n",
    "    def __init__(self,args):\n",
    "        args.reddit_args = Namespace(args.reddit_args)\n",
    "        folder = args.reddit_args.folder\n",
    "\n",
    "        # load nodes\n",
    "        cols = Namespace({'id': 0, 'feats': 1})\n",
    "        file = args.reddit_args.nodes_file\n",
    "        file = os.path.join(folder,file)\n",
    "        with open(file) as file:\n",
    "            file = file.read().splitlines()\n",
    "            \n",
    "        ids_str_to_int = {}\n",
    "        id_counter = 0\n",
    "\n",
    "        feats = []\n",
    "\n",
    "        for line in file:\n",
    "            line = line.split(',')\n",
    "            # node id\n",
    "            nd_id = line[0]\n",
    "            if nd_id not in ids_str_to_int.keys():\n",
    "                ids_str_to_int[nd_id] = id_counter\n",
    "                id_counter += 1\n",
    "                nd_feats = [float(r) for r in line[1:]]\n",
    "                feats.append(nd_feats)\n",
    "            else:\n",
    "                print('duplicate id', nd_id)\n",
    "                raise Exception('duplicate_id')\n",
    "\n",
    "        feats = torch.tensor(feats,dtype=torch.float)\n",
    "        num_nodes = feats.size(0)\n",
    "            \n",
    "        edges = []\n",
    "        not_found = 0\n",
    "\n",
    "        # load edges in title\n",
    "        edges_tmp, not_found_tmp = self.load_edges_from_file(args.reddit_args.title_edges_file, folder, ids_str_to_int)\n",
    "        edges.extend(edges_tmp)\n",
    "        not_found += not_found_tmp\n",
    "            \n",
    "        # load edges in bodies\n",
    "        edges_tmp, not_found_tmp = self.load_edges_from_file(args.reddit_args.body_edges_file, folder, ids_str_to_int)\n",
    "        edges.extend(edges_tmp)\n",
    "        not_found += not_found_tmp\n",
    "\n",
    "        # min time should be 0 and time aggregation\n",
    "        edges = torch.LongTensor(edges)\n",
    "        edges[:,2] = aggregate_by_time(edges[:,2],args.reddit_args.aggr_time)\n",
    "        max_time = edges[:,2].max()\n",
    "\n",
    "        # separate classes\n",
    "        sp_indices = edges[:,:3].t()\n",
    "        sp_values = edges[:,3]\n",
    "\n",
    "#         sp_edges = torch.sparse.LongTensor(sp_indices ,sp_values, torch.Size([num_nodes, num_nodes, max_time + 1])).coalesce()\n",
    "#         vals = sp_edges._values()\n",
    "#         print(vals[vals>0].sum() + vals[vals<0].sum()*-1)\n",
    "#         asdf\n",
    "            \n",
    "        pos_mask = sp_values == 1\n",
    "        neg_mask = sp_values == -1\n",
    "\n",
    "        neg_sp_indices = sp_indices[:,neg_mask]\n",
    "        neg_sp_values = sp_values[neg_mask]\n",
    "        neg_sp_edges = torch.sparse.LongTensor(neg_sp_indices, neg_sp_values, torch.Size([num_nodes, num_nodes, max_time + 1])).coalesce()\n",
    "\n",
    "        pos_sp_indices = sp_indices[:, pos_mask]\n",
    "        pos_sp_values = sp_values[pos_mask]\n",
    "            \n",
    "        pos_sp_edges = torch.sparse.LongTensor(pos_sp_indices, pos_sp_values, torch.Size([num_nodes, num_nodes, max_time + 1])).coalesce()\n",
    "\n",
    "        # scale positive class to separate after adding\n",
    "        pos_sp_edges *= 1000\n",
    "            \n",
    "        sp_edges = (pos_sp_edges - neg_sp_edges).coalesce()\n",
    "        \n",
    "        # separating negs and positive edges per edge/timestamp\n",
    "        vals = sp_edges._values()\n",
    "        neg_vals = vals%1000\n",
    "        pos_vals = vals//1000\n",
    "        # vals is simply the number of edges between two nodes at the same time_step, regardless of the edge label\n",
    "        vals = pos_vals - neg_vals\n",
    "\n",
    "        # creating labels new_vals -> the label of the edges\n",
    "        new_vals = torch.zeros(vals.size(0),dtype=torch.long)\n",
    "        new_vals[vals>0] = 1\n",
    "        new_vals[vals<=0] = 0\n",
    "        vals = pos_vals + neg_vals\n",
    "        indices_labels = torch.cat([sp_edges._indices().t(),new_vals.view(-1,1)],dim=1)\n",
    "            \n",
    "        self.edges = {'idx': indices_labels, 'vals': vals}\n",
    "        self.num_classes = 2\n",
    "        self.feats_per_node = feats.size(1)\n",
    "        self.num_nodes = num_nodes\n",
    "        self.nodes_feats = feats\n",
    "        self.max_time = max_time\n",
    "        self.min_time = 0\n",
    "\n",
    "    def prepare_node_feats(self,node_feats):\n",
    "        node_feats = node_feats[0]\n",
    "        return node_feats\n",
    "\n",
    "        \n",
    "    def load_edges_from_file(self,edges_file,folder,ids_str_to_int):\n",
    "        edges = []\n",
    "        not_found = 0\n",
    "\n",
    "        file = edges_file\n",
    "            \n",
    "        file = os.path.join(folder,file)\n",
    "        with open(file) as file:\n",
    "            file = file.read().splitlines()\n",
    "\n",
    "        cols = Namespace({\n",
    "            'source': 0,\n",
    "            'target': 1,\n",
    "            'time': 3,\n",
    "            'label': 4\n",
    "        })\n",
    "\n",
    "        base_time = datetime.strptime(\"19800101\", '%Y%m%d')\n",
    "\n",
    "            \n",
    "        for line in file[1:]:\n",
    "            fields = line.split('\\t')\n",
    "            sr = fields[cols.source]\n",
    "            tg = fields[cols.target]\n",
    "\n",
    "            if sr in ids_str_to_int.keys() and tg in ids_str_to_int.keys():\n",
    "                sr = ids_str_to_int[sr]\n",
    "                tg = ids_str_to_int[tg]\n",
    "\n",
    "                time = fields[cols.time].split(' ')[0]\n",
    "                time = datetime.strptime(time,'%Y-%m-%d')\n",
    "                time = (time - base_time).days\n",
    "\n",
    "                label = int(fields[cols.label])\n",
    "                edges.append([sr,tg,time,label])\n",
    "                # add the other edge to make it undirected\n",
    "                edges.append([tg,sr,time,label])\n",
    "            else:\n",
    "                not_found+=1\n",
    "\n",
    "        return edges, not_found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sbm_dataset():\n",
    "    def __init__(self,args):\n",
    "        assert args.task in ['link_pred'], 'sbm only implements link_pred'\n",
    "        self.ecols = Namespace({\n",
    "            'FromNodeId': 0,\n",
    "            'ToNodeId': 1,\n",
    "            'Weight': 2,\n",
    "            'TimeStep': 3\n",
    "        })\n",
    "        args.sbm_args = Namespace(args.sbm_args)\n",
    "\n",
    "        # build edge data structure\n",
    "        edges = self.load_edges(args.sbm_args)\n",
    "        timesteps = aggregate_by_time(edges[:,self.ecols.TimeStep], args.sbm_args.aggr_time)\n",
    "        self.max_time = timesteps.max()\n",
    "        self.min_time = timesteps.min()\n",
    "        print ('TIME', self.max_time, self.min_time )\n",
    "        \n",
    "        edges[:, self.ecols.TimeStep] = timesteps\n",
    "        edges[:, self.ecols.Weight] = self.cluster_negs_and_positives(edges[:, self.ecols.Weight])\n",
    "        \n",
    "        self.num_classes = edges[:, self.ecols.Weight].unique().size(0)\n",
    "        self.edges = self.edges_to_sp_dict(edges)\n",
    "        \n",
    "        # random node features\n",
    "        self.num_nodes = int(self.get_num_nodes(edges))\n",
    "        self.feats_per_node = args.sbm_args.feats_per_node\n",
    "        self.nodes_feats = torch.rand((self.num_nodes, self.feats_per_node))\n",
    "        self.num_non_existing = self.num_nodes ** 2 - edges.size(0)\n",
    "\n",
    "    def cluster_negs_and_positives(self, ratings):\n",
    "        pos_indices = ratings >= 0\n",
    "        neg_indices = ratings < 0\n",
    "        ratings[pos_indices] = 1\n",
    "        ratings[neg_indices] = 0\n",
    "        return ratings\n",
    "\n",
    "    def prepare_node_feats(self,node_feats):\n",
    "        node_feats = node_feats[0]\n",
    "        return node_feats\n",
    "\n",
    "    def edges_to_sp_dict(self,edges):\n",
    "        idx = edges[:, [self.ecols.FromNodeId, self.ecols.ToNodeId, self.ecols.TimeStep]]\n",
    "\n",
    "        vals = edges[:, self.ecols.Weight]\n",
    "        return {'idx': idx, 'vals': vals}\n",
    "\n",
    "    def get_num_nodes(self,edges):\n",
    "        all_ids = edges[:, [self.ecols.FromNodeId,self.ecols.ToNodeId]]\n",
    "        num_nodes = all_ids.max() + 1\n",
    "        return num_nodes\n",
    "\n",
    "    def load_edges(self,sbm_args, starting_line = 1):\n",
    "        file = os.path.join(sbm_args.folder, sbm_args.edges_file)\n",
    "        with open(file) as f:\n",
    "            lines = f.read().splitlines()\n",
    "        edges = [[float(r) for r in row.split(',')] for row in lines[starting_line:]]\n",
    "        edges = torch.tensor(edges,dtype = torch.long)\n",
    "        return edges\n",
    "\n",
    "    def make_contigous_node_ids(self,edges):\n",
    "        new_edges = edges[:,[self.ecols.FromNodeId,self.ecols.ToNodeId]]\n",
    "        _, new_edges = new_edges.unique(return_inverse=True)\n",
    "        edges[:,[self.ecols.FromNodeId,self.ecols.ToNodeId]] = new_edges\n",
    "        return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Uc_Irvine_Message_Dataset():\n",
    "    def __init__(self,args):\n",
    "        args.uc_irc_args = Namespace(args.uc_irc_args)\n",
    "\n",
    "        tar_file = os.path.join(args.uc_irc_args.folder, args.uc_irc_args.tar_file)  \n",
    "        tar_archive = tarfile.open(tar_file, 'r:bz2')\n",
    "\n",
    "        self.edges = self.load_edges(args,tar_archive)\n",
    "\n",
    "    def load_edges(self,args,tar_archive):\n",
    "        data = load_data_from_tar(args.uc_irc_args.edges_file, tar_archive, starting_line=2, sep=' ')\n",
    "        cols = Namespace({\n",
    "            'source': 0,\n",
    "            'target': 1,\n",
    "            'weight': 2,\n",
    "            'time': 3\n",
    "        })\n",
    "\n",
    "        data = data.long()\n",
    "\n",
    "        self.num_nodes = int(data[:, [cols.source,cols.target]].max())\n",
    "\n",
    "        # first id should be 0 (they are already contiguous)\n",
    "        data[:, [cols.source,cols.target]] -= 1\n",
    "\n",
    "        # add edges in the other direction (simmetric)\n",
    "        data = torch.cat([data, data[:,[cols.target, cols.source, cols.weight, cols.time]]], dim=0)\n",
    "\n",
    "        data[:, cols.time] = aggregate_by_time(data[:,cols.time], args.uc_irc_args.aggr_time)\n",
    "\n",
    "        ids = data[:,cols.source] * self.num_nodes + data[:,cols.target]\n",
    "        self.num_non_existing = float(self.num_nodes**2 - ids.unique().size(0))\n",
    "\n",
    "        idx = data[:,[cols.source, cols.target, cols.time]]\n",
    "\n",
    "        self.max_time = data[:,cols.time].max()\n",
    "        self.min_time = data[:,cols.time].min()\n",
    "            \n",
    "\n",
    "        return {'idx': idx, 'vals': torch.ones(idx.size(0))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tasker Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ECOLS = Namespace({\n",
    "    'source': 0,\n",
    "    'target': 1,\n",
    "    'time': 2,\n",
    "    'label':3\n",
    "}) # --> added for edge_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_2_hot_deg_feats(adj,max_deg_out,max_deg_in,num_nodes):\n",
    "#     # For now it'll just return a 2-hot vector\n",
    "#     adj['vals'] = torch.ones(adj['idx'].size(0))\n",
    "#     degs_out, degs_in = get_degree_vects(adj,num_nodes)\n",
    "    \n",
    "#     degs_out = {'idx': torch.cat([torch.arange(num_nodes).view(-1,1),\n",
    "#                                   degs_out.view(-1,1)],dim=1),\n",
    "#                 'vals': torch.ones(num_nodes)}\n",
    "    \n",
    "#     # print ('XXX degs_out',degs_out['idx'].size(),degs_out['vals'].size())\n",
    "#     degs_out = make_sparse_tensor(degs_out,'long',[num_nodes,max_deg_out])\n",
    "\n",
    "#     degs_in = {'idx': torch.cat([torch.arange(num_nodes).view(-1,1),\n",
    "#                                   degs_in.view(-1,1)],dim=1),\n",
    "#                 'vals': torch.ones(num_nodes)}\n",
    "#     degs_in = make_sparse_tensor(degs_in,'long',[num_nodes,max_deg_in])\n",
    "\n",
    "#     hot_2 = torch.cat([degs_out,degs_in],dim = 1)\n",
    "#     hot_2 = {'idx': hot_2._indices().t(),\n",
    "#              'vals': hot_2._values()}\n",
    "\n",
    "#     return hot_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_1_hot_deg_feats(adj,max_deg,num_nodes):\n",
    "    # For now it'll just return a 2-hot vector\n",
    "    new_vals = torch.ones(adj['idx'].size(0))\n",
    "    new_adj = {'idx':adj['idx'], 'vals': new_vals}\n",
    "    degs_out, _ = get_degree_vects(new_adj,num_nodes)\n",
    "    \n",
    "    degs_out = {\n",
    "        'idx': torch.cat([torch.arange(num_nodes).view(-1,1), degs_out.view(-1,1)],dim=1),\n",
    "        'vals': torch.ones(num_nodes)\n",
    "    }\n",
    "    \n",
    "    # print ('XXX degs_out',degs_out['idx'].size(),degs_out['vals'].size())\n",
    "    degs_out = make_sparse_tensor(degs_out, 'long', [num_nodes,max_deg])\n",
    "\n",
    "    hot_1 = {'idx': degs_out._indices().t(), 'vals': degs_out._values()}\n",
    "    return hot_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_degs(args,dataset,all_window=False):\n",
    "    max_deg_out = []\n",
    "    max_deg_in = []\n",
    "    for t in range(dataset.min_time, dataset.max_time):\n",
    "        if all_window:\n",
    "            window = t+1\n",
    "        else:\n",
    "            window = args.adj_mat_time_window\n",
    "\n",
    "        cur_adj = get_sp_adj(edges = dataset.edges, time = t, weighted = False, time_window = window)\n",
    "        # print(window)\n",
    "        cur_out, cur_in = get_degree_vects(cur_adj,dataset.num_nodes)\n",
    "        max_deg_out.append(cur_out.max())\n",
    "        max_deg_in.append(cur_in.max())\n",
    "        # max_deg_out = torch.stack([max_deg_out,cur_out.max()]).max()\n",
    "        # max_deg_in = torch.stack([max_deg_in,cur_in.max()]).max()\n",
    "    # exit()\n",
    "    max_deg_out = torch.stack(max_deg_out).max()\n",
    "    max_deg_in = torch.stack(max_deg_in).max()\n",
    "    max_deg_out = int(max_deg_out) + 1\n",
    "    max_deg_in = int(max_deg_in) + 1\n",
    "    \n",
    "    return max_deg_out, max_deg_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_degs_static(num_nodes, adj_matrix):\n",
    "    cur_out, cur_in = get_degree_vects(adj_matrix, num_nodes)\n",
    "    max_deg_out = int(cur_out.max().item()) + 1\n",
    "    max_deg_in = int(cur_in.max().item()) + 1\n",
    "    \n",
    "    return max_deg_out, max_deg_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_degree_vects(adj,num_nodes):\n",
    "    adj = make_sparse_tensor(adj,'long',[num_nodes])\n",
    "    degs_out = adj.matmul(torch.ones(num_nodes,1,dtype = torch.long))\n",
    "    degs_in = adj.t().matmul(torch.ones(num_nodes,1,dtype = torch.long))\n",
    "    return degs_out, degs_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sp_adj(edges,time,weighted,time_window):\n",
    "    idx = edges['idx']\n",
    "    subset = idx[:,ECOLS.time] <= time\n",
    "    subset = subset * (idx[:,ECOLS.time] > (time - time_window))\n",
    "    idx = edges['idx'][subset][:,[ECOLS.source, ECOLS.target]]  \n",
    "    vals = edges['vals'][subset]\n",
    "    out = torch.sparse.FloatTensor(idx.t(),vals).coalesce()\n",
    "    idx = out._indices().t()\n",
    "    \n",
    "    if weighted:\n",
    "        vals = out._values()\n",
    "    else:\n",
    "        vals = torch.ones(idx.size(0),dtype=torch.long)\n",
    "\n",
    "    return {'idx': idx, 'vals': vals}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edge_labels(edges,time):\n",
    "    idx = edges['idx']\n",
    "    subset = idx[:,ECOLS.time] == time\n",
    "    idx = edges['idx'][subset][:,[ECOLS.source, ECOLS.target]]  \n",
    "    vals = edges['idx'][subset][:,ECOLS.label]\n",
    "\n",
    "    return {'idx': idx, 'vals': vals}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_mask(cur_adj,num_nodes):\n",
    "    mask = torch.zeros(num_nodes) - float(\"Inf\")\n",
    "    non_zero = cur_adj['idx'].unique()\n",
    "    mask[non_zero] = 0\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_static_sp_adj(edges,weighted):\n",
    "    idx = edges['idx']\n",
    "    #subset = idx[:,ECOLS.time] <= time\n",
    "    #subset = subset * (idx[:,ECOLS.time] > (time - time_window))\n",
    "    #idx = edges['idx'][subset][:,[ECOLS.source, ECOLS.target]]  \n",
    "    if weighted:\n",
    "        vals = edges['vals'][subset]\n",
    "    else:\n",
    "        vals = torch.ones(idx.size(0),dtype = torch.long)\n",
    "\n",
    "    return {'idx': idx, 'vals': vals}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sp_adj_only_new(edges,time,weighted):\n",
    "    return get_sp_adj(edges, time, weighted, time_window=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adj(adj,num_nodes):\n",
    "    '''\n",
    "    takes an adj matrix as a dict with idx and vals and normalize it by: \n",
    "        - adding an identity matrix, \n",
    "        - computing the degree vector\n",
    "        - multiplying each element of the adj matrix (aij) by (di*dj)^-1/2\n",
    "    '''\n",
    "    idx = adj['idx']\n",
    "    vals = adj['vals']\n",
    "\n",
    "    \n",
    "    sp_tensor = torch.sparse.FloatTensor(idx.t(),vals.type(torch.float),torch.Size([num_nodes,num_nodes]))\n",
    "    \n",
    "    sparse_eye = make_sparse_eye(num_nodes)\n",
    "    sp_tensor = sparse_eye + sp_tensor\n",
    "\n",
    "    idx = sp_tensor._indices()\n",
    "    vals = sp_tensor._values()\n",
    "\n",
    "    degree = torch.sparse.sum(sp_tensor,dim=1).to_dense()\n",
    "    di = degree[idx[0]]\n",
    "    dj = degree[idx[1]]\n",
    "\n",
    "    vals = vals * ((di * dj) ** -0.5)\n",
    "    \n",
    "    return {'idx': idx.t(), 'vals': vals}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sparse_eye(size):\n",
    "    eye_idx = torch.arange(size)\n",
    "    eye_idx = torch.stack([eye_idx,eye_idx],dim=1).t()\n",
    "    vals = torch.ones(size)\n",
    "    eye = torch.sparse.FloatTensor(eye_idx,vals,torch.Size([size,size]))\n",
    "    return eye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_non_existing_edges(adj,tot_nodes):\n",
    "    true_ids = adj['idx'].t().numpy()\n",
    "    true_ids = get_edges_ids(true_ids,tot_nodes)\n",
    "\n",
    "    all_edges_idx = np.arange(tot_nodes)\n",
    "    all_edges_idx = np.array(np.meshgrid(all_edges_idx,\n",
    "                                         all_edges_idx)).reshape(2,-1)\n",
    "\n",
    "    all_edges_ids = get_edges_ids(all_edges_idx,tot_nodes)\n",
    "\n",
    "    #only edges that are not in the true_ids should keep here\n",
    "    mask = np.logical_not(np.isin(all_edges_ids,true_ids))\n",
    "\n",
    "    non_existing_edges_idx = all_edges_idx[:,mask]\n",
    "    edges = torch.tensor(non_existing_edges_idx).t()\n",
    "    vals = torch.zeros(edges.size(0), dtype = torch.long)\n",
    "    return {'idx': edges, 'vals': vals}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_non_existing_edges(adj,number, tot_nodes, smart_sampling, existing_nodes=None):\n",
    "    # print('----------')\n",
    "    t0 = time.time()\n",
    "    idx = adj['idx'].t().numpy()\n",
    "    true_ids = get_edges_ids(idx,tot_nodes)\n",
    "\n",
    "    true_ids = set(true_ids)\n",
    "\n",
    "    #the maximum of edges would be all edges that don't exist between nodes that have edges\n",
    "    num_edges = min(number,idx.shape[1] * (idx.shape[1]-1) - len(true_ids))\n",
    "\n",
    "    if smart_sampling:\n",
    "        #existing_nodes = existing_nodes.numpy()\n",
    "        def sample_edges(num_edges):\n",
    "            # print('smart_sampling')\n",
    "            from_id = np.random.choice(idx[0],size = num_edges,replace = True)\n",
    "            to_id = np.random.choice(existing_nodes,size = num_edges, replace = True)\n",
    "            #print ('smart_sampling', from_id, to_id)\n",
    "            \n",
    "            if num_edges>1:\n",
    "                edges = np.stack([from_id,to_id])\n",
    "            else:\n",
    "                edges = np.concatenate([from_id,to_id])\n",
    "            return edges\n",
    "    else:\n",
    "        def sample_edges(num_edges):\n",
    "            if num_edges > 1:\n",
    "                edges = np.random.randint(0,tot_nodes,(2,num_edges))\n",
    "            else:\n",
    "                edges = np.random.randint(0,tot_nodes,(2,))\n",
    "            return edges\n",
    "\n",
    "    edges = sample_edges(num_edges*4)\n",
    "\n",
    "    edge_ids = edges[0] * tot_nodes + edges[1]\n",
    "    \n",
    "    out_ids = set()\n",
    "    num_sampled = 0\n",
    "    sampled_indices = []\n",
    "    for i in range(num_edges*4):\n",
    "        eid = edge_ids[i]\n",
    "        #ignore if any of these conditions happen\n",
    "        if eid in out_ids or edges[0,i] == edges[1,i] or eid in true_ids:\n",
    "            continue\n",
    "\n",
    "        #add the eid and the index to a list\n",
    "        out_ids.add(eid)\n",
    "        sampled_indices.append(i)\n",
    "        num_sampled += 1\n",
    "\n",
    "        #if we have sampled enough edges break\n",
    "        if num_sampled >= num_edges:\n",
    "            break\n",
    "\n",
    "    edges = edges[:,sampled_indices]\n",
    "    edges = torch.tensor(edges).t()\n",
    "    vals = torch.zeros(edges.size(0),dtype = torch.long)\n",
    "    return {'idx': edges, 'vals': vals}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edges_ids(sp_idx, tot_nodes):\n",
    "    # print(sp_idx)\n",
    "    # print(tot_nodes)\n",
    "    # print(sp_idx[0]*tot_nodes)\n",
    "    return sp_idx[0]*tot_nodes + sp_idx[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Taskers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Edge_Cls_Tasker():\n",
    "    def __init__(self, args, dataset):\n",
    "        self.data = dataset\n",
    "        # max_time for link pred should be one before\n",
    "        self.max_time = dataset.max_time\n",
    "        self.args = args\n",
    "        self.num_classes = dataset.num_classes\n",
    "\n",
    "        if not args.use_1_hot_node_feats:\n",
    "            self.feats_per_node = dataset.feats_per_node\n",
    "\n",
    "        self.get_node_feats = self.build_get_node_feats(args,dataset)\n",
    "        self.prepare_node_feats = self.build_prepare_node_feats(args,dataset)\n",
    "            \n",
    "        self.is_static = False\n",
    "\n",
    "    def build_prepare_node_feats(self,args,dataset):\n",
    "        if args.use_2_hot_node_feats or args.use_1_hot_node_feats:\n",
    "            def prepare_node_feats(node_feats):\n",
    "                return sparse_prepare_tensor(node_feats, torch_size = [dataset.num_nodes, self.feats_per_node])\n",
    "        else:\n",
    "            prepare_node_feats = self.data.prepare_node_feats\n",
    "\n",
    "        return prepare_node_feats\n",
    "\n",
    "\n",
    "    def build_get_node_feats(self,args,dataset):\n",
    "        if args.use_2_hot_node_feats:\n",
    "            max_deg_out, max_deg_in = get_max_degs(args,dataset)\n",
    "            self.feats_per_node = max_deg_out + max_deg_in\n",
    "            def get_node_feats(adj):\n",
    "                return get_2_hot_deg_feats(adj, max_deg_out, max_deg_in, dataset.num_nodes)\n",
    "        elif args.use_1_hot_node_feats:\n",
    "            max_deg,_ = get_max_degs(args,dataset)\n",
    "            self.feats_per_node = max_deg\n",
    "            def get_node_feats(adj):\n",
    "                return get_1_hot_deg_feats(adj, max_deg, dataset.num_nodes)\n",
    "        else:\n",
    "            def get_node_feats(adj):\n",
    "                return dataset.nodes_feats\n",
    "\n",
    "        return get_node_feats\n",
    "\n",
    "\n",
    "    def get_sample(self,idx,test):\n",
    "        hist_adj_list = []\n",
    "        hist_ndFeats_list = []\n",
    "        hist_mask_list = []\n",
    "\n",
    "        for i in range(idx - self.args.num_hist_steps, idx+1):\n",
    "            cur_adj = get_sp_adj(edges = self.data.edges, \n",
    "                                    time = i,\n",
    "                                    weighted = True,\n",
    "                                    time_window = self.args.adj_mat_time_window)\n",
    "            node_mask = get_node_mask(cur_adj, self.data.num_nodes)\n",
    "            node_feats = self.get_node_feats(cur_adj)\n",
    "            cur_adj = normalize_adj(adj = cur_adj, num_nodes = self.data.num_nodes)\n",
    "\n",
    "            hist_adj_list.append(cur_adj)\n",
    "            hist_ndFeats_list.append(node_feats)\n",
    "            hist_mask_list.append(node_mask)\n",
    "\n",
    "        label_adj = get_edge_labels(edges = self.data.edges, time = idx)\n",
    "            \n",
    "        return {'idx': idx, 'hist_adj_list': hist_adj_list, 'hist_ndFeats_list': hist_ndFeats_list, 'label_sp': label_adj, 'node_mask_list': hist_mask_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Link_Pred_Tasker():\n",
    "    '''\n",
    "    Creates a tasker object which computes the required inputs for training on a link prediction\n",
    "    task. It receives a dataset object which should have two attributes: nodes_feats and edges, this\n",
    "    makes the tasker independent of the dataset being used (as long as mentioned attributes have the same\n",
    "    structure).\n",
    "\n",
    "    Based on the dataset it implements the get_sample function required by edge_cls_trainer.\n",
    "    This is a dictionary with:\n",
    "        - time_step: the time_step of the prediction\n",
    "        - hist_adj_list: the input adjacency matrices until t, each element of the list \n",
    "                         is a sparse tensor with the current edges. For link_pred they're\n",
    "                         unweighted\n",
    "        - nodes_feats_list: the input nodes for the GCN models, each element of the list is a tensor\n",
    "                            two dimmensions: node_idx and node_feats\n",
    "        - label_adj: a sparse representation of the target edges. A dict with two keys: idx: M by 2 \n",
    "                     matrix with the indices of the nodes conforming each edge, vals: 1 if the node exists, 0 if it doesn't\n",
    "\n",
    "    There's a test difference in the behavior, on test (or development), the number of sampled non existing \n",
    "    edges should be higher.\n",
    "    '''\n",
    "    def __init__(self,args,dataset):\n",
    "        self.data = dataset\n",
    "        # max_time for link pred should be one before\n",
    "        self.max_time = dataset.max_time - 1\n",
    "        self.args = args\n",
    "        self.num_classes = 2\n",
    "\n",
    "        if not (args.use_2_hot_node_feats or args.use_1_hot_node_feats):\n",
    "            self.feats_per_node = dataset.feats_per_node\n",
    "\n",
    "        self.get_node_feats = self.build_get_node_feats(args,dataset)\n",
    "        self.prepare_node_feats = self.build_prepare_node_feats(args,dataset)\n",
    "        self.is_static = False\n",
    "        \n",
    "        '''TO CREATE THE CSV DATASET TO USE IN DynGEM\n",
    "        print ('min max time:', self.data.min_time, self.data.max_time)\n",
    "        file = open('data/autonomous_syst100_adj.csv','w')\n",
    "        file.write ('source,target,weight,time\\n')\n",
    "        for time in range(self.data.min_time, self.data.max_time):\n",
    "            adj_t = get_sp_adj(edges = self.data.edges,\n",
    "                       time = time,\n",
    "                       weighted = True,\n",
    "                       time_window = 1)\n",
    "            # node_feats = self.get_node_feats(adj_t)\n",
    "            print (time, len(adj_t))\n",
    "            idx = adj_t['idx']\n",
    "            vals = adj_t['vals']\n",
    "            num_nodes = self.data.num_nodes\n",
    "            sp_tensor = torch.sparse.FloatTensor(idx.t(),vals.type(torch.float),torch.Size([num_nodes,num_nodes]))\n",
    "            dense_tensor = sp_tensor.to_dense()\n",
    "            idx = sp_tensor._indices()\n",
    "            for i in range(idx.size()[1]):\n",
    "                i0=idx[0,i]\n",
    "                i1=idx[1,i]\n",
    "                w = dense_tensor[i0,i1]\n",
    "                file.write(str(i0.item())+','+str(i1.item())+','+str(w.item())+','+str(time)+'\\n')\n",
    "\n",
    "            # for i, v in zip(idx, vals):\n",
    "            # file.write(str(i[0].item())+','+str(i[1].item())+','+str(v.item())+','+str(time)+'\\n')\n",
    "\n",
    "        file.close()\n",
    "        exit'''\n",
    "\n",
    "#     def build_get_non_existing(args):\n",
    "#         if args.use_smart_neg_sampling:\n",
    "#         else:\n",
    "#             return get_non_existing_edges\n",
    "\n",
    "    def build_prepare_node_feats(self,args,dataset):\n",
    "        if args.use_2_hot_node_feats or args.use_1_hot_node_feats:\n",
    "            def prepare_node_feats(node_feats):\n",
    "                return sparse_prepare_tensor(node_feats, torch_size= [dataset.num_nodes, self.feats_per_node])\n",
    "        else:\n",
    "            prepare_node_feats = self.data.prepare_node_feats\n",
    "\n",
    "        return prepare_node_feats\n",
    "\n",
    "\n",
    "    def build_get_node_feats(self,args,dataset):\n",
    "        if args.use_2_hot_node_feats:\n",
    "            max_deg_out, max_deg_in = get_max_degs(args,dataset)\n",
    "            self.feats_per_node = max_deg_out + max_deg_in\n",
    "            def get_node_feats(adj):\n",
    "                return get_2_hot_deg_feats(adj, max_deg_out, max_deg_in, dataset.num_nodes)\n",
    "        elif args.use_1_hot_node_feats:\n",
    "            max_deg,_ = get_max_degs(args,dataset)\n",
    "            self.feats_per_node = max_deg\n",
    "            def get_node_feats(adj):\n",
    "                return get_1_hot_deg_feats(adj, max_deg, dataset.num_nodes)\n",
    "        else:\n",
    "            def get_node_feats(adj):\n",
    "                return dataset.nodes_feats\n",
    "\n",
    "        return get_node_feats\n",
    "\n",
    "\n",
    "    def get_sample(self,idx,test, **kwargs):\n",
    "        hist_adj_list = []\n",
    "        hist_ndFeats_list = []\n",
    "        hist_mask_list = []\n",
    "        existing_nodes = []\n",
    "        for i in range(idx - self.args.num_hist_steps, idx+1):\n",
    "            cur_adj = get_sp_adj(edges = self.data.edges, time = i, weighted = True, time_window = self.args.adj_mat_time_window)\n",
    "\n",
    "            if self.args.smart_neg_sampling:\n",
    "                existing_nodes.append(cur_adj['idx'].unique())\n",
    "            else:\n",
    "                existing_nodes = None\n",
    "\n",
    "            node_mask = get_node_mask(cur_adj, self.data.num_nodes)\n",
    "            node_feats = self.get_node_feats(cur_adj)\n",
    "            cur_adj = normalize_adj(adj = cur_adj, num_nodes = self.data.num_nodes)\n",
    "\n",
    "            hist_adj_list.append(cur_adj)\n",
    "            hist_ndFeats_list.append(node_feats)\n",
    "            hist_mask_list.append(node_mask)\n",
    "\n",
    "        # This would be if we were training on all the edges in the time_window\n",
    "        label_adj = get_sp_adj(edges = self.data.edges, time = idx+1, weighted = False, time_window =  self.args.adj_mat_time_window)\n",
    "        if test:\n",
    "            neg_mult = self.args.negative_mult_test\n",
    "        else:\n",
    "            neg_mult = self.args.negative_mult_training\n",
    "            \n",
    "        if self.args.smart_neg_sampling:\n",
    "            existing_nodes = torch.cat(existing_nodes)\n",
    "\n",
    "            \n",
    "        if 'all_edges' in kwargs.keys() and kwargs['all_edges'] == True:\n",
    "            non_exisiting_adj = get_all_non_existing_edges(adj = label_adj, tot_nodes = self.data.num_nodes)\n",
    "        else:\n",
    "            non_exisiting_adj = get_non_existing_edges(adj = label_adj, \n",
    "                                                          number = label_adj['vals'].size(0) * neg_mult,\n",
    "                                                          tot_nodes = self.data.num_nodes,\n",
    "                                                          smart_sampling = self.args.smart_neg_sampling,\n",
    "                                                          existing_nodes = existing_nodes)\n",
    "\n",
    "#         label_adj = get_sp_adj_only_new(edges = self.data.edges,\n",
    "#                                            weighted = False,\n",
    "#                                            time = idx)\n",
    "            \n",
    "        label_adj['idx'] = torch.cat([label_adj['idx'],non_exisiting_adj['idx']])\n",
    "        label_adj['vals'] = torch.cat([label_adj['vals'],non_exisiting_adj['vals']])\n",
    "        return {'idx': idx, 'hist_adj_list': hist_adj_list, 'hist_ndFeats_list': hist_ndFeats_list, 'label_sp': label_adj, 'node_mask_list': hist_mask_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node_Cls_Tasker():\n",
    "    def __init__(self,args,dataset):\n",
    "        self.data = dataset\n",
    "        self.max_time = dataset.max_time\n",
    "        self.args = args\n",
    "        self.num_classes = 2\n",
    "        self.feats_per_node = dataset.feats_per_node\n",
    "        self.nodes_labels_times = dataset.nodes_labels_times\n",
    "        self.get_node_feats = self.build_get_node_feats(args,dataset)\n",
    "        self.prepare_node_feats = self.build_prepare_node_feats(args,dataset)\n",
    "        self.is_static = False\n",
    "\n",
    "    def build_get_node_feats(self,args,dataset):\n",
    "        if args.use_2_hot_node_feats:\n",
    "            max_deg_out, max_deg_in = get_max_degs(args,dataset,all_window = True)\n",
    "            self.feats_per_node = max_deg_out + max_deg_in\n",
    "            def get_node_feats(i,adj):\n",
    "                return get_2_hot_deg_feats(adj, max_deg_out, max_deg_in, dataset.num_nodes)\n",
    "        elif args.use_1_hot_node_feats:\n",
    "            max_deg,_ = get_max_degs(args,dataset)\n",
    "            self.feats_per_node = max_deg\n",
    "            def get_node_feats(i,adj):\n",
    "                return get_1_hot_deg_feats(adj, max_deg, dataset.num_nodes)\n",
    "        else:\n",
    "            def get_node_feats(i,adj):\n",
    "                return dataset.nodes_feats  # [i] I'm ignoring the index since the features for Elliptic are static\n",
    "\n",
    "        return get_node_feats\n",
    "\n",
    "    def build_prepare_node_feats(self,args,dataset):\n",
    "        if args.use_2_hot_node_feats or args.use_1_hot_node_feats:\n",
    "            def prepare_node_feats(node_feats):\n",
    "                return sparse_prepare_tensor(node_feats, torch_size= [dataset.num_nodes, self.feats_per_node])\n",
    "        # elif args.use_1_hot_node_feats:\n",
    "\n",
    "        else:\n",
    "            def prepare_node_feats(node_feats):\n",
    "                return node_feats[0]  # I'll have to check this up\n",
    "\n",
    "        return prepare_node_feats\n",
    "\n",
    "    def get_sample(self,idx,test):\n",
    "        hist_adj_list = []\n",
    "        hist_ndFeats_list = []\n",
    "        hist_mask_list = []\n",
    "\n",
    "        for i in range(idx - self.args.num_hist_steps, idx+1):\n",
    "            # all edgess included from the beginning\n",
    "            cur_adj = get_sp_adj(edges = self.data.edges, time = i, weighted = True, time_window = self.args.adj_mat_time_window)  # changed this to keep only a time window\n",
    "\n",
    "            node_mask = get_node_mask(cur_adj, self.data.num_nodes)\n",
    "            node_feats = self.get_node_feats(i,cur_adj)\n",
    "            cur_adj = normalize_adj(adj = cur_adj, num_nodes = self.data.num_nodes)\n",
    "\n",
    "            hist_adj_list.append(cur_adj)\n",
    "            hist_ndFeats_list.append(node_feats)\n",
    "            hist_mask_list.append(node_mask)\n",
    "\n",
    "        label_adj = self.get_node_labels(idx)\n",
    "\n",
    "        return {'idx': idx, 'hist_adj_list': hist_adj_list, 'hist_ndFeats_list': hist_ndFeats_list, 'label_sp': label_adj, 'node_mask_list': hist_mask_list}\n",
    "\n",
    "\n",
    "    def get_node_labels(self,idx):\n",
    "        \"\"\"\n",
    "        window_nodes = get_sp_adj(edges = self.data.edges, time = idx, weighted = False, time_window = self.args.adj_mat_time_window)\n",
    "        window_nodes = window_nodes['idx'].unique()\n",
    "\n",
    "        fraud_times = self.data.nodes_labels_times[window_nodes]\n",
    "\n",
    "        non_fraudulent = ((fraud_times > idx) + (fraud_times == -1))>0\n",
    "        non_fraudulent = window_nodes[non_fraudulent]\n",
    "\n",
    "        fraudulent = (fraud_times <= idx) * (fraud_times > max(idx -  self.args.adj_mat_time_window,0))\n",
    "        fraudulent = window_nodes[fraudulent]\n",
    "\n",
    "        label_idx = torch.cat([non_fraudulent,fraudulent]).view(-1,1)\n",
    "        label_vals = torch.cat([torch.zeros(non_fraudulent.size(0)), torch.ones(fraudulent.size(0))])\n",
    "        \"\"\"\n",
    "        node_labels = self.nodes_labels_times\n",
    "        subset = node_labels[:,2]==idx\n",
    "        label_idx = node_labels[subset,0]\n",
    "        label_vals = node_labels[subset,1]\n",
    "\n",
    "        return {'idx': label_idx, 'vals': label_vals}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Static_Node_Cls_Tasker(Node_Cls_Tasker):\n",
    "    def __init__(self,args,dataset):\n",
    "        self.data = dataset\n",
    "        self.args = args\n",
    "        self.num_classes = 2\n",
    "        self.adj_matrix = get_static_sp_adj(edges = self.data.edges, weighted = False)\n",
    "\n",
    "        if args.use_2_hot_node_feats:\n",
    "            max_deg_out, max_deg_in = get_max_degs_static(self.data.num_nodes,self.adj_matrix)\n",
    "            self.feats_per_node = max_deg_out + max_deg_in\n",
    "            # print ('feats_per_node',self.feats_per_node ,max_deg_out, max_deg_in)\n",
    "            self.nodes_feats = get_2_hot_deg_feats(self.adj_matrix, max_deg_out, max_deg_in, dataset.num_nodes)\n",
    "\n",
    "            # print('XXXX self.nodes_feats',self.nodes_feats)\n",
    "            self.nodes_feats = sparse_prepare_tensor(self.nodes_feats, torch_size= [self.data.num_nodes,self.feats_per_node], ignore_batch_dim = False)\n",
    "\n",
    "        else:\n",
    "            self.feats_per_node = dataset.feats_per_node\n",
    "            self.nodes_feats = self.data.node_feats\n",
    "\n",
    "        self.adj_matrix = normalize_adj(adj = self.adj_matrix, num_nodes = self.data.num_nodes)\n",
    "        self.is_static = True\n",
    "\n",
    "    def get_sample(self,idx,test):\n",
    "        # print ('self.adj_matrix',self.adj_matrix.size())\n",
    "        idx=int(idx)\n",
    "        # node_feats = self.data.node_feats_dict[idx]\n",
    "        label = self.data.nodes_labels[idx]\n",
    "\n",
    "        return {'idx': idx,\n",
    "                #'node_feats': self.data.node_feats,\n",
    "                #'adj': self.adj_matrix,\n",
    "                'label': label\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sp_GCN(torch.nn.Module):\n",
    "    def __init__(self,args,activation):\n",
    "        super().__init__()\n",
    "        self.activation = activation\n",
    "        self.num_layers = args.num_layers\n",
    "\n",
    "        self.w_list = nn.ParameterList()\n",
    "        for i in range(self.num_layers):\n",
    "            if i==0:\n",
    "                w_i = Parameter(torch.Tensor(args.feats_per_node, args.layer_1_feats))\n",
    "                reset_param(w_i)\n",
    "            else:\n",
    "                w_i = Parameter(torch.Tensor(args.layer_1_feats, args.layer_2_feats))\n",
    "                reset_param(w_i)\n",
    "            self.w_list.append(w_i)\n",
    "\n",
    "\n",
    "    def forward(self,A_list, Nodes_list, nodes_mask_list):\n",
    "        node_feats = Nodes_list[-1]\n",
    "        #A_list: T, each element sparse tensor\n",
    "        #take only last adj matrix in time\n",
    "        Ahat = A_list[-1]\n",
    "        #Ahat: NxN ~ 30k\n",
    "        #sparse multiplication\n",
    "\n",
    "        # Ahat NxN\n",
    "        # self.node_embs = Nxk\n",
    "        #\n",
    "        # note(bwheatman, tfk): change order of matrix multiply\n",
    "        last_l = self.activation(Ahat.matmul(node_feats.matmul(self.w_list[0])))\n",
    "        for i in range(1, self.num_layers):\n",
    "            last_l = self.activation(Ahat.matmul(last_l.matmul(self.w_list[i])))\n",
    "        return last_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sp_Skip_GCN(Sp_GCN):\n",
    "    def __init__(self,args,activation):\n",
    "        super().__init__(args,activation)\n",
    "        self.W_feat = Parameter(torch.Tensor(args.feats_per_node, args.layer_1_feats))\n",
    "\n",
    "    def forward(self,A_list, Nodes_list = None):\n",
    "        node_feats = Nodes_list[-1]\n",
    "        #A_list: T, each element sparse tensor\n",
    "        #take only last adj matrix in time\n",
    "        Ahat = A_list[-1]\n",
    "        #Ahat: NxN ~ 30k\n",
    "        #sparse multiplication\n",
    "\n",
    "        # Ahat NxN\n",
    "        # self.node_feats = Nxk\n",
    "        #\n",
    "        # note(bwheatman, tfk): change order of matrix multiply\n",
    "        l1 = self.activation(Ahat.matmul(node_feats.matmul(self.W1)))\n",
    "        l2 = self.activation(Ahat.matmul(l1.matmul(self.W2)) + (node_feats.matmul(self.W3)))\n",
    "\n",
    "        return l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sp_Skip_NodeFeats_GCN(Sp_GCN):\n",
    "    def __init__(self,args,activation):\n",
    "        super().__init__(args,activation)\n",
    "\n",
    "    def forward(self,A_list, Nodes_list = None):\n",
    "        node_feats = Nodes_list[-1]\n",
    "        Ahat = A_list[-1]\n",
    "        last_l = self.activation(Ahat.matmul(node_feats.matmul(self.w_list[0])))\n",
    "        for i in range(1, self.num_layers):\n",
    "            last_l = self.activation(Ahat.matmul(last_l.matmul(self.w_list[i])))\n",
    "        skip_last_l = torch.cat((last_l,node_feats), dim=1)   # use node_feats.to_dense() if 2hot encoded input\n",
    "        \n",
    "        return skip_last_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sp_GCN_LSTM_A(Sp_GCN):\n",
    "    def __init__(self,args,activation):\n",
    "        super().__init__(args,activation)\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=args.layer_2_feats,\n",
    "            hidden_size=args.lstm_l2_feats,\n",
    "            num_layers=args.lstm_l2_layers\n",
    "        )\n",
    "\n",
    "    def forward(self,A_list, Nodes_list = None, nodes_mask_list = None):\n",
    "        last_l_seq=[]\n",
    "        for t,Ahat in enumerate(A_list):\n",
    "            node_feats = Nodes_list[t]\n",
    "            #A_list: T, each element sparse tensor\n",
    "            #note(bwheatman, tfk): change order of matrix multiply\n",
    "            last_l = self.activation(Ahat.matmul(node_feats.matmul(self.w_list[0])))\n",
    "            for i in range(1, self.num_layers):\n",
    "                last_l = self.activation(Ahat.matmul(last_l.matmul(self.w_list[i])))\n",
    "            last_l_seq.append(last_l)\n",
    "\n",
    "        last_l_seq = torch.stack(last_l_seq)\n",
    "\n",
    "        out, _ = self.rnn(last_l_seq, None)\n",
    "        return out[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sp_GCN_GRU_A(Sp_GCN_LSTM_A):\n",
    "    def __init__(self,args,activation):\n",
    "        super().__init__(args,activation)\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=args.layer_2_feats,\n",
    "            hidden_size=args.lstm_l2_feats,\n",
    "            num_layers=args.lstm_l2_layers\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sp_GCN_LSTM_B(Sp_GCN):\n",
    "    def __init__(self,args,activation):\n",
    "        super().__init__(args,activation)\n",
    "        assert args.num_layers == 2, 'GCN-LSTM and GCN-GRU requires 2 conv layers.'\n",
    "        self.rnn_l1 = nn.LSTM(\n",
    "            input_size=args.layer_1_feats,\n",
    "            hidden_size=args.lstm_l1_feats,\n",
    "            num_layers=args.lstm_l1_layers\n",
    "        )\n",
    "\n",
    "        self.rnn_l2 = nn.LSTM(\n",
    "            input_size=args.layer_2_feats,\n",
    "            hidden_size=args.lstm_l2_feats,\n",
    "            num_layers=args.lstm_l2_layers\n",
    "        )\n",
    "        self.W2 = Parameter(torch.Tensor(args.lstm_l1_feats, args.layer_2_feats))\n",
    "        reset_param(self.W2)\n",
    "\n",
    "    def forward(self,A_list, Nodes_list = None, nodes_mask_list = None):\n",
    "        l1_seq=[]\n",
    "        l2_seq=[]\n",
    "        for t,Ahat in enumerate(A_list):\n",
    "            node_feats = Nodes_list[t]\n",
    "            l1 = self.activation(Ahat.matmul(node_feats.matmul(self.w_list[0])))\n",
    "            l1_seq.append(l1)\n",
    "\n",
    "        l1_seq = torch.stack(l1_seq)\n",
    "\n",
    "        out_l1, _ = self.rnn_l1(l1_seq, None)\n",
    "\n",
    "        for i in range(len(A_list)):\n",
    "            Ahat = A_list[i]\n",
    "            out_t_l1 = out_l1[i]\n",
    "            #A_list: T, each element sparse tensor\n",
    "            l2 = self.activation(Ahat.matmul(out_t_l1).matmul(self.w_list[1]))\n",
    "            l2_seq.append(l2)\n",
    "\n",
    "        l2_seq = torch.stack(l2_seq)\n",
    "\n",
    "        out, _ = self.rnn_l2(l2_seq, None)\n",
    "        return out[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sp_GCN_GRU_B(Sp_GCN_LSTM_B):\n",
    "    def __init__(self,args,activation):\n",
    "        super().__init__(args,activation)\n",
    "        self.rnn_l1 = nn.GRU(\n",
    "            input_size=args.layer_1_feats,\n",
    "            hidden_size=args.lstm_l1_feats,\n",
    "            num_layers=args.lstm_l1_layers\n",
    "        )\n",
    "\n",
    "        self.rnn_l2 = nn.GRU(\n",
    "            input_size=args.layer_2_feats,\n",
    "            hidden_size=args.lstm_l2_feats,\n",
    "            num_layers=args.lstm_l2_layers\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(torch.nn.Module):\n",
    "    def __init__(self,args,out_features=2, in_features = None):\n",
    "        super(Classifier,self).__init__()\n",
    "        activation = torch.nn.ReLU()\n",
    "\n",
    "        if in_features is not None:\n",
    "            num_feats = in_features\n",
    "        elif args.experiment_type in ['sp_lstm_A_trainer', 'sp_lstm_B_trainer',\n",
    "                                    'sp_weighted_lstm_A', 'sp_weighted_lstm_B'] :\n",
    "            num_feats = args.gcn_parameters['lstm_l2_feats'] * 2\n",
    "        else:\n",
    "            num_feats = args.gcn_parameters['layer_2_feats'] * 2\n",
    "        print ('CLS num_feats',num_feats)\n",
    "\n",
    "        self.mlp = torch.nn.Sequential(\n",
    "            torch.nn.Linear(\n",
    "                in_features = num_feats,\n",
    "                out_features =args.gcn_parameters['cls_feats']\n",
    "            ),\n",
    "            activation,\n",
    "            torch.nn.Linear(\n",
    "                in_features = args.gcn_parameters['cls_feats'],\n",
    "                out_features = out_features\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.mlp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EvolveGCN-O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopK_O(torch.nn.Module):\n",
    "    def __init__(self,feats,k):\n",
    "        super().__init__()\n",
    "        self.scorer = Parameter(torch.Tensor(feats,1))\n",
    "        self.reset_param(self.scorer)\n",
    "        self.k = k\n",
    "\n",
    "    def reset_param(self,t):\n",
    "        # Initialize based on the number of rows\n",
    "        stdv = 1. / math.sqrt(t.size(0))\n",
    "        t.data.uniform_(-stdv,stdv)\n",
    "\n",
    "    def forward(self,node_embs,mask):\n",
    "        scores = node_embs.matmul(self.scorer) / self.scorer.norm()\n",
    "        scores = scores + mask\n",
    "\n",
    "        vals, topk_indices = scores.view(-1).topk(self.k)\n",
    "        topk_indices = topk_indices[vals > -float(\"Inf\")]\n",
    "\n",
    "        if topk_indices.size(0) < self.k:\n",
    "            topk_indices = pad_with_last_val(topk_indices,self.k)\n",
    "            \n",
    "        tanh = torch.nn.Tanh()\n",
    "\n",
    "        if isinstance(node_embs, torch.sparse.FloatTensor) or isinstance(node_embs, torch.cuda.sparse.FloatTensor):\n",
    "            node_embs = node_embs.to_dense()\n",
    "\n",
    "        out = node_embs[topk_indices] * tanh(scores[topk_indices].view(-1, 1))\n",
    "\n",
    "        # we need to transpose the output\n",
    "        return out.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mat_GRU_gate_O(torch.nn.Module):\n",
    "    def __init__(self,rows,cols,activation):\n",
    "        super().__init__()\n",
    "        self.activation = activation\n",
    "        # the k here should be in_feats which is actually the rows\n",
    "        self.W = Parameter(torch.Tensor(rows,rows))\n",
    "        self.reset_param(self.W)\n",
    "\n",
    "        self.U = Parameter(torch.Tensor(rows,rows))\n",
    "        self.reset_param(self.U)\n",
    "\n",
    "        self.bias = Parameter(torch.zeros(rows,cols))\n",
    "\n",
    "    def reset_param(self,t):\n",
    "        # Initialize based on the number of columns\n",
    "        stdv = 1. / math.sqrt(t.size(1))\n",
    "        t.data.uniform_(-stdv,stdv)\n",
    "\n",
    "    def forward(self,x,hidden):\n",
    "        out = self.activation(self.W.matmul(x) + \\\n",
    "                              self.U.matmul(hidden) + \\\n",
    "                              self.bias)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mat_GRU_cell_O(torch.nn.Module):\n",
    "    def __init__(self,args):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.update = mat_GRU_gate_O(args.rows, args.cols, torch.nn.Sigmoid())\n",
    "        self.reset = mat_GRU_gate_O(args.rows, args.cols, torch.nn.Sigmoid())\n",
    "        self.htilda = mat_GRU_gate_O(args.rows, args.cols, torch.nn.Tanh())\n",
    "        self.choose_topk = TopK_O(feats = args.rows, k = args.cols)\n",
    "\n",
    "    def forward(self,prev_Q): # ,prev_Z,mask):\n",
    "        # z_topk = self.choose_topk(prev_Z,mask)\n",
    "        z_topk = prev_Q\n",
    "\n",
    "        update = self.update(z_topk,prev_Q)\n",
    "        reset = self.reset(z_topk,prev_Q)\n",
    "\n",
    "        h_cap = reset * prev_Q\n",
    "        h_cap = self.htilda(z_topk, h_cap)\n",
    "\n",
    "        new_Q = (1 - update) * prev_Q + update * h_cap\n",
    "\n",
    "        return new_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRCU_O(torch.nn.Module):\n",
    "    def __init__(self,args):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        cell_args = Namespace({})\n",
    "        cell_args.rows = args.in_feats\n",
    "        cell_args.cols = args.out_feats\n",
    "\n",
    "        self.evolve_weights = mat_GRU_cell_O(cell_args)\n",
    "\n",
    "        self.activation = self.args.activation\n",
    "        self.GCN_init_weights = Parameter(torch.Tensor(self.args.in_feats,self.args.out_feats))\n",
    "        self.reset_param(self.GCN_init_weights)\n",
    "\n",
    "    def reset_param(self,t):\n",
    "        # Initialize based on the number of columns\n",
    "        stdv = 1. / math.sqrt(t.size(1))\n",
    "        t.data.uniform_(-stdv,stdv)\n",
    "\n",
    "    def forward(self,A_list,node_embs_list):#,mask_list):\n",
    "        GCN_weights = self.GCN_init_weights\n",
    "        out_seq = []\n",
    "        for t,Ahat in enumerate(A_list):\n",
    "            node_embs = node_embs_list[t]\n",
    "            # first evolve the weights from the initial and use the new weights with the node_embs\n",
    "            GCN_weights = self.evolve_weights(GCN_weights)#,node_embs,mask_list[t])\n",
    "            node_embs = self.activation(Ahat.matmul(node_embs.matmul(GCN_weights)))\n",
    "\n",
    "            out_seq.append(node_embs)\n",
    "\n",
    "        return out_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EGCN_O(torch.nn.Module):\n",
    "    def __init__(self, args, activation, device='cpu', skipfeats=False):\n",
    "        super().__init__()\n",
    "        GRCU_args = Namespace({})\n",
    "\n",
    "        feats = [args.feats_per_node, args.layer_1_feats, args.layer_2_feats]\n",
    "        self.device = device\n",
    "        self.skipfeats = skipfeats\n",
    "        self.GRCU_layers = []\n",
    "        self._parameters = nn.ParameterList()\n",
    "        for i in range(1,len(feats)):\n",
    "            GRCU_args = Namespace({\n",
    "                'in_feats' : feats[i-1],\n",
    "                'out_feats': feats[i],\n",
    "                'activation': activation\n",
    "            })\n",
    "\n",
    "            grcu_i = GRCU_O(GRCU_args)\n",
    "            # print (i,'grcu_i', grcu_i)\n",
    "            self.GRCU_layers.append(grcu_i.to(self.device))\n",
    "            self._parameters.extend(list(self.GRCU_layers[-1].parameters()))\n",
    "\n",
    "    def parameters(self):\n",
    "        return self._parameters\n",
    "\n",
    "    def forward(self,A_list, Nodes_list,nodes_mask_list):\n",
    "        node_feats= Nodes_list[-1]\n",
    "\n",
    "        for unit in self.GRCU_layers:\n",
    "            Nodes_list = unit(A_list,Nodes_list)  # ,nodes_mask_list)\n",
    "\n",
    "        out = Nodes_list[-1]\n",
    "        if self.skipfeats:\n",
    "            out = torch.cat((out,node_feats), dim=1)   # use node_feats.to_dense() if 2hot encoded input \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EvolveGCN-H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopK_H(torch.nn.Module):\n",
    "    def __init__(self,feats,k):\n",
    "        super().__init__()\n",
    "        self.scorer = Parameter(torch.Tensor(feats,1))\n",
    "        self.reset_param(self.scorer)\n",
    "        \n",
    "        self.k = k\n",
    "\n",
    "    def reset_param(self,t):\n",
    "        # Initialize based on the number of rows\n",
    "        stdv = 1. / math.sqrt(t.size(0))\n",
    "        t.data.uniform_(-stdv,stdv)\n",
    "\n",
    "    def forward(self,node_embs,mask):\n",
    "        scores = node_embs.matmul(self.scorer) / self.scorer.norm()\n",
    "        scores = scores + mask\n",
    "\n",
    "        vals, topk_indices = scores.view(-1).topk(self.k)\n",
    "        topk_indices = topk_indices[vals > -float(\"Inf\")]\n",
    "\n",
    "        if topk_indices.size(0) < self.k:\n",
    "            topk_indices = pad_with_last_val(topk_indices,self.k)\n",
    "            \n",
    "        tanh = torch.nn.Tanh()\n",
    "\n",
    "        if isinstance(node_embs, torch.sparse.FloatTensor) or \\\n",
    "           isinstance(node_embs, torch.cuda.sparse.FloatTensor):\n",
    "            node_embs = node_embs.to_dense()\n",
    "\n",
    "        out = node_embs[topk_indices] * tanh(scores[topk_indices].view(-1,1))\n",
    "\n",
    "        # we need to transpose the output\n",
    "        return out.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mat_GRU_gate_H(torch.nn.Module):\n",
    "    def __init__(self,rows,cols,activation):\n",
    "        super().__init__()\n",
    "        self.activation = activation\n",
    "        #the k here should be in_feats which is actually the rows\n",
    "        self.W = Parameter(torch.Tensor(rows,rows))\n",
    "        self.reset_param(self.W)\n",
    "\n",
    "        self.U = Parameter(torch.Tensor(rows,rows))\n",
    "        self.reset_param(self.U)\n",
    "\n",
    "        self.bias = Parameter(torch.zeros(rows,cols))\n",
    "\n",
    "    def reset_param(self,t):\n",
    "        # Initialize based on the number of columns\n",
    "        stdv = 1. / math.sqrt(t.size(1))\n",
    "        t.data.uniform_(-stdv,stdv)\n",
    "\n",
    "    def forward(self,x,hidden):\n",
    "        out = self.activation(self.W.matmul(x) + \\\n",
    "                              self.U.matmul(hidden) + \\\n",
    "                              self.bias)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mat_GRU_cell_H(torch.nn.Module):\n",
    "    def __init__(self,args):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.update = mat_GRU_gate_H(args.rows, args.cols, torch.nn.Sigmoid())\n",
    "        self.reset = mat_GRU_gate_H(args.rows, args.cols, torch.nn.Sigmoid())\n",
    "        self.htilda = mat_GRU_gate_H(args.rows, args.cols, torch.nn.Tanh())\n",
    "        self.choose_topk = TopK_H(feats = args.rows, k = args.cols)\n",
    "\n",
    "    def forward(self,prev_Q,prev_Z,mask):\n",
    "        z_topk = self.choose_topk(prev_Z,mask)\n",
    "\n",
    "        update = self.update(z_topk,prev_Q)\n",
    "        reset = self.reset(z_topk,prev_Q)\n",
    "\n",
    "        h_cap = reset * prev_Q\n",
    "        h_cap = self.htilda(z_topk, h_cap)\n",
    "\n",
    "        new_Q = (1 - update) * prev_Q + update * h_cap\n",
    "\n",
    "        return new_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRCU_H(torch.nn.Module):\n",
    "    def __init__(self,args):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        cell_args = Namespace({})\n",
    "        cell_args.rows = args.in_feats\n",
    "        cell_args.cols = args.out_feats\n",
    "\n",
    "        self.evolve_weights = mat_GRU_cell_H(cell_args)\n",
    "\n",
    "        self.activation = self.args.activation\n",
    "        self.GCN_init_weights = Parameter(torch.Tensor(self.args.in_feats,self.args.out_feats))\n",
    "        self.reset_param(self.GCN_init_weights)\n",
    "\n",
    "    def reset_param(self,t):\n",
    "        # Initialize based on the number of columns\n",
    "        stdv = 1. / math.sqrt(t.size(1))\n",
    "        t.data.uniform_(-stdv,stdv)\n",
    "\n",
    "    def forward(self,A_list,node_embs_list,mask_list):\n",
    "        GCN_weights = self.GCN_init_weights\n",
    "        out_seq = []\n",
    "        for t,Ahat in enumerate(A_list):\n",
    "            node_embs = node_embs_list[t]\n",
    "            #first evolve the weights from the initial and use the new weights with the node_embs\n",
    "            GCN_weights = self.evolve_weights(GCN_weights,node_embs,mask_list[t])\n",
    "            node_embs = self.activation(Ahat.matmul(node_embs.matmul(GCN_weights)))\n",
    "\n",
    "            out_seq.append(node_embs)\n",
    "\n",
    "        return out_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EGCN_H(torch.nn.Module):\n",
    "    def __init__(self, args, activation, device='cpu', skipfeats=False):\n",
    "        super().__init__()\n",
    "        GRCU_args = Namespace({})\n",
    "\n",
    "        feats = [args.feats_per_node,\n",
    "                 args.layer_1_feats,\n",
    "                 args.layer_2_feats]\n",
    "        self.device = device\n",
    "        self.skipfeats = skipfeats\n",
    "        self.GRCU_layers = []\n",
    "        self._parameters = nn.ParameterList()\n",
    "        for i in range(1,len(feats)):\n",
    "            GRCU_args = Namespace({\n",
    "                'in_feats' : feats[i-1],\n",
    "                'out_feats': feats[i],\n",
    "                'activation': activation\n",
    "            })\n",
    "\n",
    "            grcu_i = GRCU_H(GRCU_args)\n",
    "            #print (i,'grcu_i', grcu_i)\n",
    "            self.GRCU_layers.append(grcu_i.to(self.device))\n",
    "            self._parameters.extend(list(self.GRCU_layers[-1].parameters()))\n",
    "\n",
    "    def parameters(self):\n",
    "        return self._parameters\n",
    "\n",
    "    def forward(self,A_list, Nodes_list,nodes_mask_list):\n",
    "        node_feats= Nodes_list[-1]\n",
    "\n",
    "        for unit in self.GRCU_layers:\n",
    "            Nodes_list = unit(A_list,Nodes_list,nodes_mask_list)\n",
    "\n",
    "        out = Nodes_list[-1]\n",
    "        if self.skipfeats:\n",
    "            out = torch.cat((out,node_feats), dim=1)   # use node_feats.to_dense() if 2hot encoded input \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class splitter():\n",
    "    '''\n",
    "    creates 3 splits\n",
    "    train\n",
    "    dev\n",
    "    test\n",
    "    '''\n",
    "    def __init__(self,args,tasker):\n",
    "        if tasker.is_static: #### For static datsets\n",
    "            assert args.train_proportion + args.dev_proportion < 1, \\\n",
    "                'there\\'s no space for test samples'\n",
    "            # only the training one requires special handling on start, the others are fine with the split IDX.\n",
    "            \n",
    "            random_perm=False\n",
    "            indexes = tasker.data.nodes_with_label\n",
    "            \n",
    "            if random_perm:\n",
    "                perm_idx = torch.randperm(indexes.size(0))\n",
    "                perm_idx = indexes[perm_idx]\n",
    "            else:\n",
    "                print ('tasker.data.nodes',indexes.size())\n",
    "                perm_idx, _ = indexes.sort()\n",
    "            # print ('perm_idx',perm_idx[:10])\n",
    "            \n",
    "            self.train_idx = perm_idx[:int(args.train_proportion*perm_idx.size(0))]\n",
    "            self.dev_idx = perm_idx[int(args.train_proportion*perm_idx.size(0)): int((args.train_proportion+args.dev_proportion)*perm_idx.size(0))]\n",
    "            self.test_idx = perm_idx[int((args.train_proportion+args.dev_proportion)*perm_idx.size(0)):]\n",
    "            # print ('train,dev,test',self.train_idx.size(), self.dev_idx.size(), self.test_idx.size())\n",
    "            \n",
    "            train = static_data_split(tasker, self.train_idx, test = False)\n",
    "            train = DataLoader(train, shuffle=True,**args.data_loading_params)\n",
    "            \n",
    "            dev = static_data_split(tasker, self.dev_idx, test = True)\n",
    "            dev = DataLoader(dev, shuffle=False,**args.data_loading_params)\n",
    "            \n",
    "            test = static_data_split(tasker, self.test_idx, test = True)\n",
    "            test = DataLoader(test, shuffle=False,**args.data_loading_params)\n",
    "                        \n",
    "            self.tasker = tasker\n",
    "            self.train = train\n",
    "            self.dev = dev\n",
    "            self.test = test\n",
    "            \n",
    "            \n",
    "        else: #### For datsets with time\n",
    "            assert args.train_proportion + args.dev_proportion < 1, \\\n",
    "                'there\\'s no space for test samples'\n",
    "            # only the training one requires special handling on start, the others are fine with the split IDX.\n",
    "            start = tasker.data.min_time + args.num_hist_steps #-1 + args.adj_mat_time_window\n",
    "            end = args.train_proportion\n",
    "            \n",
    "            end = int(np.floor(tasker.data.max_time.type(torch.float) * end))\n",
    "            train = data_split(tasker, start, end, test = False)\n",
    "            train = DataLoader(train,**args.data_loading_params)\n",
    "    \n",
    "            start = end\n",
    "            end = args.dev_proportion + args.train_proportion\n",
    "            end = int(np.floor(tasker.data.max_time.type(torch.float) * end))\n",
    "            if args.task == 'link_pred':\n",
    "                dev = data_split(tasker, start, end, test = True, all_edges=True)\n",
    "            else:\n",
    "                dev = data_split(tasker, start, end, test = True)\n",
    "\n",
    "            dev = DataLoader(dev,num_workers=args.data_loading_params['num_workers'])\n",
    "            \n",
    "            start = end\n",
    "            \n",
    "            # the +1 is because I assume that max_time exists in the dataset\n",
    "            end = int(tasker.max_time) + 1\n",
    "            if args.task == 'link_pred':\n",
    "                test = data_split(tasker, start, end, test = True, all_edges=True)\n",
    "            else:\n",
    "                test = data_split(tasker, start, end, test = True)\n",
    "                \n",
    "            test = DataLoader(test,num_workers=args.data_loading_params['num_workers'])\n",
    "            \n",
    "            print ('Dataset splits sizes:  train',len(train), 'dev',len(dev), 'test',len(test))\n",
    "            \n",
    "            self.tasker = tasker\n",
    "            self.train = train\n",
    "            self.dev = dev\n",
    "            self.test = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_split(Dataset):\n",
    "    def __init__(self, tasker, start, end, test, **kwargs):\n",
    "        '''start and end are indices indicating what items belong to this split'''\n",
    "        self.tasker = tasker\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.test = test\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.end-self.start\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        idx = self.start + idx\n",
    "        t = self.tasker.get_sample(idx, test = self.test, **self.kwargs)\n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class static_data_split(Dataset):\n",
    "    def __init__(self, tasker, indexes, test):\n",
    "        '''\n",
    "        start and end are indices indicating what items belong to this split\n",
    "        '''\n",
    "        self.tasker = tasker\n",
    "        self.indexes = indexes\n",
    "        self.test = test\n",
    "        self.adj_matrix = tasker.adj_matrix\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indexes)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        idx = self.indexes[idx]\n",
    "        return self.tasker.get_sample(idx,test = self.test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross_Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cross_Entropy(torch.nn.Module):\n",
    "    \"\"\"docstring for Cross_Entropy\"\"\"\n",
    "    def __init__(self, args, dataset):\n",
    "        super().__init__()\n",
    "        weights = torch.tensor(args.class_weights).to(args.device)\n",
    "\n",
    "        self.weights = self.dyn_scale(args.task, dataset, weights)\n",
    "        \n",
    "    \n",
    "    def dyn_scale(self,task,dataset,weights):\n",
    "        # if task == 'link_pred':  commented to have a 1:1 ratio\n",
    "\n",
    "        #     '''\n",
    "        #     when doing link prediction there is an extra weighting factor on the non-existing\n",
    "        #     edges\n",
    "        #     '''\n",
    "        #     tot_neg = dataset.num_non_existing\n",
    "        #     def scale(labels):\n",
    "        #         cur_neg = (labels == 0).sum(dtype = torch.float)\n",
    "        #         out = weights.clone()\n",
    "        #         out[0] *= tot_neg/cur_neg\n",
    "        #         return out\n",
    "        # else:\n",
    "        #     def scale(labels):\n",
    "        #         return weights\n",
    "        def scale(labels):\n",
    "            return weights\n",
    "        return scale\n",
    "    \n",
    "\n",
    "    def logsumexp(self,logits):\n",
    "        m,_ = torch.max(logits,dim=1)\n",
    "        m = m.view(-1,1)\n",
    "        sum_exp = torch.sum(torch.exp(logits-m),dim=1, keepdim=True)\n",
    "        return m + torch.log(sum_exp)\n",
    "    \n",
    "    def forward(self,logits,labels):\n",
    "        '''\n",
    "        logits is a matrix M by C where m is the number of classifications and C are the number of classes\n",
    "        labels is a integer tensor of size M where each element corresponds to the class that prediction i\n",
    "        should be matching to\n",
    "        '''\n",
    "        labels = labels.view(-1,1)\n",
    "        alpha = self.weights(labels)[labels].view(-1,1)\n",
    "        loss = alpha * (- logits.gather(-1,labels) + self.logsumexp(logits))\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger():\n",
    "    def __init__(self, args, num_classes, minibatch_log_interval=10):\n",
    "\n",
    "        if args is not None:\n",
    "            currdate=str(datetime.datetime.today().strftime('%Y%m%d%H%M%S'))\n",
    "            self.log_name= 'log/log_'+args.data+'_'+args.task+'_'+args.model+'_'+currdate+'_r'+str(args.rank)+'.log'\n",
    "\n",
    "            if args.use_logfile:\n",
    "                print (\"Log file:\", self.log_name)\n",
    "                logging.basicConfig(filename=self.log_name, level=logging.INFO)\n",
    "            else:\n",
    "                print (\"Log: STDOUT\")\n",
    "                logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "            logging.info ('*** PARAMETERS ***')\n",
    "            logging.info (pprint.pformat(args.__dict__)) # displays the string\n",
    "            logging.info ('')\n",
    "        else:\n",
    "            print (\"Log: STDOUT\")\n",
    "            logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.minibatch_log_interval = minibatch_log_interval\n",
    "        self.eval_k_list = [10, 100, 1000]\n",
    "        self.args = args\n",
    "\n",
    "\n",
    "    def get_log_file_name(self):\n",
    "        return self.log_name\n",
    "\n",
    "    def log_epoch_start(self, epoch, num_minibatches, set, minibatch_log_interval=None):\n",
    "        #ALDO\n",
    "        self.epoch = epoch\n",
    "        ######\n",
    "        self.set = set\n",
    "        self.losses = []\n",
    "        self.errors = []\n",
    "        self.MRRs = []\n",
    "        self.MAPs = []\n",
    "        #self.time_step_sizes = []\n",
    "        self.conf_mat_tp = {}\n",
    "        self.conf_mat_fn = {}\n",
    "        self.conf_mat_fp = {}\n",
    "        self.conf_mat_tp_at_k = {}\n",
    "        self.conf_mat_fn_at_k = {}\n",
    "        self.conf_mat_fp_at_k = {}\n",
    "        for k in self.eval_k_list:\n",
    "            self.conf_mat_tp_at_k[k] = {}\n",
    "            self.conf_mat_fn_at_k[k] = {}\n",
    "            self.conf_mat_fp_at_k[k] = {}\n",
    "\n",
    "        for cl in range(self.num_classes):\n",
    "            self.conf_mat_tp[cl]=0\n",
    "            self.conf_mat_fn[cl]=0\n",
    "            self.conf_mat_fp[cl]=0\n",
    "            for k in self.eval_k_list:\n",
    "                self.conf_mat_tp_at_k[k][cl]=0\n",
    "                self.conf_mat_fn_at_k[k][cl]=0\n",
    "                self.conf_mat_fp_at_k[k][cl]=0\n",
    "\n",
    "        if self.set == \"TEST\":\n",
    "            self.conf_mat_tp_list = {}\n",
    "            self.conf_mat_fn_list = {}\n",
    "            self.conf_mat_fp_list = {}\n",
    "            for cl in range(self.num_classes):\n",
    "                self.conf_mat_tp_list[cl]=[]\n",
    "                self.conf_mat_fn_list[cl]=[]\n",
    "                self.conf_mat_fp_list[cl]=[]\n",
    "\n",
    "        self.batch_sizes=[]\n",
    "        self.minibatch_done = 0\n",
    "        self.num_minibatches = num_minibatches\n",
    "        if minibatch_log_interval is not None:\n",
    "            self.minibatch_log_interval = minibatch_log_interval\n",
    "        logging.info('################ '+set+' epoch '+str(epoch)+' ###################')\n",
    "        self.lasttime = time.monotonic()\n",
    "        self.ep_time = self.lasttime\n",
    "\n",
    "    def log_minibatch(self, predictions, true_classes, loss, **kwargs):\n",
    "\n",
    "        probs = torch.softmax(predictions,dim=1)[:,1]\n",
    "        if self.set in ['TEST', 'VALID'] and self.args.task == 'link_pred':\n",
    "            MRR = self.get_MRR(probs,true_classes, kwargs['adj'],do_softmax=False)\n",
    "        else:\n",
    "            MRR = torch.tensor([0.0])\n",
    "\n",
    "        MAP = torch.tensor(self.get_MAP(probs,true_classes, do_softmax=False))\n",
    "\n",
    "        error, conf_mat_per_class = self.eval_predicitions(predictions, true_classes, self.num_classes)\n",
    "        conf_mat_per_class_at_k={}\n",
    "        for k in self.eval_k_list:\n",
    "            conf_mat_per_class_at_k[k] = self.eval_predicitions_at_k(predictions, true_classes, self.num_classes, k)\n",
    "\n",
    "        batch_size = predictions.size(0)\n",
    "        self.batch_sizes.append(batch_size)\n",
    "\n",
    "        self.losses.append(loss) #loss.detach()\n",
    "        self.errors.append(error)\n",
    "        self.MRRs.append(MRR)\n",
    "        self.MAPs.append(MAP)\n",
    "        for cl in range(self.num_classes):\n",
    "            self.conf_mat_tp[cl]+=conf_mat_per_class.true_positives[cl]\n",
    "            self.conf_mat_fn[cl]+=conf_mat_per_class.false_negatives[cl]\n",
    "            self.conf_mat_fp[cl]+=conf_mat_per_class.false_positives[cl]\n",
    "            for k in self.eval_k_list:\n",
    "                self.conf_mat_tp_at_k[k][cl]+=conf_mat_per_class_at_k[k].true_positives[cl]\n",
    "                self.conf_mat_fn_at_k[k][cl]+=conf_mat_per_class_at_k[k].false_negatives[cl]\n",
    "                self.conf_mat_fp_at_k[k][cl]+=conf_mat_per_class_at_k[k].false_positives[cl]\n",
    "            if self.set == \"TEST\":\n",
    "                self.conf_mat_tp_list[cl].append(conf_mat_per_class.true_positives[cl])\n",
    "                self.conf_mat_fn_list[cl].append(conf_mat_per_class.false_negatives[cl])\n",
    "                self.conf_mat_fp_list[cl].append(conf_mat_per_class.false_positives[cl])\n",
    "\n",
    "        self.minibatch_done+=1\n",
    "        if self.minibatch_done%self.minibatch_log_interval==0:\n",
    "            mb_error = self.calc_epoch_metric(self.batch_sizes, self.errors)\n",
    "            mb_MRR = self.calc_epoch_metric(self.batch_sizes, self.MRRs)\n",
    "            mb_MAP = self.calc_epoch_metric(self.batch_sizes, self.MAPs)\n",
    "            partial_losses = torch.stack(self.losses)\n",
    "            logging.info(self.set+ ' batch %d / %d - partial error %0.4f - partial loss %0.4f - partial MRR  %0.4f - partial MAP %0.4f' % (self.minibatch_done, self.num_minibatches, mb_error, partial_losses.mean(), mb_MRR, mb_MAP))\n",
    "\n",
    "            tp=conf_mat_per_class.true_positives\n",
    "            fn=conf_mat_per_class.false_negatives\n",
    "            fp=conf_mat_per_class.false_positives\n",
    "            logging.info(self.set+' batch %d / %d -  partial tp %s,fn %s,fp %s' % (self.minibatch_done, self.num_minibatches, tp, fn, fp))\n",
    "            precision, recall, f1 = self.calc_microavg_eval_measures(tp, fn, fp)\n",
    "            logging.info (self.set+' batch %d / %d - measures partial microavg - precision %0.4f - recall %0.4f - f1 %0.4f ' % (self.minibatch_done, self.num_minibatches, precision,recall,f1))\n",
    "            for cl in range(self.num_classes):\n",
    "                cl_precision, cl_recall, cl_f1 = self.calc_eval_measures_per_class(tp, fn, fp, cl)\n",
    "                logging.info (self.set+' batch %d / %d - measures partial for class %d - precision %0.4f - recall %0.4f - f1 %0.4f ' % (self.minibatch_done, self.num_minibatches, cl,cl_precision,cl_recall,cl_f1))\n",
    "\n",
    "            logging.info (self.set+' batch %d / %d - Batch time %d ' % (self.minibatch_done, self.num_minibatches, (time.monotonic()-self.lasttime) ))\n",
    "\n",
    "        self.lasttime=time.monotonic()\n",
    "\n",
    "    def log_epoch_done(self):\n",
    "        eval_measure = 0\n",
    "\n",
    "        self.losses = torch.stack(self.losses)\n",
    "        logging.info(self.set+' mean losses '+ str(self.losses.mean()))\n",
    "        if self.args.target_measure=='loss' or self.args.target_measure=='Loss':\n",
    "            eval_measure = self.losses.mean()\n",
    "\n",
    "        epoch_error = self.calc_epoch_metric(self.batch_sizes, self.errors)\n",
    "        logging.info(self.set+' mean errors '+ str(epoch_error))\n",
    "\n",
    "        epoch_MRR = self.calc_epoch_metric(self.batch_sizes, self.MRRs)\n",
    "        epoch_MAP = self.calc_epoch_metric(self.batch_sizes, self.MAPs)\n",
    "        logging.info(self.set+' mean MRR '+ str(epoch_MRR)+' - mean MAP '+ str(epoch_MAP))\n",
    "        if self.args.target_measure=='MRR' or self.args.target_measure=='mrr':\n",
    "            eval_measure = epoch_MRR\n",
    "        if self.args.target_measure=='MAP' or self.args.target_measure=='map':\n",
    "            eval_measure = epoch_MAP\n",
    "\n",
    "        logging.info(self.set+' tp %s,fn %s,fp %s' % (self.conf_mat_tp, self.conf_mat_fn, self.conf_mat_fp))\n",
    "        precision, recall, f1 = self.calc_microavg_eval_measures(self.conf_mat_tp, self.conf_mat_fn, self.conf_mat_fp)\n",
    "        logging.info (self.set+' measures microavg - precision %0.4f - recall %0.4f - f1 %0.4f ' % (precision,recall,f1))\n",
    "        if str(self.args.target_class) == 'AVG':\n",
    "            if self.args.target_measure=='Precision' or self.args.target_measure=='prec':\n",
    "                eval_measure = precision\n",
    "            elif self.args.target_measure=='Recall' or self.args.target_measure=='rec':\n",
    "                eval_measure = recall\n",
    "            else:\n",
    "                eval_measure = f1\n",
    "\n",
    "\n",
    "        for cl in range(self.num_classes):\n",
    "            cl_precision, cl_recall, cl_f1 = self.calc_eval_measures_per_class(self.conf_mat_tp, self.conf_mat_fn, self.conf_mat_fp, cl)\n",
    "            logging.info (self.set+' measures for class %d - precision %0.4f - recall %0.4f - f1 %0.4f ' % (cl,cl_precision,cl_recall,cl_f1))\n",
    "            if str(cl) == str(self.args.target_class):\n",
    "                if self.args.target_measure=='Precision' or self.args.target_measure=='prec':\n",
    "                    eval_measure = cl_precision\n",
    "                elif self.args.target_measure=='Recall' or self.args.target_measure=='rec':\n",
    "                    eval_measure = cl_recall\n",
    "                else:\n",
    "                    eval_measure = cl_f1\n",
    "\n",
    "        for k in self.eval_k_list: #logging.info(self.set+' @%d tp %s,fn %s,fp %s' % (k, self.conf_mat_tp_at_k[k], self.conf_mat_fn_at_k[k], self.conf_mat_fp_at_k[k]))\n",
    "            precision, recall, f1 = self.calc_microavg_eval_measures(self.conf_mat_tp_at_k[k], self.conf_mat_fn_at_k[k], self.conf_mat_fp_at_k[k])\n",
    "            logging.info (self.set+' measures@%d microavg - precision %0.4f - recall %0.4f - f1 %0.4f ' % (k,precision,recall,f1))\n",
    "\n",
    "            for cl in range(self.num_classes):\n",
    "                cl_precision, cl_recall, cl_f1 = self.calc_eval_measures_per_class(self.conf_mat_tp_at_k[k], self.conf_mat_fn_at_k[k], self.conf_mat_fp_at_k[k], cl)\n",
    "                logging.info (self.set+' measures@%d for class %d - precision %0.4f - recall %0.4f - f1 %0.4f ' % (k, cl,cl_precision,cl_recall,cl_f1))\n",
    "\n",
    "\n",
    "        logging.info (self.set+' Total epoch time: '+ str(((time.monotonic()-self.ep_time))))\n",
    "\n",
    "        return eval_measure\n",
    "\n",
    "    def get_MRR(self,predictions,true_classes, adj ,do_softmax=False):\n",
    "        if do_softmax:\n",
    "            probs = torch.softmax(predictions,dim=1)[:,1]\n",
    "        else:\n",
    "            probs = predictions\n",
    "\n",
    "        probs = probs.cpu().numpy()\n",
    "        true_classes = true_classes.cpu().numpy()\n",
    "        adj = adj.cpu().numpy()\n",
    "\n",
    "        pred_matrix = coo_matrix((probs,(adj[0],adj[1]))).toarray()\n",
    "        true_matrix = coo_matrix((true_classes,(adj[0],adj[1]))).toarray()\n",
    "\n",
    "        row_MRRs = []\n",
    "        for i,pred_row in enumerate(pred_matrix):\n",
    "            #check if there are any existing edges\n",
    "            if np.isin(1,true_matrix[i]):\n",
    "                row_MRRs.append(self.get_row_MRR(pred_row,true_matrix[i]))\n",
    "\n",
    "        avg_MRR = torch.tensor(row_MRRs).mean()\n",
    "        return avg_MRR\n",
    "\n",
    "    def get_row_MRR(self,probs,true_classes):\n",
    "        existing_mask = true_classes == 1\n",
    "        #descending in probability\n",
    "        ordered_indices = np.flip(probs.argsort())\n",
    "\n",
    "        ordered_existing_mask = existing_mask[ordered_indices]\n",
    "\n",
    "        existing_ranks = np.arange(1,\n",
    "                                   true_classes.shape[0]+1,\n",
    "                                   dtype=np.float)[ordered_existing_mask]\n",
    "\n",
    "        MRR = (1/existing_ranks).sum()/existing_ranks.shape[0]\n",
    "        return MRR\n",
    "\n",
    "\n",
    "    def get_MAP(self,predictions,true_classes, do_softmax=False):\n",
    "        if do_softmax:\n",
    "            probs = torch.softmax(predictions,dim=1)[:,1]\n",
    "        else:\n",
    "            probs = predictions\n",
    "\n",
    "        predictions_np = probs.detach().cpu().numpy()\n",
    "        true_classes_np = true_classes.detach().cpu().numpy()\n",
    "\n",
    "        return average_precision_score(true_classes_np, predictions_np)\n",
    "\n",
    "    def eval_predicitions(self, predictions, true_classes, num_classes):\n",
    "        predicted_classes = predictions.argmax(dim=1)\n",
    "        failures = (predicted_classes!=true_classes).sum(dtype=torch.float)\n",
    "        error = failures/predictions.size(0)\n",
    "\n",
    "        conf_mat_per_class = Namespace({})\n",
    "        conf_mat_per_class.true_positives = {}\n",
    "        conf_mat_per_class.false_negatives = {}\n",
    "        conf_mat_per_class.false_positives = {}\n",
    "\n",
    "        for cl in range(num_classes):\n",
    "            cl_indices = true_classes == cl\n",
    "\n",
    "            pos = predicted_classes == cl\n",
    "            hits = (predicted_classes[cl_indices] == true_classes[cl_indices])\n",
    "\n",
    "            tp = hits.sum()\n",
    "            fn = hits.size(0) - tp\n",
    "            fp = pos.sum() - tp\n",
    "\n",
    "            conf_mat_per_class.true_positives[cl] = tp\n",
    "            conf_mat_per_class.false_negatives[cl] = fn\n",
    "            conf_mat_per_class.false_positives[cl] = fp\n",
    "        return error, conf_mat_per_class\n",
    "\n",
    "\n",
    "    def eval_predicitions_at_k(self, predictions, true_classes, num_classes, k):\n",
    "        conf_mat_per_class = Namespace({})\n",
    "        conf_mat_per_class.true_positives = {}\n",
    "        conf_mat_per_class.false_negatives = {}\n",
    "        conf_mat_per_class.false_positives = {}\n",
    "\n",
    "        if predictions.size(0)<k:\n",
    "            k=predictions.size(0)\n",
    "\n",
    "        for cl in range(num_classes):\n",
    "            # sort for prediction with higher score for target class (cl)\n",
    "            _, idx_preds_at_k = torch.topk(predictions[:,cl], k, dim=0, largest=True, sorted=True)\n",
    "            predictions_at_k = predictions[idx_preds_at_k]\n",
    "            predicted_classes = predictions_at_k.argmax(dim=1)\n",
    "\n",
    "            cl_indices_at_k = true_classes[idx_preds_at_k] == cl\n",
    "            cl_indices = true_classes == cl\n",
    "\n",
    "            pos = predicted_classes == cl\n",
    "            hits = (predicted_classes[cl_indices_at_k] == true_classes[idx_preds_at_k][cl_indices_at_k])\n",
    "\n",
    "            tp = hits.sum()\n",
    "            fn = true_classes[cl_indices].size(0) - tp # This only if we want to consider the size at K -> hits.size(0) - tp\n",
    "            fp = pos.sum() - tp\n",
    "\n",
    "            conf_mat_per_class.true_positives[cl] = tp\n",
    "            conf_mat_per_class.false_negatives[cl] = fn\n",
    "            conf_mat_per_class.false_positives[cl] = fp\n",
    "        return conf_mat_per_class\n",
    "\n",
    "\n",
    "    def calc_microavg_eval_measures(self, tp, fn, fp):\n",
    "        tp_sum = sum(tp.values()).item()\n",
    "        fn_sum = sum(fn.values()).item()\n",
    "        fp_sum = sum(fp.values()).item()\n",
    "\n",
    "        p = tp_sum*1.0 / (tp_sum+fp_sum)\n",
    "        r = tp_sum*1.0 / (tp_sum+fn_sum)\n",
    "        if (p+r)>0:\n",
    "            f1 = 2.0 * (p*r) / (p+r)\n",
    "        else:\n",
    "            f1 = 0\n",
    "        return p, r, f1\n",
    "\n",
    "    def calc_eval_measures_per_class(self, tp, fn, fp, class_id):\n",
    "        #ALDO\n",
    "        if type(tp) is dict:\n",
    "            tp_sum = tp[class_id].item()\n",
    "            fn_sum = fn[class_id].item()\n",
    "            fp_sum = fp[class_id].item()\n",
    "        else:\n",
    "            tp_sum = tp.item()\n",
    "            fn_sum = fn.item()\n",
    "            fp_sum = fp.item()\n",
    "        ########\n",
    "        if tp_sum==0:\n",
    "            return 0,0,0\n",
    "\n",
    "        p = tp_sum*1.0 / (tp_sum+fp_sum)\n",
    "        r = tp_sum*1.0 / (tp_sum+fn_sum)\n",
    "        if (p+r)>0:\n",
    "            f1 = 2.0 * (p*r) / (p+r)\n",
    "        else:\n",
    "            f1 = 0\n",
    "        return p, r, f1\n",
    "\n",
    "    def calc_epoch_metric(self,batch_sizes, metric_val):\n",
    "        batch_sizes = torch.tensor(batch_sizes, dtype = torch.float)\n",
    "        epoch_metric_val = torch.stack(metric_val).cpu() * batch_sizes\n",
    "        epoch_metric_val = epoch_metric_val.sum()/batch_sizes.sum()\n",
    "\n",
    "        return epoch_metric_val.detach().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self,args, splitter, gcn, classifier, comp_loss, dataset, num_classes):\n",
    "        self.args = args\n",
    "        self.splitter = splitter\n",
    "        self.tasker = splitter.tasker\n",
    "        self.gcn = gcn\n",
    "        self.classifier = classifier\n",
    "        self.comp_loss = comp_loss\n",
    "\n",
    "        self.num_nodes = dataset.num_nodes\n",
    "        self.data = dataset\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.logger = Logger(args, self.num_classes)\n",
    "\n",
    "        self.init_optimizers(args)\n",
    "\n",
    "        if self.tasker.is_static:\n",
    "            adj_matrix = sparse_prepare_tensor(self.tasker.adj_matrix, torch_size = [self.num_nodes], ignore_batch_dim = False)\n",
    "            self.hist_adj_list = [adj_matrix]\n",
    "            self.hist_ndFeats_list = [self.tasker.nodes_feats.float()]\n",
    "\n",
    "    def init_optimizers(self,args):\n",
    "        params = self.gcn.parameters()\n",
    "        self.gcn_opt = torch.optim.Adam(params, lr = args.learning_rate)\n",
    "        params = self.classifier.parameters()\n",
    "        self.classifier_opt = torch.optim.Adam(params, lr = args.learning_rate)\n",
    "        self.gcn_opt.zero_grad()\n",
    "        self.classifier_opt.zero_grad()\n",
    "\n",
    "    def save_checkpoint(self, state, filename='checkpoint.pth.tar'):\n",
    "        torch.save(state, filename)\n",
    "\n",
    "    def load_checkpoint(self, filename, model):\n",
    "        if os.path.isfile(filename):\n",
    "            print(\"=> loading checkpoint '{}'\".format(filename))\n",
    "            checkpoint = torch.load(filename)\n",
    "            epoch = checkpoint['epoch']\n",
    "            self.gcn.load_state_dict(checkpoint['gcn_dict'])\n",
    "            self.classifier.load_state_dict(checkpoint['classifier_dict'])\n",
    "            self.gcn_opt.load_state_dict(checkpoint['gcn_optimizer'])\n",
    "            self.classifier_opt.load_state_dict(checkpoint['classifier_optimizer'])\n",
    "            self.logger.log_str(\"=> loaded checkpoint '{}' (epoch {})\".format(filename, checkpoint['epoch']))\n",
    "            return epoch\n",
    "        \n",
    "        else:\n",
    "            self.logger.log_str(\"=> no checkpoint found at '{}'\".format(filename))\n",
    "            return 0\n",
    "\n",
    "    def train(self):\n",
    "        self.tr_step = 0\n",
    "        best_eval_valid = 0\n",
    "        eval_valid = 0\n",
    "        epochs_without_impr = 0\n",
    "\n",
    "        for e in range(self.args.num_epochs):\n",
    "            eval_train, nodes_embs = self.run_epoch(self.splitter.train, e, 'TRAIN', grad = True)\n",
    "            if len(self.splitter.dev) > 0 and e > self.args.eval_after_epochs:\n",
    "                eval_valid, _ = self.run_epoch(self.splitter.dev, e, 'VALID', grad = False)\n",
    "                if eval_valid > best_eval_valid:\n",
    "                    best_eval_valid = eval_valid\n",
    "                    epochs_without_impr = 0\n",
    "                    print ('### w' + str(self.args.rank) + ') ep ' + str(e) + ' - Best valid measure:' + str(eval_valid))\n",
    "                else:\n",
    "                    epochs_without_impr+=1\n",
    "                    if epochs_without_impr>self.args.early_stop_patience:\n",
    "                        print ('### w'+str(self.args.rank)+') ep '+str(e)+' - Early stop.')\n",
    "                        break\n",
    "\n",
    "            if len(self.splitter.test)>0 and eval_valid==best_eval_valid and e>self.args.eval_after_epochs:\n",
    "                eval_test, _ = self.run_epoch(self.splitter.test, e, 'TEST', grad = False)\n",
    "\n",
    "                if self.args.save_node_embeddings:\n",
    "                    self.save_node_embs_csv(nodes_embs, self.splitter.train_idx, log_file+'_train_nodeembs.csv.gz')\n",
    "                    self.save_node_embs_csv(nodes_embs, self.splitter.dev_idx, log_file+'_valid_nodeembs.csv.gz')\n",
    "                    self.save_node_embs_csv(nodes_embs, self.splitter.test_idx, log_file+'_test_nodeembs.csv.gz')\n",
    "\n",
    "\n",
    "    def run_epoch(self, split, epoch, set_name, grad):\n",
    "        t0 = time.time()\n",
    "        log_interval=999\n",
    "        if set_name=='TEST':\n",
    "            log_interval=1\n",
    "        self.logger.log_epoch_start(epoch, len(split), set_name, minibatch_log_interval=log_interval)\n",
    "\n",
    "        torch.set_grad_enabled(grad)\n",
    "        for s in split:\n",
    "            if self.tasker.is_static:\n",
    "                s = self.prepare_static_sample(s)\n",
    "            else:\n",
    "                s = self.prepare_sample(s)\n",
    "\n",
    "            predictions, nodes_embs = self.predict(s.hist_adj_list, s.hist_ndFeats_list, s.label_sp['idx'], s.node_mask_list)\n",
    "\n",
    "            loss = self.comp_loss(predictions,s.label_sp['vals'])\n",
    "            # print(loss)\n",
    "            \n",
    "            if set_name in ['TEST', 'VALID'] and self.args.task == 'link_pred':\n",
    "                self.logger.log_minibatch(predictions, s.label_sp['vals'], loss.detach(), adj = s.label_sp['idx'])\n",
    "            else:\n",
    "                self.logger.log_minibatch(predictions, s.label_sp['vals'], loss.detach())\n",
    "                \n",
    "            if grad:\n",
    "                self.optim_step(loss)\n",
    "\n",
    "        torch.set_grad_enabled(True)\n",
    "        eval_measure = self.logger.log_epoch_done()\n",
    "\n",
    "        return eval_measure, nodes_embs\n",
    "\n",
    "    def predict(self,hist_adj_list,hist_ndFeats_list,node_indices,mask_list):\n",
    "        nodes_embs = self.gcn(hist_adj_list, hist_ndFeats_list, mask_list)\n",
    "\n",
    "        predict_batch_size = 100000\n",
    "        gather_predictions=[]\n",
    "        for i in range(1 +(node_indices.size(1)//predict_batch_size)):\n",
    "            cls_input = self.gather_node_embs(nodes_embs, node_indices[:, i*predict_batch_size:(i+1)*predict_batch_size])\n",
    "            predictions = self.classifier(cls_input)\n",
    "            gather_predictions.append(predictions)\n",
    "        gather_predictions=torch.cat(gather_predictions, dim=0)\n",
    "        return gather_predictions, nodes_embs\n",
    "\n",
    "    def gather_node_embs(self,nodes_embs,node_indices):\n",
    "        cls_input = []\n",
    "\n",
    "        for node_set in node_indices:\n",
    "            cls_input.append(nodes_embs[node_set])\n",
    "        return torch.cat(cls_input,dim = 1)\n",
    "\n",
    "    def optim_step(self,loss):\n",
    "        self.tr_step += 1\n",
    "        loss.backward()\n",
    "\n",
    "        if self.tr_step % self.args.steps_accum_gradients == 0:\n",
    "            self.gcn_opt.step()\n",
    "            self.classifier_opt.step()\n",
    "\n",
    "            self.gcn_opt.zero_grad()\n",
    "            self.classifier_opt.zero_grad()\n",
    "\n",
    "\n",
    "    def prepare_sample(self,sample):\n",
    "        sample = Namespace(sample)\n",
    "        for i,adj in enumerate(sample.hist_adj_list):\n",
    "            adj = sparse_prepare_tensor(adj,torch_size = [self.num_nodes])\n",
    "            sample.hist_adj_list[i] = adj.to(self.args.device)\n",
    "\n",
    "            nodes = self.tasker.prepare_node_feats(sample.hist_ndFeats_list[i])\n",
    "\n",
    "            sample.hist_ndFeats_list[i] = nodes.to(self.args.device)\n",
    "            node_mask = sample.node_mask_list[i]\n",
    "            sample.node_mask_list[i] = node_mask.to(self.args.device).t() #transposed to have same dimensions as scorer\n",
    "\n",
    "        label_sp = self.ignore_batch_dim(sample.label_sp)\n",
    "\n",
    "        if self.args.task in [\"link_pred\", \"edge_cls\"]:\n",
    "            label_sp['idx'] = label_sp['idx'].to(self.args.device).t()   ####### ALDO TO CHECK why there was the .t() -----> because I concatenate embeddings when there are pairs of them, the embeddings are row vectors after the transpose\n",
    "        else:\n",
    "            label_sp['idx'] = label_sp['idx'].to(self.args.device)\n",
    "\n",
    "        label_sp['vals'] = label_sp['vals'].type(torch.long).to(self.args.device)\n",
    "        sample.label_sp = label_sp\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def prepare_static_sample(self,sample):\n",
    "        sample = Namespace(sample)\n",
    "\n",
    "        sample.hist_adj_list = self.hist_adj_list\n",
    "\n",
    "        sample.hist_ndFeats_list = self.hist_ndFeats_list\n",
    "\n",
    "        label_sp = {}\n",
    "        label_sp['idx'] =  [sample.idx]\n",
    "        label_sp['vals'] = sample.label\n",
    "        sample.label_sp = label_sp\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def ignore_batch_dim(self,adj):\n",
    "        if self.args.task in [\"link_pred\", \"edge_cls\"]:\n",
    "            adj['idx'] = adj['idx'][0]\n",
    "        adj['vals'] = adj['vals'][0]\n",
    "        return adj\n",
    "\n",
    "    def save_node_embs_csv(self, nodes_embs, indexes, file_name):\n",
    "        csv_node_embs = []\n",
    "        for node_id in indexes:\n",
    "            orig_ID = torch.DoubleTensor([self.tasker.data.contID_to_origID[node_id]])\n",
    "\n",
    "            csv_node_embs.append(torch.cat((orig_ID,nodes_embs[node_id].double())).detach().numpy())\n",
    "\n",
    "        pd.DataFrame(np.array(csv_node_embs)).to_csv(file_name, header=None, index=None, compression='gzip')\n",
    "        # print ('Node embs saved in',file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_param_value(param, param_min, param_max, type='int'):\n",
    "    if str(param) is None or str(param).lower()=='none':\n",
    "        if type=='int':\n",
    "            return random.randrange(param_min, param_max+1)\n",
    "        elif type=='logscale':\n",
    "            interval=np.logspace(np.log10(param_min), np.log10(param_max), num=100)\n",
    "            return np.random.choice(interval, 1)[0]\n",
    "        else:\n",
    "            return random.uniform(param_min, param_max)\n",
    "    else:\n",
    "        return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_random_hyper_params(args):\n",
    "    if args.model == 'all':\n",
    "        model_types = ['gcn', 'egcn_o', 'egcn_h', 'gruA', 'gruB','egcn','lstmA', 'lstmB']\n",
    "        args.model=model_types[args.rank]\n",
    "    elif args.model == 'all_nogcn':\n",
    "        model_types = ['egcn_o', 'egcn_h', 'gruA', 'gruB','egcn','lstmA', 'lstmB']\n",
    "        args.model=model_types[args.rank]\n",
    "    elif args.model == 'all_noegcn3':\n",
    "        model_types = ['gcn', 'egcn_h', 'gruA', 'gruB','egcn','lstmA', 'lstmB']\n",
    "        args.model=model_types[args.rank]\n",
    "    elif args.model == 'all_nogruA':\n",
    "        model_types = ['gcn', 'egcn_o', 'egcn_h', 'gruB','egcn','lstmA', 'lstmB']\n",
    "        args.model=model_types[args.rank]\n",
    "        args.model=model_types[args.rank]\n",
    "    elif args.model == 'saveembs':\n",
    "        model_types = ['gcn', 'gcn', 'skipgcn', 'skipgcn']\n",
    "        args.model=model_types[args.rank]\n",
    "\n",
    "    args.learning_rate =random_param_value(args.learning_rate, args.learning_rate_min, args.learning_rate_max, type='logscale')\n",
    "    # args.adj_mat_time_window = random_param_value(args.adj_mat_time_window, args.adj_mat_time_window_min, args.adj_mat_time_window_max, type='int')\n",
    "\n",
    "    if args.model == 'gcn':\n",
    "        args.num_hist_steps = 0\n",
    "    else:\n",
    "        args.num_hist_steps = random_param_value(args.num_hist_steps, args.num_hist_steps_min, args.num_hist_steps_max, type='int')\n",
    "\n",
    "    args.gcn_parameters['feats_per_node'] =random_param_value(args.gcn_parameters['feats_per_node'], args.gcn_parameters['feats_per_node_min'], args.gcn_parameters['feats_per_node_max'], type='int')\n",
    "    args.gcn_parameters['layer_1_feats'] =random_param_value(args.gcn_parameters['layer_1_feats'], args.gcn_parameters['layer_1_feats_min'], args.gcn_parameters['layer_1_feats_max'], type='int')\n",
    "    if args.gcn_parameters['layer_2_feats_same_as_l1'] or args.gcn_parameters['layer_2_feats_same_as_l1'].lower()=='true':\n",
    "        args.gcn_parameters['layer_2_feats'] = args.gcn_parameters['layer_1_feats']\n",
    "    else:\n",
    "        args.gcn_parameters['layer_2_feats'] =random_param_value(args.gcn_parameters['layer_2_feats'], args.gcn_parameters['layer_1_feats_min'], args.gcn_parameters['layer_1_feats_max'], type='int')\n",
    "    args.gcn_parameters['lstm_l1_feats'] =random_param_value(args.gcn_parameters['lstm_l1_feats'], args.gcn_parameters['lstm_l1_feats_min'], args.gcn_parameters['lstm_l1_feats_max'], type='int')\n",
    "    if args.gcn_parameters['lstm_l2_feats_same_as_l1'] or args.gcn_parameters['lstm_l2_feats_same_as_l1'].lower()=='true':\n",
    "        args.gcn_parameters['lstm_l2_feats'] = args.gcn_parameters['lstm_l1_feats']\n",
    "    else:\n",
    "        args.gcn_parameters['lstm_l2_feats'] =random_param_value(args.gcn_parameters['lstm_l2_feats'], args.gcn_parameters['lstm_l1_feats_min'], args.gcn_parameters['lstm_l1_feats_max'], type='int')\n",
    "    args.gcn_parameters['cls_feats']=random_param_value(args.gcn_parameters['cls_feats'], args.gcn_parameters['cls_feats_min'], args.gcn_parameters['cls_feats_max'], type='int')\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(args):\n",
    "    if args.data == 'bitcoinotc' or args.data == 'bitcoinalpha':\n",
    "        if args.data == 'bitcoinotc':\n",
    "            args.bitcoin_args = args.bitcoinotc_args\n",
    "        elif args.data == 'bitcoinalpha':\n",
    "            args.bitcoin_args = args.bitcoinalpha_args\n",
    "        return bitcoin_dataset(args)\n",
    "    \n",
    "    elif args.data == 'aml_sim':  #\n",
    "        return aml.Aml_Dataset(args)\n",
    "    \n",
    "    elif args.data == 'elliptic':  #\n",
    "        return ell.Elliptic_Dataset(args)\n",
    "    \n",
    "    elif args.data == 'elliptic_temporal':\n",
    "        return Elliptic_Temporal_Dataset(args)\n",
    "    \n",
    "    elif args.data == 'uc_irv_mess':\n",
    "        return Uc_Irvine_Message_Dataset(args)\n",
    "    \n",
    "    elif args.data == 'dbg':  #\n",
    "        return dbg.dbg_dataset(args)\n",
    "    \n",
    "    elif args.data == 'colored_graph':  #\n",
    "        return cg.Colored_Graph(args)\n",
    "    \n",
    "    elif args.data == 'autonomous_syst':\n",
    "        return Autonomous_Systems_Dataset(args)\n",
    "    \n",
    "    elif args.data == 'reddit':\n",
    "        return Reddit_Dataset(args)\n",
    "    \n",
    "    elif args.data.startswith('sbm'):\n",
    "        if args.data == 'sbm20':\n",
    "            args.sbm_args = args.sbm20_args\n",
    "        elif args.data == 'sbm50':\n",
    "            args.sbm_args = args.sbm50_args\n",
    "        return sbm_dataset(args)\n",
    "    else:\n",
    "        raise NotImplementedError('only arxiv has been implemented')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tasker(args,dataset):\n",
    "    if args.task == 'link_pred':\n",
    "        return Link_Pred_Tasker(args,dataset)\n",
    "    elif args.task == 'edge_cls':\n",
    "        return Edge_Cls_Tasker(args,dataset)\n",
    "    elif args.task == 'node_cls':\n",
    "        return Node_Cls_Tasker(args,dataset)\n",
    "    elif args.task == 'static_node_cls':\n",
    "        return Static_Node_Cls_Tasker(args,dataset)\n",
    "    else:\n",
    "        raise NotImplementedError('still need to implement the other tasks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gcn(args,tasker):\n",
    "    gcn_args = Namespace(args.gcn_parameters)\n",
    "    gcn_args.feats_per_node = tasker.feats_per_node\n",
    "    \n",
    "    if args.model == 'gcn':\n",
    "        return Sp_GCN(gcn_args,activation = torch.nn.RReLU()).to(args.device)\n",
    "    \n",
    "    elif args.model == 'skipgcn':\n",
    "        return Sp_Skip_GCN(gcn_args,activation = torch.nn.RReLU()).to(args.device)\n",
    "    \n",
    "    elif args.model == 'skipfeatsgcn':\n",
    "        return Sp_Skip_NodeFeats_GCN(gcn_args,activation = torch.nn.RReLU()).to(args.device)\n",
    "    \n",
    "    else:\n",
    "        assert args.num_hist_steps > 0, 'more than one step is necessary to train LSTM'\n",
    "        if args.model == 'lstmA':\n",
    "            return Sp_GCN_LSTM_A(gcn_args,activation = torch.nn.RReLU()).to(args.device)\n",
    "        \n",
    "        elif args.model == 'gruA':\n",
    "            return Sp_GCN_GRU_A(gcn_args,activation = torch.nn.RReLU()).to(args.device)\n",
    "        \n",
    "        elif args.model == 'lstmB':\n",
    "            return Sp_GCN_LSTM_B(gcn_args,activation = torch.nn.RReLU()).to(args.device)\n",
    "        \n",
    "        elif args.model == 'gruB':\n",
    "            return Sp_GCN_GRU_B(gcn_args,activation = torch.nn.RReLU()).to(args.device)\n",
    "        \n",
    "        elif args.model == 'egcn': #\n",
    "            return egcn.EGCN(gcn_args, activation = torch.nn.RReLU()).to(args.device)\n",
    "        \n",
    "        elif args.model == 'egcn_h':\n",
    "            return EGCN_H(gcn_args, activation = torch.nn.RReLU(), device = args.device)\n",
    "        \n",
    "        elif args.model == 'skipfeatsegcn_h':\n",
    "            return EGCN_H(gcn_args, activation = torch.nn.RReLU(), device = args.device, skipfeats=True)\n",
    "        \n",
    "        elif args.model == 'egcn_o':\n",
    "            return EGCN_O(gcn_args, activation = torch.nn.RReLU(), device = args.device)\n",
    "        \n",
    "        else:\n",
    "            raise NotImplementedError('need to finish modifying the models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier(args,tasker):\n",
    "    if 'node_cls' == args.task or 'static_node_cls' == args.task:\n",
    "        mult = 1\n",
    "    else:\n",
    "        mult = 2\n",
    "        \n",
    "    if 'gru' in args.model or 'lstm' in args.model:\n",
    "        in_feats = args.gcn_parameters['lstm_l2_feats'] * mult\n",
    "        \n",
    "    elif args.model == 'skipfeatsgcn' or args.model == 'skipfeatsegcn_h':\n",
    "        in_feats = (args.gcn_parameters['layer_2_feats'] + args.gcn_parameters['feats_per_node']) * mult\n",
    "        \n",
    "    else:\n",
    "        in_feats = args.gcn_parameters['layer_2_feats'] * mult\n",
    "\n",
    "    return Classifier(args,in_features = in_feats, out_features = tasker.num_classes).to(args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = create_parser()\n",
    "args = parse_args(parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "global rank, wsize, use_cuda\n",
    "args.use_cuda = (torch.cuda.is_available() and args.use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use CUDA: False - device: cpu\n"
     ]
    }
   ],
   "source": [
    "args.device = \"cpu\"\n",
    "if args.use_cuda:\n",
    "    args.device = \"cuda\"\n",
    "print (\"use CUDA:\", args.use_cuda, \"- device:\", args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPI backend not preset. Set process rank to 0 (out of 1)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    dist.init_process_group(backend='mpi') #, world_size=4\n",
    "    rank = dist.get_rank()\n",
    "    wsize = dist.get_world_size()\n",
    "    print('Hello from process {} (out of {})'.format(dist.get_rank(), dist.get_world_size()))\n",
    "    \n",
    "    if args.use_cuda:\n",
    "        torch.cuda.set_device(rank)  # are we sure of the rank+1????\n",
    "        print('using the device {}'.format(torch.cuda.current_device())) \n",
    "except:\n",
    "    rank = 0\n",
    "    wsize = 1\n",
    "    print(('MPI backend not preset. Set process rank to {} (out of {})'.format(rank, wsize)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.seed is None and args.seed != \"None\":\n",
    "    seed = 123 + rank # int(time.time())+rank\n",
    "else:\n",
    "    seed = args.seed # +rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "args.seed=seed\n",
    "args.rank=rank\n",
    "args.wsize=wsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the requested random hyper parameters\n",
    "args = build_random_hyper_params(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TIME tensor(49) tensor(0)\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "dataset = build_dataset(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the tasker\n",
    "tasker = build_tasker(args, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splits sizes:  train 29 dev 5 test 10\n"
     ]
    }
   ],
   "source": [
    "# build the splitter\n",
    "splitter = splitter(args, tasker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLS num_feats 200\n"
     ]
    }
   ],
   "source": [
    "# build the models\n",
    "gcn = build_gcn(args, tasker)\n",
    "classifier = build_classifier(args, tasker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a loss\n",
    "cross_entropy = Cross_Entropy(args, dataset).to(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log: STDOUT\n",
      "INFO:root:*** PARAMETERS ***\n",
      "INFO:root:{'adj_mat_time_window': 1,\n",
      " 'class_weights': [0.1, 0.9],\n",
      " 'comments': ['comments'],\n",
      " 'data': 'sbm50',\n",
      " 'data_loading_params': {'batch_size': 1, 'num_workers': 0},\n",
      " 'dev_proportion': 0.1,\n",
      " 'device': 'cpu',\n",
      " 'early_stop_patience': 50,\n",
      " 'eval_after_epochs': 5,\n",
      " 'gcn_parameters': {'cls_feats': 100,\n",
      "                    'cls_feats_max': 800,\n",
      "                    'cls_feats_min': 100,\n",
      "                    'feats_per_node': 100,\n",
      "                    'feats_per_node_max': 256,\n",
      "                    'feats_per_node_min': 50,\n",
      "                    'k_top_grcu': 200,\n",
      "                    'layer_1_feats': 100,\n",
      "                    'layer_1_feats_max': 200,\n",
      "                    'layer_1_feats_min': 10,\n",
      "                    'layer_2_feats': 100,\n",
      "                    'layer_2_feats_same_as_l1': True,\n",
      "                    'lstm_l1_feats': 100,\n",
      "                    'lstm_l1_feats_max': 200,\n",
      "                    'lstm_l1_feats_min': 10,\n",
      "                    'lstm_l1_layers': 1,\n",
      "                    'lstm_l2_feats': 100,\n",
      "                    'lstm_l2_feats_same_as_l1': True,\n",
      "                    'lstm_l2_layers': 1,\n",
      "                    'num_layers': 2},\n",
      " 'learning_rate': 0.005,\n",
      " 'learning_rate_max': 0.1,\n",
      " 'learning_rate_min': 0.0001,\n",
      " 'model': 'egcn_o',\n",
      " 'negative_mult_test': 100,\n",
      " 'negative_mult_training': 50,\n",
      " 'num_epochs': 100,\n",
      " 'num_hist_steps': 5,\n",
      " 'num_hist_steps_max': 10,\n",
      " 'num_hist_steps_min': 1,\n",
      " 'rank': 0,\n",
      " 'save_node_embeddings': False,\n",
      " 'sbm50_args': {'aggr_time': 1,\n",
      "                'edges_file': 'sbm_50t_1000n_adj.csv',\n",
      "                'feats_per_node': 3,\n",
      "                'folder': 'C:\\\\Users\\\\sss\\\\Desktop\\\\EvolveGCN-master\\\\data/'},\n",
      " 'sbm_args': <__main__.Namespace object at 0x0000028E8D7005C0>,\n",
      " 'seed': 1234,\n",
      " 'smart_neg_sampling': True,\n",
      " 'steps_accum_gradients': 1,\n",
      " 'target_class': 1,\n",
      " 'target_measure': 'MAP',\n",
      " 'task': 'link_pred',\n",
      " 'train_proportion': 0.7,\n",
      " 'use_1_hot_node_feats': True,\n",
      " 'use_2_hot_node_feats': False,\n",
      " 'use_cuda': False,\n",
      " 'use_logfile': False,\n",
      " 'wsize': 1}\n",
      "INFO:root:\n"
     ]
    }
   ],
   "source": [
    "#trainer\n",
    "trainer = Trainer(args, splitter=splitter, gcn=gcn, classifier=classifier, comp_loss=cross_entropy, dataset=dataset, num_classes=tasker.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:################ TRAIN epoch 0 ###################\n",
      "INFO:root:TRAIN mean losses tensor(0.1159)\n",
      "INFO:root:TRAIN mean errors 0.4268079996109009\n",
      "INFO:root:TRAIN mean MRR 0.0 - mean MAP 0.1651397294252547\n",
      "INFO:root:TRAIN tp {0: tensor(14709637), 1: tensor(1896303)},fn {0: tensor(11426164), 1: tensor(938887)},fp {0: tensor(938887), 1: tensor(11426164)}\n",
      "INFO:root:TRAIN measures microavg - precision 0.5732 - recall 0.5732 - f1 0.5732 \n",
      "INFO:root:TRAIN measures for class 0 - precision 0.9400 - recall 0.5628 - f1 0.7041 \n",
      "INFO:root:TRAIN measures for class 1 - precision 0.1423 - recall 0.6688 - f1 0.2347 \n",
      "INFO:root:TRAIN measures@10 microavg - precision 0.5596 - recall 0.0000 - f1 0.0000 \n",
      "INFO:root:TRAIN measures@10 for class 0 - precision 0.9000 - recall 0.0000 - f1 0.0000 \n",
      "INFO:root:TRAIN measures@10 for class 1 - precision 0.2043 - recall 0.0000 - f1 0.0000 \n",
      "INFO:root:TRAIN measures@100 microavg - precision 0.5740 - recall 0.0001 - f1 0.0002 \n",
      "INFO:root:TRAIN measures@100 for class 0 - precision 0.9121 - recall 0.0001 - f1 0.0002 \n",
      "INFO:root:TRAIN measures@100 for class 1 - precision 0.2213 - recall 0.0002 - f1 0.0004 \n",
      "INFO:root:TRAIN measures@1000 microavg - precision 0.5762 - recall 0.0009 - f1 0.0019 \n",
      "INFO:root:TRAIN measures@1000 for class 0 - precision 0.9161 - recall 0.0008 - f1 0.0017 \n",
      "INFO:root:TRAIN measures@1000 for class 1 - precision 0.2215 - recall 0.0018 - f1 0.0036 \n",
      "INFO:root:TRAIN Total epoch time: 570.859000000055\n",
      "INFO:root:################ TRAIN epoch 1 ###################\n",
      "INFO:root:TRAIN mean losses tensor(0.0995)\n",
      "INFO:root:TRAIN mean errors 0.433289498090744\n",
      "INFO:root:TRAIN mean MRR 0.0 - mean MAP 0.1991868872136773\n",
      "INFO:root:TRAIN tp {0: tensor(13759081), 1: tensor(2659079)},fn {0: tensor(12376714), 1: tensor(176111)},fp {0: tensor(176111), 1: tensor(12376714)}\n",
      "INFO:root:TRAIN measures microavg - precision 0.5667 - recall 0.5667 - f1 0.5667 \n",
      "INFO:root:TRAIN measures for class 0 - precision 0.9874 - recall 0.5264 - f1 0.6867 \n",
      "INFO:root:TRAIN measures for class 1 - precision 0.1768 - recall 0.9379 - f1 0.2976 \n",
      "INFO:root:TRAIN measures@10 microavg - precision 0.6293 - recall 0.0000 - f1 0.0000 \n",
      "INFO:root:TRAIN measures@10 for class 0 - precision 0.9966 - recall 0.0000 - f1 0.0000 \n",
      "INFO:root:TRAIN measures@10 for class 1 - precision 0.2621 - recall 0.0000 - f1 0.0001 \n",
      "INFO:root:TRAIN measures@100 microavg - precision 0.6322 - recall 0.0001 - f1 0.0003 \n",
      "INFO:root:TRAIN measures@100 for class 0 - precision 0.9972 - recall 0.0001 - f1 0.0002 \n",
      "INFO:root:TRAIN measures@100 for class 1 - precision 0.2672 - recall 0.0003 - f1 0.0005 \n",
      "INFO:root:TRAIN measures@1000 microavg - precision 0.6254 - recall 0.0013 - f1 0.0025 \n",
      "INFO:root:TRAIN measures@1000 for class 0 - precision 0.9977 - recall 0.0011 - f1 0.0022 \n",
      "INFO:root:TRAIN measures@1000 for class 1 - precision 0.2531 - recall 0.0026 - f1 0.0051 \n",
      "INFO:root:TRAIN Total epoch time: 560.890000000014\n",
      "INFO:root:################ TRAIN epoch 2 ###################\n",
      "INFO:root:TRAIN mean losses tensor(0.0993)\n",
      "INFO:root:TRAIN mean errors 0.4235842227935791\n",
      "INFO:root:TRAIN mean MRR 0.0 - mean MAP 0.19971366055345977\n",
      "INFO:root:TRAIN tp {0: tensor(14068318), 1: tensor(2631015)},fn {0: tensor(12067476), 1: tensor(204175)},fp {0: tensor(204175), 1: tensor(12067476)}\n",
      "INFO:root:TRAIN measures microavg - precision 0.5764 - recall 0.5764 - f1 0.5764 \n",
      "INFO:root:TRAIN measures for class 0 - precision 0.9857 - recall 0.5383 - f1 0.6963 \n",
      "INFO:root:TRAIN measures for class 1 - precision 0.1790 - recall 0.9280 - f1 0.3001 \n",
      "INFO:root:TRAIN measures@10 microavg - precision 0.6155 - recall 0.0000 - f1 0.0000 \n",
      "INFO:root:TRAIN measures@10 for class 0 - precision 1.0000 - recall 0.0000 - f1 0.0000 \n",
      "INFO:root:TRAIN measures@10 for class 1 - precision 0.2310 - recall 0.0000 - f1 0.0000 \n",
      "INFO:root:TRAIN measures@100 microavg - precision 0.6317 - recall 0.0001 - f1 0.0003 \n",
      "INFO:root:TRAIN measures@100 for class 0 - precision 0.9986 - recall 0.0001 - f1 0.0002 \n",
      "INFO:root:TRAIN measures@100 for class 1 - precision 0.2648 - recall 0.0003 - f1 0.0005 \n",
      "INFO:root:TRAIN measures@1000 microavg - precision 0.6249 - recall 0.0013 - f1 0.0025 \n",
      "INFO:root:TRAIN measures@1000 for class 0 - precision 0.9972 - recall 0.0011 - f1 0.0022 \n",
      "INFO:root:TRAIN measures@1000 for class 1 - precision 0.2525 - recall 0.0026 - f1 0.0051 \n",
      "INFO:root:TRAIN Total epoch time: 554.625\n",
      "INFO:root:################ TRAIN epoch 3 ###################\n",
      "INFO:root:TRAIN mean losses tensor(0.1002)\n",
      "INFO:root:TRAIN mean errors 0.4378359317779541\n",
      "INFO:root:TRAIN mean MRR 0.0 - mean MAP 0.20003320426690094\n",
      "INFO:root:TRAIN tp {0: tensor(13619395), 1: tensor(2667057)},fn {0: tensor(12516408), 1: tensor(168133)},fp {0: tensor(168133), 1: tensor(12516408)}\n",
      "INFO:root:TRAIN measures microavg - precision 0.5622 - recall 0.5622 - f1 0.5622 \n",
      "INFO:root:TRAIN measures for class 0 - precision 0.9878 - recall 0.5211 - f1 0.6823 \n",
      "INFO:root:TRAIN measures for class 1 - precision 0.1757 - recall 0.9407 - f1 0.2960 \n",
      "INFO:root:TRAIN measures@10 microavg - precision 0.5914 - recall 0.0000 - f1 0.0000 \n",
      "INFO:root:TRAIN measures@10 for class 0 - precision 1.0000 - recall 0.0000 - f1 0.0000 \n",
      "INFO:root:TRAIN measures@10 for class 1 - precision 0.1828 - recall 0.0000 - f1 0.0000 \n",
      "INFO:root:TRAIN measures@100 microavg - precision 0.6193 - recall 0.0001 - f1 0.0002 \n",
      "INFO:root:TRAIN measures@100 for class 0 - precision 0.9983 - recall 0.0001 - f1 0.0002 \n",
      "INFO:root:TRAIN measures@100 for class 1 - precision 0.2403 - recall 0.0002 - f1 0.0005 \n",
      "INFO:root:TRAIN measures@1000 microavg - precision 0.6246 - recall 0.0013 - f1 0.0025 \n",
      "INFO:root:TRAIN measures@1000 for class 0 - precision 0.9975 - recall 0.0011 - f1 0.0022 \n",
      "INFO:root:TRAIN measures@1000 for class 1 - precision 0.2518 - recall 0.0026 - f1 0.0051 \n",
      "INFO:root:TRAIN Total epoch time: 568.0\n",
      "INFO:root:################ TRAIN epoch 4 ###################\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\sss\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2961, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-88-3435b262f1ae>\", line 1, in <module>\n",
      "    trainer.train()\n",
      "  File \"<ipython-input-68-e16c400f61ac>\", line 57, in train\n",
      "    eval_train, nodes_embs = self.run_epoch(self.splitter.train, e, 'TRAIN', grad = True)\n",
      "  File \"<ipython-input-68-e16c400f61ac>\", line 87, in run_epoch\n",
      "    for s in split:\n",
      "  File \"c:\\users\\sss\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 517, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"c:\\users\\sss\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 557, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"c:\\users\\sss\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 44, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"c:\\users\\sss\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 44, in <listcomp>\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"<ipython-input-64-6322135fc74a>\", line 15, in __getitem__\n",
      "    t = self.tasker.get_sample(idx, test = self.test, **self.kwargs)\n",
      "  File \"<ipython-input-42-a3bc93447e54>\", line 137, in get_sample\n",
      "    existing_nodes = existing_nodes)\n",
      "  File \"<ipython-input-39-bf1e571b852f>\", line 43, in get_non_existing_edges\n",
      "    if eid in out_ids or edges[0,i] == edges[1,i] or eid in true_ids:\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\sss\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 1863, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\sss\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"c:\\users\\sss\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"c:\\users\\sss\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"c:\\users\\sss\\appdata\\local\\programs\\python\\python36\\lib\\inspect.py\", line 1480, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"c:\\users\\sss\\appdata\\local\\programs\\python\\python36\\lib\\inspect.py\", line 1438, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"c:\\users\\sss\\appdata\\local\\programs\\python\\python36\\lib\\inspect.py\", line 693, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"c:\\users\\sss\\appdata\\local\\programs\\python\\python36\\lib\\inspect.py\", line 729, in getmodule\n",
      "    for modname, module in list(sys.modules.items()):\n",
      "KeyboardInterrupt\n",
      "INFO:root:\n",
      "Unfortunately, your original traceback can not be constructed.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
