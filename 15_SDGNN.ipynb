{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import subprocess\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "import re\n",
    "import json\n",
    "import pickle\n",
    "import math\n",
    "import random\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import svm\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NUM_DIC = {\n",
    "    'epinions': 131828,\n",
    "    'slashdot': 82140,\n",
    "    'bitcoin_alpha': 3783,\n",
    "    'bitcoin_otc': 5881,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaExtra(object):\n",
    "    def __init__(self, dataset='epinions', k=1, debug=False):\n",
    "        filename = r'C:\\Users\\sss\\Desktop\\SiGAT/experiment-data/{}-train-{}.edgelist'.format(dataset, k)  # 导入训练数据\n",
    "        if debug:\n",
    "            filename = './test.edgelists'\n",
    "        res = self.init_edgelists(filename=filename)\n",
    "        self.pos_in_edgelists, self.pos_out_edgelists, self.neg_in_edgelists, self.neg_out_edgelists = res\n",
    "\n",
    "    def init_edgelists(self, filename=r'C:\\Users\\sss\\Desktop\\SiGAT/experiment-data/epinions-train-1.edgelist'):\n",
    "        \n",
    "        pos_out_edgelists = defaultdict(list)\n",
    "        neg_out_edgelists = defaultdict(list)\n",
    "        pos_in_edgelists = defaultdict(list)\n",
    "        neg_in_edgelists = defaultdict(list)\n",
    "        \n",
    "        with open(filename) as f:\n",
    "            for line in f.readlines():\n",
    "                x, y, z = line.split()\n",
    "                x = int(x)\n",
    "                y = int(y)\n",
    "                z = int(z)\n",
    "                \n",
    "                if z == 1:\n",
    "                    pos_out_edgelists[x].append(y)  # u->v; u向外指向v的有向边\n",
    "                    pos_in_edgelists[y].append(x)  # v->u: v被u指向\n",
    "                else:\n",
    "                    neg_out_edgelists[x].append(y)\n",
    "                    neg_in_edgelists[y].append(x)\n",
    "        return pos_in_edgelists, pos_out_edgelists, neg_in_edgelists, neg_out_edgelists\n",
    "\n",
    "    def get_pos_indegree(self, v):\n",
    "        return len(self.pos_in_edgelists[v])\n",
    "\n",
    "    def get_pos_outdegree(self, v):\n",
    "        return len(self.pos_out_edgelists[v])\n",
    "\n",
    "    def get_neg_indegree(self, v):\n",
    "        return len(self.neg_in_edgelists[v])\n",
    "\n",
    "    def get_neg_outdegree(self, v):\n",
    "        return len(self.neg_out_edgelists[v])\n",
    "\n",
    "    def common_neighbors(self, u, v):\n",
    "        u_neighbors = self.pos_in_edgelists[u] + self.neg_in_edgelists[u] + \\\n",
    "                      self.pos_out_edgelists[u] + self.neg_out_edgelists[u]\n",
    "        v_neighbors = self.pos_in_edgelists[v] + self.neg_in_edgelists[v] + \\\n",
    "                      self.pos_out_edgelists[v] + self.neg_out_edgelists[v]\n",
    "        return len(set(u_neighbors).intersection(set(v_neighbors)))\n",
    "\n",
    "    def feature_part1(self, u, v):\n",
    "        d_pos_in_u = self.get_pos_indegree(u)\n",
    "        d_neg_in_v = self.get_neg_indegree(v)\n",
    "        d_pos_out_u = self.get_pos_outdegree(u)\n",
    "        d_neg_out_v = self.get_neg_outdegree(v)\n",
    "\n",
    "        # d_pos_in_v = self.get_pos_indegree(v)\n",
    "        # d_neg_in_u = self.get_neg_indegree(u)\n",
    "        # d_pos_out_v = self.get_pos_outdegree(v)\n",
    "        # d_neg_out_u = self.get_neg_outdegree(u)\n",
    "\n",
    "        c_u_v = self.common_neighbors(u, v)\n",
    "        d_out_u = self.get_neg_outdegree(u) + self.get_pos_outdegree(u)\n",
    "        d_in_v = self.get_neg_indegree(v) + self.get_pos_indegree(v)\n",
    "        return d_pos_in_u, d_neg_in_v, d_pos_out_u, d_neg_out_v, c_u_v, d_out_u, d_in_v\n",
    "\n",
    "    def feature_part2(self, u, v):\n",
    "        \"\"\"\n",
    "        /^ \\v /^ \\^ /v \\v /v ^\\\n",
    "        ++\n",
    "        /^ \\v /^ \\^ /v \\v /v ^\\\n",
    "        +-\n",
    "        /^ \\v /^ \\^ /v \\v /v ^\\\n",
    "        -+\n",
    "        /^ \\v /^ \\^ /v \\v /v ^\\\n",
    "        --\n",
    "        \"\"\"\n",
    "        d1_1 = len(set(self.pos_out_edgelists[u]).intersection(set(self.pos_in_edgelists[v])))  # 集合交集\n",
    "        d1_2 = len(set(self.pos_out_edgelists[u]).intersection(set(self.neg_in_edgelists[v])))\n",
    "        d1_3 = len(set(self.neg_out_edgelists[u]).intersection(set(self.pos_in_edgelists[v])))\n",
    "        d1_4 = len(set(self.neg_out_edgelists[u]).intersection(set(self.neg_in_edgelists[v])))\n",
    "\n",
    "        d2_1 = len(set(self.pos_out_edgelists[u]).intersection(set(self.pos_out_edgelists[v])))\n",
    "        d2_2 = len(set(self.pos_out_edgelists[u]).intersection(set(self.neg_out_edgelists[v])))\n",
    "        d2_3 = len(set(self.neg_out_edgelists[u]).intersection(set(self.pos_out_edgelists[v])))\n",
    "        d2_4 = len(set(self.neg_out_edgelists[u]).intersection(set(self.neg_out_edgelists[v])))\n",
    "\n",
    "        d3_1 = len(set(self.pos_in_edgelists[u]).intersection(set(self.pos_out_edgelists[v])))\n",
    "        d3_2 = len(set(self.pos_in_edgelists[u]).intersection(set(self.neg_out_edgelists[v])))\n",
    "        d3_3 = len(set(self.neg_in_edgelists[u]).intersection(set(self.pos_out_edgelists[v])))\n",
    "        d3_4 = len(set(self.neg_in_edgelists[u]).intersection(set(self.neg_out_edgelists[v])))\n",
    "\n",
    "        d4_1 = len(set(self.pos_in_edgelists[u]).intersection(set(self.pos_in_edgelists[v])))\n",
    "        d4_2 = len(set(self.pos_in_edgelists[u]).intersection(set(self.neg_in_edgelists[v])))\n",
    "        d4_3 = len(set(self.neg_in_edgelists[u]).intersection(set(self.pos_in_edgelists[v])))\n",
    "        d4_4 = len(set(self.neg_in_edgelists[u]).intersection(set(self.neg_in_edgelists[v])))\n",
    "\n",
    "        return d1_1, d1_2, d1_3, d1_4, d2_1, d2_2, d2_3, d2_4, d3_1, d3_2, d3_3, d3_4, d4_1, d4_2, d4_3, d4_4\n",
    "\n",
    "    def get_features(self, u, v):\n",
    "        x11 = self.feature_part1(u, v)\n",
    "        x12 = self.feature_part2(u, v)\n",
    "        return x11 + x12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--devices', type=str, default='cpu', help='Devices')\n",
    "parser.add_argument('--seed', type=int, default=13, help='Random seed.')\n",
    "parser.add_argument('--epochs', type=int, default=100, help='Number of epochs to train.')\n",
    "parser.add_argument('--lr', type=float, default=0.001, help='Initial learning rate.')\n",
    "parser.add_argument('--weight_decay', type=float, default=0.001, help='Weight decay (L2 loss on parameters).')\n",
    "parser.add_argument('--dataset', default='bitcoin_alpha', help='Dataset')\n",
    "parser.add_argument('--dim', type=int, default=20, help='Embedding dimension')\n",
    "parser.add_argument('--fea_dim', type=int, default=20, help='Feature embedding dimension')\n",
    "parser.add_argument('--batch_size', type=int, default=500, help='Batch size')\n",
    "parser.add_argument('--dropout', type=float, default=0.0, help='Dropout k')\n",
    "parser.add_argument('--k', default=1, help='Folder k')\n",
    "parser.add_argument('--agg', default='attention', choices=['mean', 'attantion'], help='Aggregator choose')\n",
    "\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出路径\n",
    "\n",
    "root = r\"C:\\Users\\sss\\Desktop\\SiGAT\"\n",
    "OUTPUT_DIR = root + f'/embeddings/sdgnn-{args.agg}'\n",
    "if not os.path.exists(root + \"/\" + 'embeddings'):\n",
    "    os.mkdir(root + \"/\" + 'embeddings')\n",
    "\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "        os.mkdir(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1df3b5ee390>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 随机种子\n",
    "\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEG_LOSS_RATIO = 1\n",
    "INTERVAL_PRINT = 2\n",
    "\n",
    "NUM_NODE = DATASET_NUM_DIC[args.dataset]\n",
    "WEIGHT_DECAY = args.weight_decay\n",
    "NODE_FEAT_SIZE = args.fea_dim\n",
    "EMBEDDING_SIZE1 = args.dim\n",
    "DEVICES = torch.device(args.devices)\n",
    "LEARNING_RATE = args.lr\n",
    "BATCH_SIZE = args.batch_size\n",
    "EPOCHS = args.epochs\n",
    "DROUPOUT = args.dropout\n",
    "K = args.k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(DEVICES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 20\n",
    "\n",
    "SINE_MODEL_PATH_DIC = {\n",
    "    'epinions': root + '/embeddings/sine_epinions_models',\n",
    "    'slashdot': root + '/embeddings/sine_slashdot_models',\n",
    "    'bitcoin_alpha': root + '/embeddings/sine_bitcoin_alpha_models',\n",
    "    'bitcoin_otc': root + '/embeddings/sine_bitcoin_otc_models'\n",
    "}\n",
    "\n",
    "SIDE_MODEL_PATH_DIC = {\n",
    "    'epinions': root + '/embeddings/side_epinions_models',\n",
    "    'slashdot': root + '/embeddings/side_slashdot_models',\n",
    "    'bitcoin_alpha': root + '/embeddings/side_bitcoin_alpha_models',\n",
    "    'bitcoin_otc': root + '/embeddings/side_bitcoin_otc_models'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_train_test_data(dataset, k):\n",
    "    train_X = []\n",
    "    train_y = []\n",
    "    with open(root + '/experiment-data/{}-train-{}.edgelist'.format(dataset, k)) as f:\n",
    "        for line in f:\n",
    "            i, j, flag = line.split()\n",
    "            i = int(i)\n",
    "            j = int(j)\n",
    "            flag = int((int(flag) + 1) / 2)\n",
    "            train_X.append((i, j))\n",
    "            train_y.append(flag)\n",
    "    test_X = []\n",
    "    test_y = []\n",
    "    \n",
    "    with open(root + '/experiment-data/{}-test-{}.edgelist'.format(dataset, k)) as f:\n",
    "        for line in f:\n",
    "            i, j, flag = line.split()\n",
    "            i = int(i)\n",
    "            j = int(j)\n",
    "            flag = int((int(flag) + 1) / 2)\n",
    "            test_X.append((i, j))\n",
    "            test_y.append(flag)\n",
    "            \n",
    "    return np.array(train_X), np.array(train_y), np.array(test_X), np.array(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_logistic(dataset, k, embeddings, model):\n",
    "    train_X, train_y, test_X, test_y  = read_train_test_data(dataset, k)\n",
    "    \n",
    "    train_X1 = []\n",
    "    test_X1 = []\n",
    "    \n",
    "    for i, j in train_X:\n",
    "        train_X1.append(np.concatenate([embeddings[i], embeddings[j]]))\n",
    "        \n",
    "    for i, j in test_X:\n",
    "        test_X1.append(np.concatenate([embeddings[i], embeddings[j]]))\n",
    "        \n",
    "    logistic_function = linear_model.LogisticRegression()\n",
    "    logistic_function.fit(train_X1, train_y)\n",
    "    pred = logistic_function.predict(test_X1)\n",
    "    pred_p = logistic_function.predict_proba(test_X1)\n",
    "    \n",
    "    pos_ratio =  np.sum(test_y) / test_y.shape[0]\n",
    "    accuracy =  metrics.accuracy_score(test_y, pred)\n",
    "    f1_score0 =  metrics.f1_score(test_y, pred)\n",
    "    f1_score1 =  metrics.f1_score(test_y, pred, average='macro')\n",
    "    f1_score2 =  metrics.f1_score(test_y, pred, average='micro')\n",
    "    \n",
    "    auc_score =  metrics.roc_auc_score(test_y, pred_p[:, 1])\n",
    "    # print(\"pos_ratio:\", pos_ratio)\n",
    "    # print('accuracy:', accuracy)\n",
    "    # print(\"f1_score:\", f1_score0)\n",
    "    # print(\"macro f1_score:\", f1_score1)\n",
    "    # print(\"micro f1_score:\", f1_score2)\n",
    "    # print(\"auc score:\", auc_score)\n",
    "\n",
    "    return pos_ratio, accuracy, f1_score0, f1_score1, f1_score2,  auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_emb(fpath, dataset):\n",
    "    dim = 0\n",
    "    embeddings = 0\n",
    "    with open(fpath) as f:\n",
    "        for i, line in enumerate(f.readlines()):\n",
    "            if i == 0:\n",
    "                ll = line.split()\n",
    "                assert len(ll) == 2, 'First line must be 2 numbers'\n",
    "                dim = int(ll[1])\n",
    "                embeddings = np.random.rand(DATASET_NUM_DIC[dataset], dim)\n",
    "            else:\n",
    "                line_l = line.split()\n",
    "                node = line_l[0]\n",
    "                emb = [float(j) for j in line_l[1:]]\n",
    "                embeddings[int(node)] = np.array(emb)\n",
    "                \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_embedding0(k=1, dataset='epinions'):\n",
    "    \"\"\"using random embedding to train logistic\n",
    "\n",
    "    Keyword Arguments:\n",
    "        k {int} -- [folder] (default: {1})\n",
    "        dataset {str} -- [dataset] (default: {'epinions'})\n",
    "\n",
    "    Returns:\n",
    "        [type] -- [pos_ratio, accuracy, f1_score0, f1_score1, f1_score2, auc_score]\n",
    "    \"\"\"\n",
    "    \n",
    "    print('random embeddings')\n",
    "    embeddings = np.random.rand(DATASET_NUM_DIC[dataset], EMBEDDING_SIZE)\n",
    "    pos_ratio, accuracy, f1_score0, f1_score1, f1_score2, auc_score = common_logistic(dataset, k, embeddings, 'random')\n",
    "    return pos_ratio, accuracy, f1_score0, f1_score1, f1_score2, auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_embedding1(k=1, dataset='epinions'):\n",
    "    \n",
    "    \"\"\"use deepwalk embeddings to train logistic function\n",
    "\n",
    "    Returns:\n",
    "        pos_ratio, accuracy, f1_score0, f1_score1, f1_score2, auc_score\n",
    "    \"\"\"\n",
    "\n",
    "    fpath = os.path.join(root + \"/\" + 'embeddings/deepwalk_emb', '{}-{}.emb'.format(dataset, k))\n",
    "    embeddings = read_emb(fpath, dataset)\n",
    "    pos_ratio, accuracy, f1_score0, f1_score1, f1_score2, auc_score = common_logistic(dataset, k, embeddings, 'deepwalk')\n",
    "    return pos_ratio, accuracy, f1_score0, f1_score1, f1_score2, auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_embedding2(k=1, dataset='epinions'):\n",
    "    \n",
    "    \"\"\"use node2vec embeddings to train logistic function\n",
    "    \n",
    "    Returns:\n",
    "        pos_ratio, accuracy, f1_score0, f1_score1, f1_score2, auc_score\n",
    "    \"\"\"\n",
    "\n",
    "    fpath = os.path.join(root + \"/\" + 'embeddings/node2vec_emb', '{}-{}.emb'.format(dataset, k))\n",
    "    embeddings = read_emb(fpath, dataset)\n",
    "    pos_ratio, accuracy, f1_score0, f1_score1, f1_score2, auc_score = common_logistic(dataset, k, embeddings, 'node2vec')\n",
    "    return pos_ratio, accuracy, f1_score0, f1_score1, f1_score2, auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_embedding3(k=1, dataset='epinions'):\n",
    "    \n",
    "    \"\"\"use line embeddings to train logistic function\n",
    "    Returns:\n",
    "        pos_ratio, accuracy, f1_score0, f1_score1, f1_score2, auc_score\n",
    "    \"\"\"\n",
    "    \n",
    "    fpath = os.path.join(root + \"/\" + 'embeddings/line_emb', '{}-{}.emb'.format(dataset, k))\n",
    "    embeddings = read_emb(fpath, dataset)\n",
    "    pos_ratio, accuracy, f1_score0, f1_score1, f1_score2, auc_score = common_logistic(dataset, k, embeddings, 'line')\n",
    "    return pos_ratio, accuracy, f1_score0, f1_score1, f1_score2, auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_embedding4(k=1, dataset='epinions', epoch=6, dirname='graphssa-results'):\n",
    "    \n",
    "    \"\"\"use graphssa to train logistic function\n",
    "    Returns:\n",
    "        pos_ratio, accuracy, f1_score0, f1_score1, f1_score2, auc_score\n",
    "    \"\"\"\n",
    "    \n",
    "    print('item: graphssa with feo', k, epoch)\n",
    "\n",
    "    filename = os.path.join(root + \"/\" + dirname, 'embedding-{}-{}-{}.npy'.format(dataset, k, epoch))\n",
    "    embeddings = np.load(filename)\n",
    "    pos_ratio, accuracy, f1_score0, f1_score1, f1_score2, auc_score = common_logistic(dataset, k, embeddings, 'graphssa')\n",
    "    return pos_ratio, accuracy, f1_score0, f1_score1, f1_score2, auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_embedding5(k=1, dataset='epinions', epoch=50, v0=True):\n",
    "    \n",
    "    \"\"\"use sine embeddings to train logistic function\n",
    "    Returns:\n",
    "        pos_ratio, accuracy, f1_score0, f1_score1, f1_score2, auc_score\n",
    "    \"\"\"\n",
    "\n",
    "    print('sine', k, 'v0', v0)\n",
    "    embeddings = []\n",
    "    if v0:\n",
    "        filename = os.path.join(SINE_MODEL_PATH_DIC[dataset], str(k) + 'a', str(epoch) + '.p')\n",
    "    else:\n",
    "        filename = os.path.join(SINE_MODEL_PATH_DIC[dataset], str(k) + 'b', str(epoch) + '.p')\n",
    "\n",
    "    # filename = os.path.join('./models/', str(epoch) + '.p')\n",
    "    print(filename)\n",
    "    params = \"\"\n",
    "    with open(filename, 'rb') as fp:\n",
    "        params = pickle.load(fp)\n",
    "        embeddings = params[0].get_value()\n",
    "    embeddings = embeddings[1:,]\n",
    "    print(embeddings.shape)\n",
    "    \n",
    "    pos_ratio, accuracy, f1_score0, f1_score1, f1_score2, auc_score = common_logistic(dataset, k, embeddings, 'sine')\n",
    "    return pos_ratio, accuracy, f1_score0, f1_score1, f1_score2, auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_embedding6(k=1, dataset='epinions', epoch=1):\n",
    "    \n",
    "    \"\"\"use side embeddings to train logistic function\n",
    "    Returns:\n",
    "        pos_ratio, accuracy, f1_score0, f1_score1, f1_score2, auc_score\n",
    "    \"\"\"\n",
    "    \n",
    "    def read_side_emb():\n",
    "        voc_path = os.path.join(root + \"/\" + 'embeddings/side', '{}{}.vocab'.format(dataset, k))\n",
    "        order_dict = defaultdict(int)\n",
    "        with open(voc_path) as f:\n",
    "            for index, line in enumerate(f.readlines()):\n",
    "                num = re.findall(r'b\\'(\\d+)\\'', line)\n",
    "                order_dict[index] = int(\"\".join(num))\n",
    "                \n",
    "        embeddings = np.zeros((DATASET_NUM_DIC[dataset], 50))\n",
    "        embed_path = os.path.join(root + \"/\" + 'embeddings/side', '{}{}{}.emb'.format(dataset, k, epoch))\n",
    "        \n",
    "        with open(embed_path) as f:\n",
    "            for i, line in enumerate(f.readlines()):\n",
    "                line_l = line.split()\n",
    "                emb = [np.float(j) for j in line_l]\n",
    "                embeddings[order_dict[i]] = np.array(emb)\n",
    "                \n",
    "        return embeddings\n",
    "\n",
    "    embeddings = read_side_emb()\n",
    "    pos_ratio, accuracy, f1_score0, f1_score1, f1_score2, auc_score = common_logistic(dataset, k, embeddings, 'side')\n",
    "    return pos_ratio, accuracy, f1_score0, f1_score1, f1_score2, auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_embedding7(k=1, dataset='epinions', dirname=\"sign2vec\"):\n",
    "    \n",
    "    \"\"\"use signet embeddings to train logistic function\n",
    "    Returns:\n",
    "        pos_ratio, accuracy, f1_score0, f1_score1, f1_score2, auc_score\n",
    "    \"\"\"\n",
    "    \n",
    "    print('signet', k, dataset)\n",
    "    filename = os.path.join(root + \"/\" + 'embeddings', dirname, 'embeddings-{}-{}.npy'.format(dataset, k))\n",
    "    embeddings = np.load(filename)\n",
    "    pos_ratio, accuracy, f1_score0, f1_score1, f1_score2, auc_score = common_logistic(dataset, k, embeddings, 'signet')\n",
    "    return pos_ratio, accuracy, f1_score0, f1_score1, f1_score2, auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_embedding8(k=1, dataset='epinions'):\n",
    "    \"\"\"use feature to train logistic function\n",
    "    Returns:\n",
    "        pos_ratio, accuracy, f1_score0, f1_score1, f1_score2, auc_score\n",
    "    \"\"\"\n",
    "    print(dataset, k, 'fea')\n",
    "    train_X, train_y, test_X, test_y  = read_train_test_data(dataset, k)\n",
    "    fea = FeaExtra(k=k, dataset=dataset)\n",
    "    train_X1 = []\n",
    "    test_X1 = []\n",
    "\n",
    "    for i, j in train_X:\n",
    "        train_X1.append(fea.get_features(i, j))\n",
    "\n",
    "    for i, j in test_X:\n",
    "        test_X1.append(fea.get_features(i, j))\n",
    "\n",
    "    logistic = linear_model.LogisticRegression()\n",
    "    logistic.fit(train_X1, train_y)\n",
    "\n",
    "    pred = logistic.predict(test_X1)\n",
    "    pred_p = logistic.predict_proba(test_X1)\n",
    "    pos_ratio =  np.sum(test_y) / test_y.shape[0]\n",
    "    accuracy =  metrics.accuracy_score(test_y, pred)\n",
    "    f1_score0 =  metrics.f1_score(test_y, pred)\n",
    "    f1_score1 =  metrics.f1_score(test_y, pred, average='macro')\n",
    "    f1_score2 =  metrics.f1_score(test_y, pred, average='micro')\n",
    "\n",
    "    auc_score =  metrics.roc_auc_score(test_y, pred_p[:, 1])\n",
    "    print(\"pos_ratio:\", pos_ratio)\n",
    "    print('accuracy:', accuracy)\n",
    "    print(\"f1_score:\", f1_score0)\n",
    "    print(\"macro f1_score:\", f1_score1)\n",
    "    print(\"micro f1_score:\", f1_score2)\n",
    "    print(\"auc score:\",auc_score)\n",
    "\n",
    "    return pos_ratio, accuracy, f1_score0, f1_score1, f1_score2, auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_embedding9(k=1, dataset='epinions', epoch=10, dirname='sigat'):\n",
    "    \"\"\"use sigat embedding to train logistic function\n",
    "    Returns:\n",
    "        pos_ratio, accuracy, f1_score0, f1_score1, f1_score2, auc_score\n",
    "    \"\"\"\n",
    "\n",
    "    filename = os.path.join(root + \"/\" + 'embeddings', dirname, 'embedding-{}-{}-{}.npy'.format(dataset, k, epoch))\n",
    "    embeddings = np.load(filename)\n",
    "    pos_ratio, accuracy, f1_score0, f1_score1, f1_score2, auc_score = common_logistic(dataset, k, embeddings, 'sigat')\n",
    "    return pos_ratio, accuracy, f1_score0, f1_score1, f1_score2, auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_embedding(k=1, dataset='bitcoin_otc', epoch = 10, dirname='sgae'):\n",
    "\n",
    "    print(epoch, dataset)\n",
    "    fpath = os.path.join(dirname, 'embedding-{}-{}-{}.npy'.format(dataset, k, epoch))\n",
    "    embeddings = np.load(fpath)\n",
    "    pos_ratio, accuracy, f1_score0, f1_score1, f1_score2, auc_score = common_logistic(dataset, k, embeddings, dirname)\n",
    "    return pos_ratio, accuracy, f1_score0, f1_score1, f1_score2, auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型组件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encode features to embeddings\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, features, feature_dim, embed_dim, adj_lists, aggs):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.features = features\n",
    "        self.feat_dim = feature_dim\n",
    "        self.adj_lists = adj_lists\n",
    "        self.aggs = aggs\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        for i, agg in enumerate(self.aggs):\n",
    "            self.add_module('agg_{}'.format(i), agg)\n",
    "            self.aggs[i] = agg.to(DEVICES)\n",
    "\n",
    "        def init_weights(m):\n",
    "            if type(m) == nn.Linear:\n",
    "                torch.nn.init.kaiming_normal_(m.weight)\n",
    "                fan_in, _ = nn.init._calculate_fan_in_and_fan_out(m.weight)\n",
    "                bound = 1 / math.sqrt(fan_in)\n",
    "                nn.init.uniform_(m.bias, -bound, bound)\n",
    "        self.nonlinear_layer = nn.Sequential(\n",
    "                nn.Linear((len(adj_lists) + 1) * feature_dim, feature_dim),  # motifs+本身\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(feature_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "        self.nonlinear_layer.apply(init_weights)  # 初始化\n",
    "\n",
    "\n",
    "    def forward(self, nodes):\n",
    "        \"\"\"\n",
    "        Generates embeddings for nodes.\n",
    "        \"\"\"\n",
    "\n",
    "        if not isinstance(nodes, list) and nodes.is_cuda:\n",
    "            nodes = nodes.data.cpu().numpy().tolist()\n",
    "\n",
    "        neigh_feats = [agg(nodes, adj, ind) for adj, agg, ind in zip(self.adj_lists, self.aggs, range(len(self.adj_lists)))]\n",
    "        self_feats = self.features(torch.LongTensor(nodes).to(DEVICES))  # 节点本身motifs的特征\n",
    "        combined = torch.cat([self_feats] + neigh_feats, 1)  # 邻居+节点本身\n",
    "        combined = self.nonlinear_layer(combined)\n",
    "        return combined\n",
    "\n",
    "        k = self.k(self_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionAggregator(nn.Module):\n",
    "    def __init__(self, features, in_dim, out_dim, node_num, dropout_rate=DROUPOUT, slope_ratio=0.1):\n",
    "        super(AttentionAggregator, self).__init__()\n",
    "\n",
    "        self.features = features\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.slope_ratio = slope_ratio\n",
    "        self.a = nn.Parameter(torch.FloatTensor(out_dim * 2, 1))  # Whi||Whj\n",
    "        nn.init.kaiming_normal_(self.a.data)\n",
    "\n",
    "        self.out_linear_layer = nn.Linear(self.in_dim, self.out_dim)  # W*H\n",
    "        self.unique_nodes_dict = np.zeros(node_num, dtype=np.int32)\n",
    "\n",
    "\n",
    "    def forward(self, nodes, adj, ind):\n",
    "        \"\"\"\n",
    "        nodes --- list of nodes in a batch\n",
    "        adj --- sp.csr_matrix\n",
    "        \"\"\"\n",
    "        node_pku = np.array(nodes)\n",
    "        edges = np.array(adj[nodes, :].nonzero()).T\n",
    "        edges[:, 0] = node_pku[edges[:, 0]]  # 将选择后的node对应到之前的node编号上\n",
    "\n",
    "        unique_nodes_list = np.unique(np.hstack((np.unique(edges), np.array(nodes))))\n",
    "\n",
    "        batch_node_num = len(unique_nodes_list)\n",
    "        # this dict can map new i to originial node id\n",
    "        self.unique_nodes_dict[unique_nodes_list] = np.arange(batch_node_num)  # 选择node的编号\n",
    "\n",
    "        edges[:, 0] = self.unique_nodes_dict[edges[:, 0]]\n",
    "        edges[:, 1] = self.unique_nodes_dict[edges[:, 1]]\n",
    "\n",
    "        n2 = torch.LongTensor(unique_nodes_list).to(DEVICES)  # 第一层涉及到的邻居\n",
    "        f = self.features(n2)\n",
    "        new_embeddings = self.out_linear_layer(f)  # self.features(n2)\n",
    "\n",
    "        original_node_edge = np.array([self.unique_nodes_dict[nodes], self.unique_nodes_dict[nodes]]).T\n",
    "        edges = np.vstack((edges, original_node_edge))  # 加上自连接的边\n",
    "\n",
    "        edges = torch.LongTensor(edges).to(DEVICES)\n",
    "\n",
    "        edge_h_2 = torch.cat((new_embeddings[edges[:, 0], :], new_embeddings[edges[:, 1], :]), dim=1)  # Whi||Whj\n",
    "\n",
    "        edges_h = torch.exp(F.leaky_relu(torch.einsum(\"ij,jl->il\", [edge_h_2, self.a]), self.slope_ratio))  # attention\n",
    "        indices = edges\n",
    "        \n",
    "        matrix = torch.sparse_coo_tensor(indices.t(), edges_h[:, 0], \\\n",
    "                                         torch.Size([batch_node_num, batch_node_num]), device=DEVICES)\n",
    "        row_sum = torch.sparse.mm(matrix, torch.ones(size=(batch_node_num, 1)).to(DEVICES))  # attention求和\n",
    "\n",
    "        results = torch.sparse.mm(matrix, new_embeddings)  # attention和embedding内积\n",
    "\n",
    "        output_emb = results.div(row_sum)  # 归一化\n",
    "\n",
    "        return output_emb[self.unique_nodes_dict[nodes]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanAggregator(nn.Module):\n",
    "    def __init__(self, features, in_dim, out_dim, node_num):\n",
    "        super(MeanAggregator, self).__init__()\n",
    "\n",
    "        self.features = features\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.out_linear_layer = nn.Sequential(\n",
    "            nn.Linear(self.in_dim, self.out_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.out_dim, self.out_dim)\n",
    "        )\n",
    "\n",
    "        self.unique_nodes_dict = np.zeros(node_num, dtype=np.int32)\n",
    "\n",
    "    def forward(self, nodes, adj, ind):\n",
    "        \"\"\"\n",
    "\n",
    "        :param nodes:\n",
    "        :param adj:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        mask = [1, 1, 0, 0]\n",
    "        node_tmp = np.array(nodes)\n",
    "        edges = np.array(adj[nodes, :].nonzero()).T\n",
    "        edges[:, 0] = node_tmp[edges[:, 0]]\n",
    "\n",
    "        unique_nodes_list = np.unique(np.hstack((np.unique(edges), np.array(nodes))))\n",
    "        batch_node_num = len(unique_nodes_list)\n",
    "        self.unique_nodes_dict[unique_nodes_list] = np.arange(batch_node_num)\n",
    "\n",
    "        ## transform 2 new axis\n",
    "        edges[:, 0] = self.unique_nodes_dict[edges[:, 0]]\n",
    "        edges[:, 1] = self.unique_nodes_dict[edges[:, 1]]\n",
    "\n",
    "        n2 = torch.LongTensor(unique_nodes_list).to(DEVICES)\n",
    "        new_embeddings = self.out_linear_layer(self.features(n2))\n",
    "        edges = torch.LongTensor(edges).to(DEVICES)\n",
    "\n",
    "        values = torch.where(edges[:, 0] == edges[:, 1], torch.FloatTensor([mask[ind]]).to(DEVICES), torch.FloatTensor([1]).to(DEVICES))\n",
    "        # values = torch.ones(edges.shape[0]).to(DEVICES)\n",
    "        matrix = torch.sparse_coo_tensor(edges.t(), values, torch.Size([batch_node_num, batch_node_num]), device=DEVICES)\n",
    "        row_sum = torch.spmm(matrix, torch.ones(size=(batch_node_num, 1)).to(DEVICES))\n",
    "        row_sum = torch.where(row_sum == 0, torch.ones(row_sum.shape).to(DEVICES), row_sum)\n",
    "\n",
    "        results = torch.spmm(matrix, new_embeddings)\n",
    "        output_emb = results.div(row_sum)\n",
    "\n",
    "        return output_emb[self.unique_nodes_dict[nodes]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SDGNN(nn.Module):\n",
    "    def __init__(self, enc):\n",
    "        super(SDGNN, self).__init__()\n",
    "        self.enc = enc\n",
    "        self.score_function1 = nn.Sequential(  # 定义status ranking\n",
    "            nn.Linear(EMBEDDING_SIZE1, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.score_function2 = nn.Sequential(\n",
    "            nn.Linear(EMBEDDING_SIZE1, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.fc = nn.Linear(EMBEDDING_SIZE1 * 2, 1)\n",
    "\n",
    "    def forward(self, nodes):\n",
    "        embeds = self.enc(nodes)\n",
    "        return embeds\n",
    "\n",
    "    def criterion(self, nodes, pos_neighbors, neg_neighbors, adj_lists1_1, adj_lists2_1, weight_dict):\n",
    "        pos_neighbors_list = [set.union(pos_neighbors[i]) for i in nodes]  # pos的邻居\n",
    "        neg_neighbors_list = [set.union(neg_neighbors[i]) for i in nodes]  # neg的邻居\n",
    "        unique_nodes_list = list(set.union(*pos_neighbors_list).union(*neg_neighbors_list).union(nodes))  # 所有涉及到的节点\n",
    "        unique_nodes_dict = {n: i for i, n in enumerate(unique_nodes_list)}  # 节点编号\n",
    "        nodes_embs = self.enc(unique_nodes_list)\n",
    "\n",
    "        loss_total = 0\n",
    "        for index, node in enumerate(nodes):\n",
    "            z1 = nodes_embs[unique_nodes_dict[node], :]  # 节点的embedding\n",
    "            pos_neigs = list([unique_nodes_dict[i] for i in pos_neighbors[node]])  # 节点的positive邻居\n",
    "            neg_neigs = list([unique_nodes_dict[i] for i in neg_neighbors[node]])  # 节点的negative邻居\n",
    "            pos_num = len(pos_neigs)\n",
    "            neg_num = len(neg_neigs)\n",
    "\n",
    "            sta_pos_neighs = list([unique_nodes_dict[i] for i in adj_lists1_1[node]])  # pos:u->v 对应的index\n",
    "            sta_neg_neighs = list([unique_nodes_dict[i] for i in adj_lists2_1[node]])  # neg:u->v\n",
    "            # 平衡三角形weight\n",
    "            pos_neigs_weight = torch.FloatTensor([weight_dict[node][i] for i in adj_lists1_1[node]]).to(DEVICES)  # u->v之间满足平衡三角形weight\n",
    "            neg_neigs_weight = torch.FloatTensor([weight_dict[node][i] for i in adj_lists2_1[node]]).to(DEVICES)\n",
    "\n",
    "            if pos_num > 0:\n",
    "                pos_neig_embs = nodes_embs[pos_neigs, :]  # pos节点embedding\n",
    "                loss_pku = F.binary_cross_entropy_with_logits(torch.einsum(\"nj,j->n\", [pos_neig_embs, z1]),\n",
    "                                                              torch.ones(pos_num).to(DEVICES))\n",
    "\n",
    "                if len(sta_pos_neighs) > 0:\n",
    "                    sta_pos_neig_embs = nodes_embs[sta_pos_neighs, :]  # pos的u->v连接embedding\n",
    "\n",
    "                    z11 = z1.repeat(len(sta_pos_neighs), 1)  # 重复n次\n",
    "                    rs = self.fc(torch.cat([z11, sta_pos_neig_embs], 1)).squeeze(-1)  # Z1||pos_emb -> 1\n",
    "                    loss_pku += F.binary_cross_entropy_with_logits(rs, torch.ones(len(sta_pos_neighs)).to(DEVICES), \\\n",
    "                                                                   weight=pos_neigs_weight\n",
    "                                                                   )  # 边类别预测\n",
    "                    s1 = self.score_function1(z1).repeat(len(sta_pos_neighs), 1)  # z1重复n次\n",
    "                    s2 = self.score_function2(sta_pos_neig_embs)\n",
    "\n",
    "                    q = torch.where((s1 - s2) > -0.5, torch.Tensor([-0.5]).repeat(s1.shape).to(DEVICES), s1 - s2)  # !!!! 0.5\n",
    "                    tmp = (q - (s1 - s2))\n",
    "                    loss_pku += 5 * torch.einsum(\"ij,ij->\", [tmp, tmp])  # ^2\n",
    "\n",
    "                loss_total += loss_pku\n",
    "\n",
    "            if neg_num > 0:\n",
    "                neg_neig_embs = nodes_embs[neg_neigs, :]\n",
    "                loss_pku = F.binary_cross_entropy_with_logits(torch.einsum(\"nj,j->n\", [neg_neig_embs, z1]),\n",
    "                                                              torch.zeros(neg_num).to(DEVICES))\n",
    "                if len(sta_neg_neighs) > 0:\n",
    "                    sta_neg_neig_embs = nodes_embs[sta_neg_neighs, :]\n",
    "\n",
    "                    z12 = z1.repeat(len(sta_neg_neighs), 1)\n",
    "                    rs = self.fc(torch.cat([z12, sta_neg_neig_embs], 1)).squeeze(-1)\n",
    "\n",
    "                    loss_pku += F.binary_cross_entropy_with_logits(rs, torch.zeros(len(sta_neg_neighs)).to(DEVICES), \\\n",
    "                                                                   weight=neg_neigs_weight)\n",
    "\n",
    "                    s1 = self.score_function1(z1).repeat(len(sta_neg_neighs), 1)\n",
    "                    s2 = self.score_function2(sta_neg_neig_embs)\n",
    "\n",
    "                    q = torch.where(s1 - s2 > 0.5, s1 - s2, torch.Tensor([0.5]).repeat(s1.shape).to(DEVICES))\n",
    "\n",
    "                    tmp = (q - (s1 - s2))\n",
    "                    loss_pku += 5 * torch.einsum(\"ij,ij->\", [tmp, tmp])\n",
    "\n",
    "                loss_total += loss_pku\n",
    "\n",
    "        return loss_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data2(filename=''):\n",
    "    \n",
    "    adj_lists1 = defaultdict(set)\n",
    "    adj_lists1_1 = defaultdict(set)\n",
    "    adj_lists1_2 = defaultdict(set)\n",
    "    adj_lists2 = defaultdict(set)\n",
    "    adj_lists2_1 = defaultdict(set)\n",
    "    adj_lists2_2 = defaultdict(set)\n",
    "    adj_lists3 = defaultdict(set)\n",
    "\n",
    "    with open(filename) as fp:\n",
    "        for i, line in enumerate(fp):\n",
    "            info = line.strip().split()\n",
    "            person1 = int(info[0])\n",
    "            person2 = int(info[1])\n",
    "            v = int(info[2])\n",
    "            adj_lists3[person2].add(person1)\n",
    "            adj_lists3[person1].add(person2)\n",
    "\n",
    "            if v == 1:\n",
    "                adj_lists1[person1].add(person2)\n",
    "                adj_lists1[person2].add(person1)\n",
    "\n",
    "                adj_lists1_1[person1].add(person2)\n",
    "                adj_lists1_2[person2].add(person1)\n",
    "            else:\n",
    "                adj_lists2[person1].add(person2)\n",
    "                adj_lists2[person2].add(person1)\n",
    "\n",
    "                adj_lists2_1[person1].add(person2)\n",
    "                adj_lists2_2[person2].add(person1)\n",
    "\n",
    "    return adj_lists1, adj_lists1_1, adj_lists1_2, adj_lists2, adj_lists2_1, adj_lists2_2, adj_lists3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(dataset, k):\n",
    "    num_nodes = DATASET_NUM_DIC[dataset] + 3\n",
    "\n",
    "    # adj_lists1, adj_lists2, adj_lists3 = load_data(k, dataset)\n",
    "    filename = root + '/experiment-data/{}-train-{}.edgelist'.format(dataset, k)\n",
    "    adj_lists1, adj_lists1_1, adj_lists1_2, adj_lists2, adj_lists2_1, adj_lists2_2, adj_lists3 = load_data2(filename)\n",
    "    print(k, dataset, 'data load!')\n",
    "\n",
    "    features = nn.Embedding(num_nodes, NODE_FEAT_SIZE)\n",
    "    features.weight.requires_grad = True\n",
    "\n",
    "    features = features.to(DEVICES)\n",
    "    # 有向的四个边; 4个motifs\n",
    "    adj_lists = [adj_lists1_1, adj_lists1_2,  adj_lists2_1, adj_lists2_2]\n",
    "\n",
    "  \n",
    "    weight_dict = defaultdict(dict)\n",
    "    fea_model = FeaExtra(dataset=dataset, k=k)\n",
    "    # # u -> v\n",
    "    for i in adj_lists1_1:\n",
    "        for j in adj_lists1_1[i]:\n",
    "            v_list1 = fea_model.feature_part2(i, j)\n",
    "            mask = [1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1]\n",
    "            counts1 = np.dot(v_list1, mask)  # 构造balance三角形，在后面计算loss时候使用;\n",
    "            weight_dict[i][j] = counts1\n",
    "\n",
    "    for i in adj_lists2_1:\n",
    "        for j in adj_lists2_1[i]:\n",
    "            v_list1 = fea_model.feature_part2(i, j)\n",
    "            mask = [0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0]\n",
    "\n",
    "            counts1 = np.dot(v_list1, mask)\n",
    "            weight_dict[i][j] = counts1\n",
    "\n",
    "    adj_lists = adj_lists\n",
    "\n",
    "\n",
    "    print(len(adj_lists), 'motifs')\n",
    "\n",
    "    def func(adj_list):\n",
    "        edges = []\n",
    "        for a in adj_list:\n",
    "            for b in adj_list[a]:\n",
    "                edges.append((a, b))\n",
    "        edges = np.array(edges)\n",
    "        adj = sp.csr_matrix((np.ones(len(edges)), (edges[:, 0], edges[:, 1])), shape=(num_nodes, num_nodes))\n",
    "        return adj\n",
    "\n",
    "    if args.agg == 'mean':\n",
    "        aggregator = MeanAggregator\n",
    "    else:\n",
    "        aggregator = AttentionAggregator\n",
    "\n",
    "    adj_lists = list(map(func, adj_lists))\n",
    "    aggs = [aggregator(features, NODE_FEAT_SIZE, NODE_FEAT_SIZE, num_nodes) for adj in adj_lists]  # 构建motifs下节点的邻居聚合\n",
    "    enc1 = Encoder(features, NODE_FEAT_SIZE, EMBEDDING_SIZE1, adj_lists, aggs)  # 4个motifs+本身节点 + cls => embedding\n",
    "    enc1 = enc1.to(DEVICES)\n",
    "\n",
    "\n",
    "    aggs2 = [aggregator(lambda n: enc1(n), EMBEDDING_SIZE1, EMBEDDING_SIZE1, num_nodes) for _ in adj_lists]\n",
    "    enc2 = Encoder(lambda n: enc1(n), EMBEDDING_SIZE1, EMBEDDING_SIZE1, adj_lists, aggs2)\n",
    "\n",
    "    model = SDGNN(enc2)\n",
    "    model = model.to(DEVICES)\n",
    "\n",
    "    # print(model.train())\n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad,\n",
    "                                        list(model.parameters()) + list(enc1.parameters()) \\\n",
    "                                        + list(features.parameters())),\n",
    "                                 lr=LEARNING_RATE,\n",
    "                                 weight_decay=WEIGHT_DECAY\n",
    "                                 )\n",
    "\n",
    "    for epoch in range(EPOCHS + 2):\n",
    "        total_loss = []\n",
    "        if epoch % INTERVAL_PRINT == 1:\n",
    "            model.eval()\n",
    "            all_embedding = np.zeros((NUM_NODE, EMBEDDING_SIZE1))\n",
    "            for i in range(0, NUM_NODE, BATCH_SIZE):\n",
    "                begin_index = i\n",
    "                end_index = i + BATCH_SIZE if i + BATCH_SIZE < NUM_NODE else NUM_NODE\n",
    "                values = np.arange(begin_index, end_index)\n",
    "                embed = model.forward(values.tolist())\n",
    "                embed = embed.data.cpu().numpy()\n",
    "                all_embedding[begin_index: end_index] = embed\n",
    "\n",
    "            fpath = os.path.join(OUTPUT_DIR, 'embedding-{}-{}-{}.npy'.format(dataset, k, str(epoch)))\n",
    "            np.save(fpath, all_embedding)\n",
    "            pos_ratio, accuracy, f1_score0, f1_score1, f1_score2, auc_score = logistic_embedding(k=k, dataset=dataset,\n",
    "                                                                                                 epoch=epoch,\n",
    "                                                                                                dirname=OUTPUT_DIR)\n",
    "            model.train()\n",
    "\n",
    "        time1 = time.time()\n",
    "        nodes_pku = np.random.permutation(NUM_NODE).tolist()\n",
    "        for batch in range(NUM_NODE // BATCH_SIZE):\n",
    "            optimizer.zero_grad()\n",
    "            b_index = batch * BATCH_SIZE\n",
    "            e_index = (batch + 1) * BATCH_SIZE\n",
    "            nodes = nodes_pku[b_index:e_index]\n",
    "\n",
    "            loss = model.criterion(\n",
    "                nodes, adj_lists1, adj_lists2, adj_lists1_1, adj_lists2_1, weight_dict  # i,j节点的平衡三角形数量\n",
    "            )\n",
    "            total_loss.append(loss.data.cpu().numpy())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f'epoch: {epoch}, loss: {np.mean(total_loss)}, time: {time.time()-time1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print('NUM_NODE', NUM_NODE)\n",
    "    print('WEIGHT_DECAY', WEIGHT_DECAY)\n",
    "    print('NODE_FEAT_SIZE', NODE_FEAT_SIZE)\n",
    "    print('EMBEDDING_SIZE1', EMBEDDING_SIZE1)\n",
    "    print('LEARNING_RATE', LEARNING_RATE)\n",
    "    print('BATCH_SIZE', BATCH_SIZE)\n",
    "    print('EPOCHS', EPOCHS)\n",
    "    print('DROUPOUT', DROUPOUT)\n",
    "    \n",
    "    print(\"--\" * 20)\n",
    "    \n",
    "    dataset = args.dataset\n",
    "    run(dataset=dataset, k=K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_NODE 3783\n",
      "WEIGHT_DECAY 0.001\n",
      "NODE_FEAT_SIZE 20\n",
      "EMBEDDING_SIZE1 20\n",
      "LEARNING_RATE 0.001\n",
      "BATCH_SIZE 500\n",
      "EPOCHS 100\n",
      "DROUPOUT 0.0\n",
      "----------------------------------------\n",
      "1 bitcoin_alpha data load!\n",
      "4 motifs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sss\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\optim\\adam.py:48: UserWarning: optimizer contains a parameter group with duplicate parameters; in future, this will cause an error; see github.com/pytorch/pytorch/issues/40967 for more information\n",
      "  super(Adam, self).__init__(params, defaults)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: 2969.317138671875, time: 17.703672170639038\n",
      "1 bitcoin_alpha\n",
      "epoch: 1, loss: 2110.322998046875, time: 17.17906427383423\n",
      "epoch: 2, loss: 1721.8831787109375, time: 17.487241744995117\n",
      "3 bitcoin_alpha\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-34-ad45531a651f>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-31-1918699b3e5d>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(dataset, k)\u001b[0m\n\u001b[0;32m    105\u001b[0m             \u001b[0mtotal_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'epoch: {epoch}, loss: {np.mean(total_loss)}, time: {time.time()-time1}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sss\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sss\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
