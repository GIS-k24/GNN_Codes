{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))  #  矩阵行求和\n",
    "    r_inv = np.power(rowsum, -1).flatten()  # 求和的-1次方\n",
    "    r_inv[np.isinf(r_inv)] = 0.   # 如果是inf，转换成0\n",
    "    r_mat_inv = sp.diags(r_inv)  # 构造对角戏矩阵\n",
    "    mx = r_mat_inv.dot(mx)  # 构造D-1*A，非对称方式，简化方式\n",
    "    return mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path=r\"C:\\Users\\sss\\Desktop\\cora/\", dataset=\"cora\"):\n",
    "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset),\n",
    "                                        dtype=np.dtype(str))\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)  # 取特征feature\n",
    "    labels = encode_onehot(idx_features_labels[:, -1])  # one-hot label\n",
    "\n",
    "    # build graph\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)  # 节点\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}   # 构建节点的索引字典\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset),  # 导入edge的数据\n",
    "                                    dtype=np.int32)\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())),\n",
    "                     dtype=np.int32).reshape(edges_unordered.shape)    # 将之前的转换成字典编号后的边\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])),  # 构建边的邻接矩阵\n",
    "                        shape=(labels.shape[0], labels.shape[0]),\n",
    "                        dtype=np.float32)\n",
    "\n",
    "    # build symmetric adjacency matrix，计算转置矩阵。将有向图转成无向图\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    features = normalize(features)   # 对特征做了归一化的操作\n",
    "    adj = normalize(adj + sp.eye(adj.shape[0]))   # 对A+I归一化\n",
    "    \n",
    "    # 训练，验证，测试的样本\n",
    "    idx_train = range(140)\n",
    "    idx_val = range(200, 500)\n",
    "    idx_test = range(500, 1500)\n",
    "    # 将numpy的数据转换成torch格式\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "    adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    }
   ],
   "source": [
    "adj, features, labels, idx_train, idx_val, idx_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 特征 \t # 标签 \t # 矩阵关系\n",
      "torch.Size([2708, 2708]) \t torch.Size([2708, 1433]) \t torch.Size([2708])\n"
     ]
    }
   ],
   "source": [
    "print(\"# 特征 \\t # 标签 \\t # 矩阵关系\")\n",
    "print(adj.shape, \"\\t\", features.shape, \"\\t\", labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))  # input_features, out_features\n",
    "        if bias:\n",
    "            self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "#         标准化\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "        self.weight.data.uniform_(-stdv, stdv)  # 随机化参数\n",
    "        if self.bias is not None:\n",
    "            self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = torch.mm(input, self.weight)  # GraphConvolution forward。input*weight\n",
    "        output = torch.spmm(adj, support)  # 稀疏矩阵的相乘，和mm一样的效果\n",
    "        if self.bias is not None:\n",
    "            return output + self.bias\n",
    "        else:\n",
    "            return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        self.gc1 = GraphConvolution(nfeat, nhid) # 构建第一层 GCN\n",
    "        self.gc2 = GraphConvolution(nhid, nclass) # 构建第二层 GCN\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.gc2(x, adj)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = 16\n",
    "dropout = 0.5\n",
    "\n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=hidden,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "cuda = not False and torch.cuda.is_available()\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cuda:\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    adj = adj.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练和测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad() # GraphConvolution forward\n",
    "    # 重点输出 ！！！\n",
    "    output = model(features, adj)   # 运行模型，输入参数 (features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if not False:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        model.eval()\n",
    "        output = model(features, adj)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 1.9396 acc_train: 0.1714 loss_val: 1.9192 acc_val: 0.1567 time: 0.0558s\n",
      "Epoch: 0002 loss_train: 1.9019 acc_train: 0.2357 loss_val: 1.9034 acc_val: 0.1567 time: 0.0219s\n",
      "Epoch: 0003 loss_train: 1.8979 acc_train: 0.2571 loss_val: 1.8888 acc_val: 0.1567 time: 0.0840s\n",
      "Epoch: 0004 loss_train: 1.8899 acc_train: 0.2500 loss_val: 1.8750 acc_val: 0.1600 time: 0.0219s\n",
      "Epoch: 0005 loss_train: 1.8752 acc_train: 0.2286 loss_val: 1.8621 acc_val: 0.2400 time: 0.0219s\n",
      "Epoch: 0006 loss_train: 1.8578 acc_train: 0.3571 loss_val: 1.8497 acc_val: 0.4067 time: 0.0219s\n",
      "Epoch: 0007 loss_train: 1.8447 acc_train: 0.3429 loss_val: 1.8380 acc_val: 0.4600 time: 0.0229s\n",
      "Epoch: 0008 loss_train: 1.8377 acc_train: 0.3500 loss_val: 1.8268 acc_val: 0.4567 time: 0.0259s\n",
      "Epoch: 0009 loss_train: 1.8414 acc_train: 0.2714 loss_val: 1.8164 acc_val: 0.4233 time: 0.0289s\n",
      "Epoch: 0010 loss_train: 1.8040 acc_train: 0.3714 loss_val: 1.8066 acc_val: 0.3767 time: 0.0239s\n",
      "Epoch: 0011 loss_train: 1.7967 acc_train: 0.3500 loss_val: 1.7973 acc_val: 0.3667 time: 0.0249s\n",
      "Epoch: 0012 loss_train: 1.7899 acc_train: 0.3929 loss_val: 1.7883 acc_val: 0.3633 time: 0.0239s\n",
      "Epoch: 0013 loss_train: 1.7894 acc_train: 0.3500 loss_val: 1.7798 acc_val: 0.3633 time: 0.0269s\n",
      "Epoch: 0014 loss_train: 1.7675 acc_train: 0.3429 loss_val: 1.7718 acc_val: 0.3633 time: 0.0239s\n",
      "Epoch: 0015 loss_train: 1.7769 acc_train: 0.3143 loss_val: 1.7643 acc_val: 0.3633 time: 0.0259s\n",
      "Epoch: 0016 loss_train: 1.7556 acc_train: 0.3214 loss_val: 1.7572 acc_val: 0.3633 time: 0.0269s\n",
      "Epoch: 0017 loss_train: 1.7694 acc_train: 0.3357 loss_val: 1.7508 acc_val: 0.3633 time: 0.0229s\n",
      "Epoch: 0018 loss_train: 1.7511 acc_train: 0.3357 loss_val: 1.7447 acc_val: 0.3633 time: 0.0229s\n",
      "Epoch: 0019 loss_train: 1.7303 acc_train: 0.4000 loss_val: 1.7387 acc_val: 0.3633 time: 0.0239s\n",
      "Epoch: 0020 loss_train: 1.7076 acc_train: 0.3714 loss_val: 1.7329 acc_val: 0.3633 time: 0.0209s\n",
      "Epoch: 0021 loss_train: 1.7161 acc_train: 0.3071 loss_val: 1.7270 acc_val: 0.3633 time: 0.0219s\n",
      "Epoch: 0022 loss_train: 1.7018 acc_train: 0.3429 loss_val: 1.7212 acc_val: 0.3633 time: 0.0249s\n",
      "Epoch: 0023 loss_train: 1.7052 acc_train: 0.3929 loss_val: 1.7155 acc_val: 0.3633 time: 0.0229s\n",
      "Epoch: 0024 loss_train: 1.7241 acc_train: 0.3071 loss_val: 1.7098 acc_val: 0.3633 time: 0.0249s\n",
      "Epoch: 0025 loss_train: 1.6680 acc_train: 0.3929 loss_val: 1.7041 acc_val: 0.3633 time: 0.0229s\n",
      "Epoch: 0026 loss_train: 1.6929 acc_train: 0.3429 loss_val: 1.6983 acc_val: 0.3667 time: 0.0259s\n",
      "Epoch: 0027 loss_train: 1.6565 acc_train: 0.3786 loss_val: 1.6921 acc_val: 0.3667 time: 0.0309s\n",
      "Epoch: 0028 loss_train: 1.6521 acc_train: 0.3786 loss_val: 1.6859 acc_val: 0.3700 time: 0.0279s\n",
      "Epoch: 0029 loss_train: 1.6144 acc_train: 0.4071 loss_val: 1.6796 acc_val: 0.3767 time: 0.0319s\n",
      "Epoch: 0030 loss_train: 1.6258 acc_train: 0.4071 loss_val: 1.6733 acc_val: 0.3867 time: 0.0299s\n",
      "Epoch: 0031 loss_train: 1.6007 acc_train: 0.4143 loss_val: 1.6668 acc_val: 0.4000 time: 0.0269s\n",
      "Epoch: 0032 loss_train: 1.5950 acc_train: 0.4214 loss_val: 1.6601 acc_val: 0.4100 time: 0.0289s\n",
      "Epoch: 0033 loss_train: 1.6300 acc_train: 0.4143 loss_val: 1.6530 acc_val: 0.4100 time: 0.0429s\n",
      "Epoch: 0034 loss_train: 1.5929 acc_train: 0.4286 loss_val: 1.6454 acc_val: 0.4133 time: 0.0319s\n",
      "Epoch: 0035 loss_train: 1.5895 acc_train: 0.4071 loss_val: 1.6375 acc_val: 0.4200 time: 0.0329s\n",
      "Epoch: 0036 loss_train: 1.5568 acc_train: 0.4357 loss_val: 1.6291 acc_val: 0.4333 time: 0.0279s\n",
      "Epoch: 0037 loss_train: 1.5599 acc_train: 0.4357 loss_val: 1.6200 acc_val: 0.4467 time: 0.0309s\n",
      "Epoch: 0038 loss_train: 1.5544 acc_train: 0.4429 loss_val: 1.6105 acc_val: 0.4467 time: 0.0399s\n",
      "Epoch: 0039 loss_train: 1.5188 acc_train: 0.4571 loss_val: 1.6007 acc_val: 0.4467 time: 0.0319s\n",
      "Epoch: 0040 loss_train: 1.5086 acc_train: 0.4500 loss_val: 1.5901 acc_val: 0.4467 time: 0.0598s\n",
      "Epoch: 0041 loss_train: 1.4738 acc_train: 0.4714 loss_val: 1.5794 acc_val: 0.4433 time: 0.0349s\n",
      "Epoch: 0042 loss_train: 1.4769 acc_train: 0.4714 loss_val: 1.5683 acc_val: 0.4433 time: 0.0319s\n",
      "Epoch: 0043 loss_train: 1.4767 acc_train: 0.4714 loss_val: 1.5572 acc_val: 0.4500 time: 0.0309s\n",
      "Epoch: 0044 loss_train: 1.4536 acc_train: 0.5000 loss_val: 1.5457 acc_val: 0.4633 time: 0.0309s\n",
      "Epoch: 0045 loss_train: 1.4347 acc_train: 0.5214 loss_val: 1.5340 acc_val: 0.4767 time: 0.0329s\n",
      "Epoch: 0046 loss_train: 1.4141 acc_train: 0.5143 loss_val: 1.5222 acc_val: 0.4900 time: 0.0349s\n",
      "Epoch: 0047 loss_train: 1.4347 acc_train: 0.5143 loss_val: 1.5097 acc_val: 0.4933 time: 0.0349s\n",
      "Epoch: 0048 loss_train: 1.3887 acc_train: 0.5071 loss_val: 1.4971 acc_val: 0.5033 time: 0.0319s\n",
      "Epoch: 0049 loss_train: 1.3881 acc_train: 0.5000 loss_val: 1.4842 acc_val: 0.5167 time: 0.0359s\n",
      "Epoch: 0050 loss_train: 1.3746 acc_train: 0.5429 loss_val: 1.4712 acc_val: 0.5300 time: 0.0319s\n",
      "Epoch: 0051 loss_train: 1.3430 acc_train: 0.5571 loss_val: 1.4577 acc_val: 0.5467 time: 0.0369s\n",
      "Epoch: 0052 loss_train: 1.2911 acc_train: 0.5786 loss_val: 1.4443 acc_val: 0.5667 time: 0.0459s\n",
      "Epoch: 0053 loss_train: 1.2881 acc_train: 0.5857 loss_val: 1.4310 acc_val: 0.5767 time: 0.0319s\n",
      "Epoch: 0054 loss_train: 1.3216 acc_train: 0.6071 loss_val: 1.4175 acc_val: 0.5967 time: 0.0399s\n",
      "Epoch: 0055 loss_train: 1.2514 acc_train: 0.6214 loss_val: 1.4039 acc_val: 0.6133 time: 0.0369s\n",
      "Epoch: 0056 loss_train: 1.2696 acc_train: 0.6571 loss_val: 1.3899 acc_val: 0.6200 time: 0.0319s\n",
      "Epoch: 0057 loss_train: 1.2978 acc_train: 0.5857 loss_val: 1.3755 acc_val: 0.6267 time: 0.0299s\n",
      "Epoch: 0058 loss_train: 1.2313 acc_train: 0.6714 loss_val: 1.3609 acc_val: 0.6333 time: 0.0319s\n",
      "Epoch: 0059 loss_train: 1.1881 acc_train: 0.6786 loss_val: 1.3461 acc_val: 0.6300 time: 0.0449s\n",
      "Epoch: 0060 loss_train: 1.1928 acc_train: 0.6786 loss_val: 1.3317 acc_val: 0.6400 time: 0.0259s\n",
      "Epoch: 0061 loss_train: 1.1601 acc_train: 0.6929 loss_val: 1.3178 acc_val: 0.6433 time: 0.0249s\n",
      "Epoch: 0062 loss_train: 1.1141 acc_train: 0.7214 loss_val: 1.3043 acc_val: 0.6500 time: 0.0279s\n",
      "Epoch: 0063 loss_train: 1.1340 acc_train: 0.7357 loss_val: 1.2913 acc_val: 0.6600 time: 0.0249s\n",
      "Epoch: 0064 loss_train: 1.1110 acc_train: 0.7500 loss_val: 1.2783 acc_val: 0.6767 time: 0.0229s\n",
      "Epoch: 0065 loss_train: 1.0547 acc_train: 0.7214 loss_val: 1.2656 acc_val: 0.6800 time: 0.0239s\n",
      "Epoch: 0066 loss_train: 1.0840 acc_train: 0.7214 loss_val: 1.2533 acc_val: 0.6833 time: 0.0249s\n",
      "Epoch: 0067 loss_train: 1.0656 acc_train: 0.7571 loss_val: 1.2411 acc_val: 0.6900 time: 0.0239s\n",
      "Epoch: 0068 loss_train: 1.0592 acc_train: 0.7500 loss_val: 1.2291 acc_val: 0.6933 time: 0.0329s\n",
      "Epoch: 0069 loss_train: 1.0657 acc_train: 0.7429 loss_val: 1.2173 acc_val: 0.7000 time: 0.0259s\n",
      "Epoch: 0070 loss_train: 1.0407 acc_train: 0.7357 loss_val: 1.2051 acc_val: 0.7000 time: 0.0618s\n",
      "Epoch: 0071 loss_train: 0.9858 acc_train: 0.7786 loss_val: 1.1930 acc_val: 0.7167 time: 0.0229s\n",
      "Epoch: 0072 loss_train: 1.0196 acc_train: 0.7714 loss_val: 1.1809 acc_val: 0.7167 time: 0.0219s\n",
      "Epoch: 0073 loss_train: 1.0360 acc_train: 0.7857 loss_val: 1.1690 acc_val: 0.7267 time: 0.0239s\n",
      "Epoch: 0074 loss_train: 1.0376 acc_train: 0.7214 loss_val: 1.1575 acc_val: 0.7300 time: 0.0239s\n",
      "Epoch: 0075 loss_train: 0.9137 acc_train: 0.7929 loss_val: 1.1469 acc_val: 0.7333 time: 0.0249s\n",
      "Epoch: 0076 loss_train: 0.9565 acc_train: 0.7857 loss_val: 1.1368 acc_val: 0.7333 time: 0.0279s\n",
      "Epoch: 0077 loss_train: 0.9808 acc_train: 0.7786 loss_val: 1.1273 acc_val: 0.7500 time: 0.0229s\n",
      "Epoch: 0078 loss_train: 0.9197 acc_train: 0.8071 loss_val: 1.1180 acc_val: 0.7567 time: 0.0219s\n",
      "Epoch: 0079 loss_train: 0.9213 acc_train: 0.7929 loss_val: 1.1094 acc_val: 0.7567 time: 0.0229s\n",
      "Epoch: 0080 loss_train: 0.9150 acc_train: 0.8429 loss_val: 1.1021 acc_val: 0.7600 time: 0.0239s\n",
      "Epoch: 0081 loss_train: 0.8778 acc_train: 0.8071 loss_val: 1.0951 acc_val: 0.7633 time: 0.0319s\n",
      "Epoch: 0082 loss_train: 0.8743 acc_train: 0.8143 loss_val: 1.0869 acc_val: 0.7633 time: 0.0289s\n",
      "Epoch: 0083 loss_train: 0.8749 acc_train: 0.7714 loss_val: 1.0774 acc_val: 0.7667 time: 0.0259s\n",
      "Epoch: 0084 loss_train: 0.8574 acc_train: 0.8071 loss_val: 1.0686 acc_val: 0.7700 time: 0.0249s\n",
      "Epoch: 0085 loss_train: 0.8234 acc_train: 0.8357 loss_val: 1.0602 acc_val: 0.7700 time: 0.0229s\n",
      "Epoch: 0086 loss_train: 0.8608 acc_train: 0.8571 loss_val: 1.0520 acc_val: 0.7733 time: 0.0289s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0087 loss_train: 0.8495 acc_train: 0.8071 loss_val: 1.0435 acc_val: 0.7733 time: 0.0229s\n",
      "Epoch: 0088 loss_train: 0.8584 acc_train: 0.7857 loss_val: 1.0354 acc_val: 0.7867 time: 0.0259s\n",
      "Epoch: 0089 loss_train: 0.7945 acc_train: 0.8286 loss_val: 1.0273 acc_val: 0.7867 time: 0.0349s\n",
      "Epoch: 0090 loss_train: 0.7463 acc_train: 0.8786 loss_val: 1.0196 acc_val: 0.7967 time: 0.0309s\n",
      "Epoch: 0091 loss_train: 0.7909 acc_train: 0.8357 loss_val: 1.0124 acc_val: 0.8000 time: 0.0409s\n",
      "Epoch: 0092 loss_train: 0.8045 acc_train: 0.8429 loss_val: 1.0053 acc_val: 0.8000 time: 0.0469s\n",
      "Epoch: 0093 loss_train: 0.7821 acc_train: 0.8571 loss_val: 0.9985 acc_val: 0.8033 time: 0.0469s\n",
      "Epoch: 0094 loss_train: 0.7671 acc_train: 0.8500 loss_val: 0.9922 acc_val: 0.8033 time: 0.0489s\n",
      "Epoch: 0095 loss_train: 0.7350 acc_train: 0.8500 loss_val: 0.9849 acc_val: 0.8033 time: 0.0429s\n",
      "Epoch: 0096 loss_train: 0.7821 acc_train: 0.8143 loss_val: 0.9781 acc_val: 0.8000 time: 0.0399s\n",
      "Epoch: 0097 loss_train: 0.7978 acc_train: 0.8357 loss_val: 0.9716 acc_val: 0.8000 time: 0.0449s\n",
      "Epoch: 0098 loss_train: 0.7582 acc_train: 0.8571 loss_val: 0.9660 acc_val: 0.7967 time: 0.0259s\n",
      "Epoch: 0099 loss_train: 0.7672 acc_train: 0.8286 loss_val: 0.9615 acc_val: 0.7967 time: 0.0249s\n",
      "Epoch: 0100 loss_train: 0.7330 acc_train: 0.8500 loss_val: 0.9573 acc_val: 0.7900 time: 0.0259s\n",
      "Epoch: 0101 loss_train: 0.7053 acc_train: 0.8786 loss_val: 0.9537 acc_val: 0.7933 time: 0.0196s\n",
      "Epoch: 0102 loss_train: 0.7405 acc_train: 0.8571 loss_val: 0.9504 acc_val: 0.7933 time: 0.0239s\n",
      "Epoch: 0103 loss_train: 0.6803 acc_train: 0.8786 loss_val: 0.9468 acc_val: 0.7933 time: 0.0329s\n",
      "Epoch: 0104 loss_train: 0.7499 acc_train: 0.8714 loss_val: 0.9415 acc_val: 0.7933 time: 0.0219s\n",
      "Epoch: 0105 loss_train: 0.6655 acc_train: 0.8929 loss_val: 0.9358 acc_val: 0.7933 time: 0.0219s\n",
      "Epoch: 0106 loss_train: 0.7073 acc_train: 0.8714 loss_val: 0.9290 acc_val: 0.7933 time: 0.0239s\n",
      "Epoch: 0107 loss_train: 0.7057 acc_train: 0.8714 loss_val: 0.9219 acc_val: 0.7967 time: 0.0279s\n",
      "Epoch: 0108 loss_train: 0.6736 acc_train: 0.8571 loss_val: 0.9163 acc_val: 0.8000 time: 0.0259s\n",
      "Epoch: 0109 loss_train: 0.6528 acc_train: 0.8786 loss_val: 0.9111 acc_val: 0.8067 time: 0.0289s\n",
      "Epoch: 0110 loss_train: 0.6727 acc_train: 0.8857 loss_val: 0.9062 acc_val: 0.8067 time: 0.0329s\n",
      "Epoch: 0111 loss_train: 0.6363 acc_train: 0.8429 loss_val: 0.9023 acc_val: 0.8000 time: 0.0249s\n",
      "Epoch: 0112 loss_train: 0.6319 acc_train: 0.9000 loss_val: 0.8994 acc_val: 0.7933 time: 0.0239s\n",
      "Epoch: 0113 loss_train: 0.6442 acc_train: 0.8643 loss_val: 0.8973 acc_val: 0.7933 time: 0.0229s\n",
      "Epoch: 0114 loss_train: 0.6532 acc_train: 0.8929 loss_val: 0.8941 acc_val: 0.7967 time: 0.0289s\n",
      "Epoch: 0115 loss_train: 0.6485 acc_train: 0.8500 loss_val: 0.8906 acc_val: 0.7967 time: 0.0259s\n",
      "Epoch: 0116 loss_train: 0.6682 acc_train: 0.9000 loss_val: 0.8865 acc_val: 0.7933 time: 0.0239s\n",
      "Epoch: 0117 loss_train: 0.6255 acc_train: 0.9000 loss_val: 0.8818 acc_val: 0.7967 time: 0.0319s\n",
      "Epoch: 0118 loss_train: 0.6001 acc_train: 0.9000 loss_val: 0.8781 acc_val: 0.8000 time: 0.0389s\n",
      "Epoch: 0119 loss_train: 0.5866 acc_train: 0.8929 loss_val: 0.8752 acc_val: 0.7933 time: 0.0479s\n",
      "Epoch: 0120 loss_train: 0.5726 acc_train: 0.9000 loss_val: 0.8717 acc_val: 0.7933 time: 0.0419s\n",
      "Epoch: 0121 loss_train: 0.6191 acc_train: 0.8786 loss_val: 0.8680 acc_val: 0.8000 time: 0.0349s\n",
      "Epoch: 0122 loss_train: 0.6362 acc_train: 0.8571 loss_val: 0.8640 acc_val: 0.8000 time: 0.0439s\n",
      "Epoch: 0123 loss_train: 0.6225 acc_train: 0.8643 loss_val: 0.8605 acc_val: 0.8000 time: 0.0509s\n",
      "Epoch: 0124 loss_train: 0.6047 acc_train: 0.8929 loss_val: 0.8567 acc_val: 0.7967 time: 0.0339s\n",
      "Epoch: 0125 loss_train: 0.6194 acc_train: 0.8643 loss_val: 0.8540 acc_val: 0.7967 time: 0.0419s\n",
      "Epoch: 0126 loss_train: 0.6036 acc_train: 0.8857 loss_val: 0.8507 acc_val: 0.7967 time: 0.0529s\n",
      "Epoch: 0127 loss_train: 0.5430 acc_train: 0.9000 loss_val: 0.8471 acc_val: 0.8000 time: 0.0509s\n",
      "Epoch: 0128 loss_train: 0.6200 acc_train: 0.8429 loss_val: 0.8431 acc_val: 0.8033 time: 0.0708s\n",
      "Epoch: 0129 loss_train: 0.5698 acc_train: 0.9071 loss_val: 0.8383 acc_val: 0.8067 time: 0.0509s\n",
      "Epoch: 0130 loss_train: 0.5573 acc_train: 0.9071 loss_val: 0.8346 acc_val: 0.8133 time: 0.0389s\n",
      "Epoch: 0131 loss_train: 0.5806 acc_train: 0.8786 loss_val: 0.8321 acc_val: 0.8133 time: 0.0439s\n",
      "Epoch: 0132 loss_train: 0.5958 acc_train: 0.8786 loss_val: 0.8296 acc_val: 0.8100 time: 0.0469s\n",
      "Epoch: 0133 loss_train: 0.5632 acc_train: 0.8929 loss_val: 0.8282 acc_val: 0.8100 time: 0.0339s\n",
      "Epoch: 0134 loss_train: 0.5793 acc_train: 0.8786 loss_val: 0.8274 acc_val: 0.8033 time: 0.0339s\n",
      "Epoch: 0135 loss_train: 0.5534 acc_train: 0.9071 loss_val: 0.8284 acc_val: 0.8000 time: 0.0349s\n",
      "Epoch: 0136 loss_train: 0.5733 acc_train: 0.8714 loss_val: 0.8285 acc_val: 0.8000 time: 0.0329s\n",
      "Epoch: 0137 loss_train: 0.5351 acc_train: 0.9000 loss_val: 0.8282 acc_val: 0.7967 time: 0.0269s\n",
      "Epoch: 0138 loss_train: 0.5379 acc_train: 0.9071 loss_val: 0.8268 acc_val: 0.7933 time: 0.0349s\n",
      "Epoch: 0139 loss_train: 0.5350 acc_train: 0.8857 loss_val: 0.8225 acc_val: 0.7967 time: 0.0529s\n",
      "Epoch: 0140 loss_train: 0.5655 acc_train: 0.9000 loss_val: 0.8164 acc_val: 0.8000 time: 0.0489s\n",
      "Epoch: 0141 loss_train: 0.5361 acc_train: 0.9000 loss_val: 0.8095 acc_val: 0.8067 time: 0.0449s\n",
      "Epoch: 0142 loss_train: 0.5189 acc_train: 0.8857 loss_val: 0.8036 acc_val: 0.8100 time: 0.0439s\n",
      "Epoch: 0143 loss_train: 0.5391 acc_train: 0.9071 loss_val: 0.7987 acc_val: 0.8133 time: 0.0389s\n",
      "Epoch: 0144 loss_train: 0.5336 acc_train: 0.9000 loss_val: 0.7948 acc_val: 0.8200 time: 0.0479s\n",
      "Epoch: 0145 loss_train: 0.5391 acc_train: 0.8786 loss_val: 0.7926 acc_val: 0.8167 time: 0.0409s\n",
      "Epoch: 0146 loss_train: 0.5347 acc_train: 0.8857 loss_val: 0.7914 acc_val: 0.8133 time: 0.0339s\n",
      "Epoch: 0147 loss_train: 0.5333 acc_train: 0.8786 loss_val: 0.7910 acc_val: 0.8133 time: 0.0279s\n",
      "Epoch: 0148 loss_train: 0.4743 acc_train: 0.8929 loss_val: 0.7898 acc_val: 0.8200 time: 0.0399s\n",
      "Epoch: 0149 loss_train: 0.5278 acc_train: 0.8929 loss_val: 0.7883 acc_val: 0.8133 time: 0.0359s\n",
      "Epoch: 0150 loss_train: 0.5303 acc_train: 0.9143 loss_val: 0.7872 acc_val: 0.8133 time: 0.0339s\n",
      "Epoch: 0151 loss_train: 0.4461 acc_train: 0.9357 loss_val: 0.7862 acc_val: 0.8133 time: 0.0409s\n",
      "Epoch: 0152 loss_train: 0.5242 acc_train: 0.9000 loss_val: 0.7848 acc_val: 0.8133 time: 0.0329s\n",
      "Epoch: 0153 loss_train: 0.4952 acc_train: 0.9143 loss_val: 0.7826 acc_val: 0.8133 time: 0.0379s\n",
      "Epoch: 0154 loss_train: 0.4999 acc_train: 0.9214 loss_val: 0.7808 acc_val: 0.8100 time: 0.0369s\n",
      "Epoch: 0155 loss_train: 0.4855 acc_train: 0.9286 loss_val: 0.7795 acc_val: 0.8100 time: 0.0349s\n",
      "Epoch: 0156 loss_train: 0.4918 acc_train: 0.9286 loss_val: 0.7784 acc_val: 0.8100 time: 0.0309s\n",
      "Epoch: 0157 loss_train: 0.4880 acc_train: 0.9071 loss_val: 0.7770 acc_val: 0.8067 time: 0.0329s\n",
      "Epoch: 0158 loss_train: 0.4571 acc_train: 0.9071 loss_val: 0.7748 acc_val: 0.8067 time: 0.0439s\n",
      "Epoch: 0159 loss_train: 0.4590 acc_train: 0.9143 loss_val: 0.7717 acc_val: 0.8067 time: 0.0429s\n",
      "Epoch: 0160 loss_train: 0.4720 acc_train: 0.9286 loss_val: 0.7674 acc_val: 0.8133 time: 0.0339s\n",
      "Epoch: 0161 loss_train: 0.4643 acc_train: 0.9143 loss_val: 0.7638 acc_val: 0.8167 time: 0.0449s\n",
      "Epoch: 0162 loss_train: 0.4589 acc_train: 0.9500 loss_val: 0.7610 acc_val: 0.8233 time: 0.0369s\n",
      "Epoch: 0163 loss_train: 0.4868 acc_train: 0.9143 loss_val: 0.7608 acc_val: 0.8233 time: 0.0349s\n",
      "Epoch: 0164 loss_train: 0.4485 acc_train: 0.9214 loss_val: 0.7614 acc_val: 0.8233 time: 0.0329s\n",
      "Epoch: 0165 loss_train: 0.4941 acc_train: 0.9143 loss_val: 0.7616 acc_val: 0.8200 time: 0.0309s\n",
      "Epoch: 0166 loss_train: 0.4719 acc_train: 0.8786 loss_val: 0.7627 acc_val: 0.8200 time: 0.0299s\n",
      "Epoch: 0167 loss_train: 0.4889 acc_train: 0.9286 loss_val: 0.7633 acc_val: 0.8167 time: 0.0319s\n",
      "Epoch: 0168 loss_train: 0.4962 acc_train: 0.9143 loss_val: 0.7623 acc_val: 0.8200 time: 0.0319s\n",
      "Epoch: 0169 loss_train: 0.4983 acc_train: 0.8786 loss_val: 0.7608 acc_val: 0.8200 time: 0.0359s\n",
      "Epoch: 0170 loss_train: 0.4955 acc_train: 0.8643 loss_val: 0.7602 acc_val: 0.8133 time: 0.0339s\n",
      "Epoch: 0171 loss_train: 0.4310 acc_train: 0.9286 loss_val: 0.7595 acc_val: 0.8133 time: 0.0319s\n",
      "Epoch: 0172 loss_train: 0.4812 acc_train: 0.9071 loss_val: 0.7582 acc_val: 0.8100 time: 0.0269s\n",
      "Epoch: 0173 loss_train: 0.4647 acc_train: 0.9143 loss_val: 0.7573 acc_val: 0.8100 time: 0.0309s\n",
      "Epoch: 0174 loss_train: 0.4732 acc_train: 0.9214 loss_val: 0.7562 acc_val: 0.8100 time: 0.0329s\n",
      "Epoch: 0175 loss_train: 0.4511 acc_train: 0.9357 loss_val: 0.7551 acc_val: 0.8100 time: 0.0409s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0176 loss_train: 0.4452 acc_train: 0.9071 loss_val: 0.7522 acc_val: 0.8067 time: 0.0319s\n",
      "Epoch: 0177 loss_train: 0.4778 acc_train: 0.8786 loss_val: 0.7496 acc_val: 0.8067 time: 0.0309s\n",
      "Epoch: 0178 loss_train: 0.4636 acc_train: 0.9143 loss_val: 0.7488 acc_val: 0.8133 time: 0.0379s\n",
      "Epoch: 0179 loss_train: 0.4658 acc_train: 0.9500 loss_val: 0.7490 acc_val: 0.8100 time: 0.0419s\n",
      "Epoch: 0180 loss_train: 0.4564 acc_train: 0.9357 loss_val: 0.7487 acc_val: 0.8100 time: 0.0349s\n",
      "Epoch: 0181 loss_train: 0.4813 acc_train: 0.9286 loss_val: 0.7481 acc_val: 0.8100 time: 0.0309s\n",
      "Epoch: 0182 loss_train: 0.4271 acc_train: 0.9214 loss_val: 0.7454 acc_val: 0.8100 time: 0.0349s\n",
      "Epoch: 0183 loss_train: 0.4567 acc_train: 0.9071 loss_val: 0.7416 acc_val: 0.8167 time: 0.0319s\n",
      "Epoch: 0184 loss_train: 0.3984 acc_train: 0.9357 loss_val: 0.7374 acc_val: 0.8133 time: 0.0309s\n",
      "Epoch: 0185 loss_train: 0.4377 acc_train: 0.9214 loss_val: 0.7351 acc_val: 0.8200 time: 0.0319s\n",
      "Epoch: 0186 loss_train: 0.4739 acc_train: 0.9286 loss_val: 0.7336 acc_val: 0.8200 time: 0.0399s\n",
      "Epoch: 0187 loss_train: 0.4535 acc_train: 0.9071 loss_val: 0.7329 acc_val: 0.8200 time: 0.0299s\n",
      "Epoch: 0188 loss_train: 0.4316 acc_train: 0.9286 loss_val: 0.7332 acc_val: 0.8200 time: 0.0399s\n",
      "Epoch: 0189 loss_train: 0.4738 acc_train: 0.9143 loss_val: 0.7340 acc_val: 0.8167 time: 0.0409s\n",
      "Epoch: 0190 loss_train: 0.4431 acc_train: 0.9071 loss_val: 0.7352 acc_val: 0.8100 time: 0.0369s\n",
      "Epoch: 0191 loss_train: 0.4068 acc_train: 0.9500 loss_val: 0.7361 acc_val: 0.8133 time: 0.0509s\n",
      "Epoch: 0192 loss_train: 0.4622 acc_train: 0.9214 loss_val: 0.7364 acc_val: 0.8133 time: 0.0409s\n",
      "Epoch: 0193 loss_train: 0.4597 acc_train: 0.9071 loss_val: 0.7364 acc_val: 0.8133 time: 0.0489s\n",
      "Epoch: 0194 loss_train: 0.4490 acc_train: 0.9357 loss_val: 0.7345 acc_val: 0.8100 time: 0.0449s\n",
      "Epoch: 0195 loss_train: 0.4229 acc_train: 0.9143 loss_val: 0.7327 acc_val: 0.8100 time: 0.0459s\n",
      "Epoch: 0196 loss_train: 0.4680 acc_train: 0.9429 loss_val: 0.7321 acc_val: 0.8100 time: 0.0319s\n",
      "Epoch: 0197 loss_train: 0.4172 acc_train: 0.9357 loss_val: 0.7304 acc_val: 0.8100 time: 0.0349s\n",
      "Epoch: 0198 loss_train: 0.4061 acc_train: 0.9214 loss_val: 0.7278 acc_val: 0.8100 time: 0.0279s\n",
      "Epoch: 0199 loss_train: 0.4020 acc_train: 0.9357 loss_val: 0.7254 acc_val: 0.8067 time: 0.0439s\n",
      "Epoch: 0200 loss_train: 0.3976 acc_train: 0.9500 loss_val: 0.7239 acc_val: 0.8133 time: 0.0349s\n",
      "Optimization Finished\n",
      "Total time elapsed: 7.2957s\n"
     ]
    }
   ],
   "source": [
    "t_total = time.time()\n",
    "for epoch in range(200):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set results: loss= 0.7576 accuracy= 0.8230\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
