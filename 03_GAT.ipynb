{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        self.W = nn.Parameter(torch.empty(size=(in_features, out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        self.a = nn.Parameter(torch.empty(size=(2*out_features, 1)))  # concat(V,NeigV)\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "    def forward(self, h, adj):\n",
    "        Wh = torch.mm(h, self.W) # h.shape: (N, in_features), Wh.shape: (N, out_features)\n",
    "        a_input = self._prepare_attentional_mechanism_input(Wh)  # 每一个节点和所有节点，特征。(Vall, Vall, feature)\n",
    "        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n",
    "        # 之前计算的是一个节点和所有节点的attention，其实需要的是连接的节点的attention系数\n",
    "        zero_vec = -9e15*torch.ones_like(e)\n",
    "        attention = torch.where(adj > 0, e, zero_vec)    # 将邻接矩阵中小于0的变成负无穷\n",
    "        attention = F.softmax(attention, dim=1)  # 按行求softmax。 sum(axis=1) === 1\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        h_prime = torch.matmul(attention, Wh)   # 聚合邻居函数\n",
    "\n",
    "        if self.concat:\n",
    "            return F.elu(h_prime)   # elu-激活函数\n",
    "        else:\n",
    "            return h_prime\n",
    "\n",
    "    def _prepare_attentional_mechanism_input(self, Wh):\n",
    "        N = Wh.size()[0] # number of nodes\n",
    "\n",
    "        # Below, two matrices are created that contain embeddings in their rows in different orders.\n",
    "        # (e stands for embedding)\n",
    "        # These are the rows of the first matrix (Wh_repeated_in_chunks): \n",
    "        # e1, e1, ..., e1,            e2, e2, ..., e2,            ..., eN, eN, ..., eN\n",
    "        # '-------------' -> N times  '-------------' -> N times       '-------------' -> N times\n",
    "        # \n",
    "        # These are the rows of the second matrix (Wh_repeated_alternating): \n",
    "        # e1, e2, ..., eN, e1, e2, ..., eN, ..., e1, e2, ..., eN \n",
    "        # '----------------------------------------------------' -> N times\n",
    "        # \n",
    "        \n",
    "        Wh_repeated_in_chunks = Wh.repeat_interleave(N, dim=0)  # 复制 pytorch >= 1.1.0\n",
    "        Wh_repeated_alternating = Wh.repeat(N, 1)\n",
    "        # Wh_repeated_in_chunks.shape == Wh_repeated_alternating.shape == (N * N, out_features)\n",
    "\n",
    "        # The all_combination_matrix, created below, will look like this (|| denotes concatenation):\n",
    "        # e1 || e1\n",
    "        # e1 || e2\n",
    "        # e1 || e3\n",
    "        # ...\n",
    "        # e1 || eN\n",
    "        # e2 || e1\n",
    "        # e2 || e2\n",
    "        # e2 || e3\n",
    "        # ...\n",
    "        # e2 || eN\n",
    "        # ...\n",
    "        # eN || e1\n",
    "        # eN || e2\n",
    "        # eN || e3\n",
    "        # ...\n",
    "        # eN || eN\n",
    "\n",
    "        all_combinations_matrix = torch.cat([Wh_repeated_in_chunks, Wh_repeated_alternating], dim=1)\n",
    "        # all_combinations_matrix.shape == (N * N, 2 * out_features)\n",
    "\n",
    "        return all_combinations_matrix.view(N, N, 2 * self.out_features)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpecialSpmmFunction(torch.autograd.Function):\n",
    "    \"\"\"Special function for only sparse region backpropataion layer.\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, indices, values, shape, b):\n",
    "        assert indices.requires_grad == False\n",
    "        a = torch.sparse_coo_tensor(indices, values, shape)\n",
    "        ctx.save_for_backward(a, b)\n",
    "        ctx.N = shape[0]\n",
    "        return torch.matmul(a, b)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        a, b = ctx.saved_tensors\n",
    "        grad_values = grad_b = None\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            grad_a_dense = grad_output.matmul(b.t())\n",
    "            edge_idx = a._indices()[0, :] * ctx.N + a._indices()[1, :]\n",
    "            grad_values = grad_a_dense.view(-1)[edge_idx]\n",
    "        if ctx.needs_input_grad[3]:\n",
    "            grad_b = a.t().matmul(grad_output)\n",
    "        return None, grad_values, None, grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpecialSpmm(nn.Module):\n",
    "    def forward(self, indices, values, shape, b):\n",
    "        return SpecialSpmmFunction.apply(indices, values, shape, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpGraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Sparse version GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
    "        super(SpGraphAttentionLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "        nn.init.xavier_normal_(self.W.data, gain=1.414)\n",
    "                \n",
    "        self.a = nn.Parameter(torch.zeros(size=(1, 2*out_features)))\n",
    "        nn.init.xavier_normal_(self.a.data, gain=1.414)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "        self.special_spmm = SpecialSpmm()\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        dv = 'cuda' if input.is_cuda else 'cpu'\n",
    "\n",
    "        N = input.size()[0]\n",
    "        edge = adj.nonzero().t()\n",
    "\n",
    "        h = torch.mm(input, self.W)\n",
    "        # h: N x out\n",
    "        assert not torch.isnan(h).any()\n",
    "\n",
    "        # Self-attention on the nodes - Shared attention mechanism\n",
    "        edge_h = torch.cat((h[edge[0, :], :], h[edge[1, :], :]), dim=1).t()\n",
    "        # edge: 2*D x E\n",
    "\n",
    "        edge_e = torch.exp(-self.leakyrelu(self.a.mm(edge_h).squeeze()))\n",
    "        assert not torch.isnan(edge_e).any()\n",
    "        # edge_e: E\n",
    "\n",
    "        e_rowsum = self.special_spmm(edge, edge_e, torch.Size([N, N]), torch.ones(size=(N,1), device=dv))\n",
    "        # e_rowsum: N x 1\n",
    "\n",
    "        edge_e = self.dropout(edge_e)\n",
    "        # edge_e: E\n",
    "\n",
    "        h_prime = self.special_spmm(edge, edge_e, torch.Size([N, N]), h)\n",
    "        assert not torch.isnan(h_prime).any()\n",
    "        # h_prime: N x out\n",
    "        \n",
    "        h_prime = h_prime.div(e_rowsum)\n",
    "        # h_prime: N x out\n",
    "        assert not torch.isnan(h_prime).any()\n",
    "\n",
    "        if self.concat:\n",
    "            # if this layer is not last layer,\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            # if this layer is last layer,\n",
    "            return h_prime\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):\n",
    "        \"\"\"Dense version of GAT.\"\"\"\n",
    "        super(GAT, self).__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.attentions = [GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "\n",
    "        self.out_att = GraphAttentionLayer(nhid * nheads, nclass, dropout=dropout, alpha=alpha, concat=False)  # 第二层(最后一层)的attention layer\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)  # 将每层attention拼接\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.elu(self.out_att(x, adj))   # 第二层的attention layer\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpGAT(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):\n",
    "        \"\"\"Sparse version of GAT.\"\"\"\n",
    "        super(SpGAT, self).__init__()\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.attentions = [SpGraphAttentionLayer(nfeat, \n",
    "                                                 nhid, \n",
    "                                                 dropout=dropout, \n",
    "                                                 alpha=alpha, \n",
    "                                                 concat=True) for _ in range(nheads)]\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "\n",
    "        self.out_att = SpGraphAttentionLayer(nhid * nheads, \n",
    "                                             nclass, \n",
    "                                             dropout=dropout, \n",
    "                                             alpha=alpha, \n",
    "                                             concat=False)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = F.elu(self.out_att(x, adj))\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_onehot(labels):\n",
    "    # The classes must be sorted before encoding to enable static class encoding.\n",
    "    # In other words, make sure the first class always maps to index 0.\n",
    "    classes = sorted(list(set(labels)))\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)), dtype=np.int32)\n",
    "    return labels_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path=r\"C:\\Users\\sss\\Desktop/cora/\", dataset=\"cora\"):\n",
    "    \"\"\"Load citation network dataset (cora only for now)\"\"\"\n",
    "    print('Loading {} dataset...'.format(dataset))\n",
    "\n",
    "    idx_features_labels = np.genfromtxt(\"{}{}.content\".format(path, dataset), dtype=np.dtype(str))\n",
    "    features = sp.csr_matrix(idx_features_labels[:, 1:-1], dtype=np.float32)\n",
    "    labels = encode_onehot(idx_features_labels[:, -1])\n",
    "\n",
    "    # build graph\n",
    "    idx = np.array(idx_features_labels[:, 0], dtype=np.int32)\n",
    "    idx_map = {j: i for i, j in enumerate(idx)}\n",
    "    edges_unordered = np.genfromtxt(\"{}{}.cites\".format(path, dataset), dtype=np.int32) # 将边导入\n",
    "    edges = np.array(list(map(idx_map.get, edges_unordered.flatten())), dtype=np.int32).reshape(edges_unordered.shape)  # 将点对应到dictionary中\n",
    "    # 建立边的邻接矩阵\n",
    "    adj = sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])), shape=(labels.shape[0], labels.shape[0]), dtype=np.float32)\n",
    "\n",
    "    # build symmetric adjacency matrix。构建对称矩阵\n",
    "    adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "    features = normalize_features(features)\n",
    "    adj = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "\n",
    "    idx_train = range(140)\n",
    "    idx_val = range(200, 500)\n",
    "    idx_test = range(500, 1500)\n",
    "\n",
    "    adj = torch.FloatTensor(np.array(adj.todense()))\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    labels = torch.LongTensor(np.where(labels)[1])\n",
    "\n",
    "    idx_train = torch.LongTensor(idx_train)\n",
    "    idx_val = torch.LongTensor(idx_val)\n",
    "    idx_test = torch.LongTensor(idx_test)\n",
    "\n",
    "    return adj, features, labels, idx_train, idx_val, idx_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_adj(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    r_inv_sqrt[np.isinf(r_inv_sqrt)] = 0.\n",
    "    r_mat_inv_sqrt = sp.diags(r_inv_sqrt)\n",
    "    return mx.dot(r_mat_inv_sqrt).transpose().dot(r_mat_inv_sqrt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False, help='Disables CUDA training.')\n",
    "parser.add_argument('--fastmode', action='store_true', default=False, help='Validate during training pass.')\n",
    "parser.add_argument('--sparse', action='store_true', default=False, help='GAT with sparse version or not.')\n",
    "parser.add_argument('--seed', type=int, default=72, help='Random seed.')\n",
    "parser.add_argument('--epochs', type=int, default=200, help='Number of epochs to train.')\n",
    "parser.add_argument('--lr', type=float, default=0.005, help='Initial learning rate.')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4, help='Weight decay (L2 loss on parameters).')\n",
    "parser.add_argument('--hidden', type=int, default=8, help='Number of hidden units.')\n",
    "parser.add_argument('--nb_heads', type=int, default=8, help='Number of head attentions.')\n",
    "parser.add_argument('--dropout', type=float, default=0.6, help='Dropout rate (1 - keep probability).')\n",
    "parser.add_argument('--alpha', type=float, default=0.2, help='Alpha for the leaky_relu.')\n",
    "parser.add_argument('--patience', type=int, default=100, help='Patience')\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x16b05467350>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载Cora数据 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    }
   ],
   "source": [
    "adj, features, labels, idx_train, idx_val, idx_test = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- args.sparse = True 时，取消 features, adj, labels = Variable(features), Variable(adj), Variable(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and optimizer\n",
    "args.sparse = True\n",
    "if args.sparse:\n",
    "    model = SpGAT(nfeat=features.shape[1], \n",
    "                nhid=args.hidden, \n",
    "                nclass=int(labels.max()) + 1, \n",
    "                dropout=args.dropout, \n",
    "                nheads=args.nb_heads, \n",
    "                alpha=args.alpha)\n",
    "else:\n",
    "    model = GAT(nfeat=features.shape[1], \n",
    "                nhid=args.hidden, \n",
    "                nclass=int(labels.max()) + 1, \n",
    "                dropout=args.dropout, \n",
    "                nheads=args.nb_heads, \n",
    "                alpha=args.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpGAT(\n",
       "  (attention_0): SpGraphAttentionLayer (1433 -> 8)\n",
       "  (attention_1): SpGraphAttentionLayer (1433 -> 8)\n",
       "  (attention_2): SpGraphAttentionLayer (1433 -> 8)\n",
       "  (attention_3): SpGraphAttentionLayer (1433 -> 8)\n",
       "  (attention_4): SpGraphAttentionLayer (1433 -> 8)\n",
       "  (attention_5): SpGraphAttentionLayer (1433 -> 8)\n",
       "  (attention_6): SpGraphAttentionLayer (1433 -> 8)\n",
       "  (attention_7): SpGraphAttentionLayer (1433 -> 8)\n",
       "  (out_att): SpGraphAttentionLayer (64 -> 7)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.cuda:\n",
    "    model.cuda()\n",
    "    features = features.cuda()\n",
    "    adj = adj.cuda()\n",
    "    labels = labels.cuda()\n",
    "    idx_train = idx_train.cuda()\n",
    "    idx_val = idx_val.cuda()\n",
    "    idx_test = idx_test.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features, adj, labels = Variable(features), Variable(adj), Variable(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练和测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)  # GAT模块\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if not args.fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        model.eval()\n",
    "        output = model(features, adj)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.data.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.data.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.data.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.data.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "    return loss_val.data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_values = []\n",
    "bad_counter = 0\n",
    "best = args.epochs + 1\n",
    "best_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 1.9495 acc_train: 0.0857 loss_val: 1.9380 acc_val: 0.3800 time: 5.8033s\n",
      "Epoch: 0002 loss_train: 1.9386 acc_train: 0.2000 loss_val: 1.9276 acc_val: 0.5233 time: 0.8058s\n",
      "Epoch: 0003 loss_train: 1.9218 acc_train: 0.3571 loss_val: 1.9175 acc_val: 0.4800 time: 0.7749s\n",
      "Epoch: 0004 loss_train: 1.9077 acc_train: 0.3929 loss_val: 1.9077 acc_val: 0.4467 time: 0.7687s\n",
      "Epoch: 0005 loss_train: 1.8922 acc_train: 0.4429 loss_val: 1.8977 acc_val: 0.4367 time: 0.7739s\n",
      "Epoch: 0006 loss_train: 1.8974 acc_train: 0.4143 loss_val: 1.8878 acc_val: 0.4300 time: 0.7749s\n",
      "Epoch: 0007 loss_train: 1.8738 acc_train: 0.4357 loss_val: 1.8778 acc_val: 0.4200 time: 0.7719s\n",
      "Epoch: 0008 loss_train: 1.8712 acc_train: 0.4500 loss_val: 1.8677 acc_val: 0.4233 time: 0.8321s\n",
      "Epoch: 0009 loss_train: 1.8521 acc_train: 0.4214 loss_val: 1.8573 acc_val: 0.4200 time: 0.8837s\n",
      "Epoch: 0010 loss_train: 1.8423 acc_train: 0.4071 loss_val: 1.8467 acc_val: 0.4167 time: 0.9505s\n",
      "Epoch: 0011 loss_train: 1.8346 acc_train: 0.4286 loss_val: 1.8360 acc_val: 0.4200 time: 0.8437s\n",
      "Epoch: 0012 loss_train: 1.8158 acc_train: 0.4857 loss_val: 1.8250 acc_val: 0.4133 time: 0.8432s\n",
      "Epoch: 0013 loss_train: 1.8083 acc_train: 0.3786 loss_val: 1.8137 acc_val: 0.4133 time: 0.8647s\n",
      "Epoch: 0014 loss_train: 1.7787 acc_train: 0.4000 loss_val: 1.8023 acc_val: 0.4133 time: 0.9026s\n",
      "Epoch: 0015 loss_train: 1.7602 acc_train: 0.4571 loss_val: 1.7906 acc_val: 0.4133 time: 0.8457s\n",
      "Epoch: 0016 loss_train: 1.7562 acc_train: 0.4143 loss_val: 1.7787 acc_val: 0.4167 time: 0.8068s\n",
      "Epoch: 0017 loss_train: 1.7451 acc_train: 0.4500 loss_val: 1.7668 acc_val: 0.4167 time: 0.7939s\n",
      "Epoch: 0018 loss_train: 1.7339 acc_train: 0.4357 loss_val: 1.7547 acc_val: 0.4200 time: 0.7899s\n",
      "Epoch: 0019 loss_train: 1.7358 acc_train: 0.4000 loss_val: 1.7427 acc_val: 0.4233 time: 0.8557s\n",
      "Epoch: 0020 loss_train: 1.7367 acc_train: 0.3500 loss_val: 1.7308 acc_val: 0.4267 time: 0.7959s\n",
      "Epoch: 0021 loss_train: 1.6770 acc_train: 0.4643 loss_val: 1.7186 acc_val: 0.4333 time: 0.8776s\n",
      "Epoch: 0022 loss_train: 1.7115 acc_train: 0.4714 loss_val: 1.7067 acc_val: 0.4333 time: 0.8328s\n",
      "Epoch: 0023 loss_train: 1.6731 acc_train: 0.4643 loss_val: 1.6947 acc_val: 0.4333 time: 0.7969s\n",
      "Epoch: 0024 loss_train: 1.6636 acc_train: 0.4786 loss_val: 1.6827 acc_val: 0.4467 time: 0.8318s\n",
      "Epoch: 0025 loss_train: 1.6519 acc_train: 0.4571 loss_val: 1.6707 acc_val: 0.4500 time: 0.8388s\n",
      "Epoch: 0026 loss_train: 1.6428 acc_train: 0.4571 loss_val: 1.6589 acc_val: 0.4667 time: 0.7809s\n",
      "Epoch: 0027 loss_train: 1.6086 acc_train: 0.5000 loss_val: 1.6471 acc_val: 0.4733 time: 0.7669s\n",
      "Epoch: 0028 loss_train: 1.6282 acc_train: 0.4714 loss_val: 1.6354 acc_val: 0.4767 time: 0.7758s\n",
      "Epoch: 0029 loss_train: 1.6007 acc_train: 0.4714 loss_val: 1.6236 acc_val: 0.4800 time: 0.7659s\n",
      "Epoch: 0030 loss_train: 1.5429 acc_train: 0.5571 loss_val: 1.6117 acc_val: 0.4967 time: 0.7650s\n",
      "Epoch: 0031 loss_train: 1.5612 acc_train: 0.4786 loss_val: 1.5999 acc_val: 0.5067 time: 0.7849s\n",
      "Epoch: 0032 loss_train: 1.5914 acc_train: 0.5071 loss_val: 1.5883 acc_val: 0.5200 time: 0.7610s\n",
      "Epoch: 0033 loss_train: 1.5384 acc_train: 0.5571 loss_val: 1.5766 acc_val: 0.5333 time: 0.7610s\n",
      "Epoch: 0034 loss_train: 1.5300 acc_train: 0.5500 loss_val: 1.5649 acc_val: 0.5500 time: 0.7640s\n",
      "Epoch: 0035 loss_train: 1.4608 acc_train: 0.5786 loss_val: 1.5532 acc_val: 0.5600 time: 0.7637s\n",
      "Epoch: 0036 loss_train: 1.4952 acc_train: 0.5643 loss_val: 1.5417 acc_val: 0.5833 time: 0.7829s\n",
      "Epoch: 0037 loss_train: 1.5240 acc_train: 0.5500 loss_val: 1.5301 acc_val: 0.6000 time: 0.7690s\n",
      "Epoch: 0038 loss_train: 1.4170 acc_train: 0.6000 loss_val: 1.5186 acc_val: 0.6100 time: 0.7630s\n",
      "Epoch: 0039 loss_train: 1.4522 acc_train: 0.6143 loss_val: 1.5069 acc_val: 0.6267 time: 0.7849s\n",
      "Epoch: 0040 loss_train: 1.4492 acc_train: 0.6071 loss_val: 1.4953 acc_val: 0.6367 time: 0.7590s\n",
      "Epoch: 0041 loss_train: 1.4094 acc_train: 0.6286 loss_val: 1.4835 acc_val: 0.6467 time: 0.7622s\n",
      "Epoch: 0042 loss_train: 1.4716 acc_train: 0.5929 loss_val: 1.4720 acc_val: 0.6500 time: 0.7600s\n",
      "Epoch: 0043 loss_train: 1.3939 acc_train: 0.6357 loss_val: 1.4604 acc_val: 0.6567 time: 0.7745s\n",
      "Epoch: 0044 loss_train: 1.3413 acc_train: 0.6500 loss_val: 1.4489 acc_val: 0.6667 time: 0.7989s\n",
      "Epoch: 0045 loss_train: 1.4244 acc_train: 0.6286 loss_val: 1.4373 acc_val: 0.6933 time: 0.7677s\n",
      "Epoch: 0046 loss_train: 1.3234 acc_train: 0.6429 loss_val: 1.4257 acc_val: 0.7067 time: 0.7829s\n",
      "Epoch: 0047 loss_train: 1.3467 acc_train: 0.6143 loss_val: 1.4143 acc_val: 0.7100 time: 0.7600s\n",
      "Epoch: 0048 loss_train: 1.3909 acc_train: 0.6214 loss_val: 1.4029 acc_val: 0.7200 time: 0.7590s\n",
      "Epoch: 0049 loss_train: 1.3922 acc_train: 0.6357 loss_val: 1.3919 acc_val: 0.7333 time: 0.7829s\n",
      "Epoch: 0050 loss_train: 1.3556 acc_train: 0.6786 loss_val: 1.3808 acc_val: 0.7467 time: 0.7708s\n",
      "Epoch: 0051 loss_train: 1.2691 acc_train: 0.7071 loss_val: 1.3699 acc_val: 0.7467 time: 0.7789s\n",
      "Epoch: 0052 loss_train: 1.3091 acc_train: 0.6714 loss_val: 1.3591 acc_val: 0.7600 time: 0.7630s\n",
      "Epoch: 0053 loss_train: 1.2809 acc_train: 0.7286 loss_val: 1.3483 acc_val: 0.7600 time: 0.7994s\n",
      "Epoch: 0054 loss_train: 1.2660 acc_train: 0.7286 loss_val: 1.3377 acc_val: 0.7667 time: 0.7839s\n",
      "Epoch: 0055 loss_train: 1.2487 acc_train: 0.7143 loss_val: 1.3272 acc_val: 0.7700 time: 0.7580s\n",
      "Epoch: 0056 loss_train: 1.2612 acc_train: 0.6929 loss_val: 1.3169 acc_val: 0.7767 time: 0.7669s\n",
      "Epoch: 0057 loss_train: 1.2137 acc_train: 0.6786 loss_val: 1.3067 acc_val: 0.7833 time: 0.7570s\n",
      "Epoch: 0058 loss_train: 1.1844 acc_train: 0.7571 loss_val: 1.2965 acc_val: 0.7867 time: 0.8268s\n",
      "Epoch: 0059 loss_train: 1.3430 acc_train: 0.6786 loss_val: 1.2864 acc_val: 0.7867 time: 0.8577s\n",
      "Epoch: 0060 loss_train: 1.2302 acc_train: 0.7286 loss_val: 1.2762 acc_val: 0.7933 time: 0.8677s\n",
      "Epoch: 0061 loss_train: 1.2018 acc_train: 0.7500 loss_val: 1.2662 acc_val: 0.7933 time: 0.7869s\n",
      "Epoch: 0062 loss_train: 1.2284 acc_train: 0.7714 loss_val: 1.2562 acc_val: 0.7933 time: 0.7709s\n",
      "Epoch: 0063 loss_train: 1.1921 acc_train: 0.7286 loss_val: 1.2463 acc_val: 0.7867 time: 0.7729s\n",
      "Epoch: 0064 loss_train: 1.1568 acc_train: 0.7643 loss_val: 1.2366 acc_val: 0.7900 time: 0.7839s\n",
      "Epoch: 0065 loss_train: 1.2407 acc_train: 0.7357 loss_val: 1.2271 acc_val: 0.7900 time: 0.7658s\n",
      "Epoch: 0066 loss_train: 1.1666 acc_train: 0.7071 loss_val: 1.2179 acc_val: 0.7900 time: 0.7809s\n",
      "Epoch: 0067 loss_train: 1.1362 acc_train: 0.7786 loss_val: 1.2086 acc_val: 0.7867 time: 0.7779s\n",
      "Epoch: 0068 loss_train: 1.1510 acc_train: 0.7000 loss_val: 1.1993 acc_val: 0.7900 time: 0.7570s\n",
      "Epoch: 0069 loss_train: 1.1185 acc_train: 0.7286 loss_val: 1.1903 acc_val: 0.7900 time: 0.7849s\n",
      "Epoch: 0070 loss_train: 1.1665 acc_train: 0.7571 loss_val: 1.1813 acc_val: 0.7933 time: 0.7949s\n",
      "Epoch: 0071 loss_train: 1.0954 acc_train: 0.7214 loss_val: 1.1724 acc_val: 0.7967 time: 0.7669s\n",
      "Epoch: 0072 loss_train: 1.0701 acc_train: 0.8000 loss_val: 1.1634 acc_val: 0.8033 time: 0.7650s\n",
      "Epoch: 0073 loss_train: 1.0633 acc_train: 0.7429 loss_val: 1.1545 acc_val: 0.8033 time: 0.7719s\n",
      "Epoch: 0074 loss_train: 1.1126 acc_train: 0.7357 loss_val: 1.1458 acc_val: 0.8067 time: 0.7909s\n",
      "Epoch: 0075 loss_train: 1.0731 acc_train: 0.7214 loss_val: 1.1373 acc_val: 0.8100 time: 0.8048s\n",
      "Epoch: 0076 loss_train: 1.1396 acc_train: 0.7571 loss_val: 1.1292 acc_val: 0.8067 time: 0.8507s\n",
      "Epoch: 0077 loss_train: 1.0881 acc_train: 0.7357 loss_val: 1.1212 acc_val: 0.8067 time: 0.8138s\n",
      "Epoch: 0078 loss_train: 1.1429 acc_train: 0.7214 loss_val: 1.1134 acc_val: 0.8067 time: 0.7879s\n",
      "Epoch: 0079 loss_train: 1.1253 acc_train: 0.7000 loss_val: 1.1058 acc_val: 0.8100 time: 0.8308s\n",
      "Epoch: 0080 loss_train: 1.0830 acc_train: 0.7500 loss_val: 1.0983 acc_val: 0.8100 time: 0.7979s\n",
      "Epoch: 0081 loss_train: 1.0135 acc_train: 0.7643 loss_val: 1.0908 acc_val: 0.8100 time: 0.7699s\n",
      "Epoch: 0082 loss_train: 1.0385 acc_train: 0.7429 loss_val: 1.0835 acc_val: 0.8100 time: 0.7680s\n",
      "Epoch: 0083 loss_train: 0.9788 acc_train: 0.7214 loss_val: 1.0766 acc_val: 0.8100 time: 0.7839s\n",
      "Epoch: 0084 loss_train: 1.0685 acc_train: 0.7286 loss_val: 1.0698 acc_val: 0.8100 time: 0.7909s\n",
      "Epoch: 0085 loss_train: 1.0265 acc_train: 0.7429 loss_val: 1.0632 acc_val: 0.8067 time: 0.7681s\n",
      "Epoch: 0086 loss_train: 1.0859 acc_train: 0.7000 loss_val: 1.0566 acc_val: 0.8100 time: 0.7729s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0087 loss_train: 1.0655 acc_train: 0.7214 loss_val: 1.0502 acc_val: 0.8133 time: 0.7789s\n",
      "Epoch: 0088 loss_train: 0.9726 acc_train: 0.7714 loss_val: 1.0439 acc_val: 0.8167 time: 0.7709s\n",
      "Epoch: 0089 loss_train: 1.0720 acc_train: 0.7214 loss_val: 1.0377 acc_val: 0.8167 time: 0.7899s\n",
      "Epoch: 0090 loss_train: 1.0017 acc_train: 0.7429 loss_val: 1.0315 acc_val: 0.8167 time: 0.7819s\n",
      "Epoch: 0091 loss_train: 0.9554 acc_train: 0.7929 loss_val: 1.0252 acc_val: 0.8200 time: 0.7902s\n",
      "Epoch: 0092 loss_train: 0.9909 acc_train: 0.7571 loss_val: 1.0191 acc_val: 0.8167 time: 0.7729s\n",
      "Epoch: 0093 loss_train: 0.9902 acc_train: 0.7500 loss_val: 1.0131 acc_val: 0.8167 time: 0.7779s\n",
      "Epoch: 0094 loss_train: 0.9486 acc_train: 0.7571 loss_val: 1.0073 acc_val: 0.8167 time: 0.7789s\n",
      "Epoch: 0095 loss_train: 1.0501 acc_train: 0.7000 loss_val: 1.0018 acc_val: 0.8167 time: 0.7799s\n",
      "Epoch: 0096 loss_train: 0.8771 acc_train: 0.8000 loss_val: 0.9963 acc_val: 0.8167 time: 0.7799s\n",
      "Epoch: 0097 loss_train: 0.9610 acc_train: 0.7643 loss_val: 0.9908 acc_val: 0.8167 time: 0.7899s\n",
      "Epoch: 0098 loss_train: 0.9171 acc_train: 0.7857 loss_val: 0.9852 acc_val: 0.8133 time: 0.7620s\n",
      "Epoch: 0099 loss_train: 0.8845 acc_train: 0.8571 loss_val: 0.9795 acc_val: 0.8167 time: 0.7679s\n",
      "Epoch: 0100 loss_train: 0.9774 acc_train: 0.7429 loss_val: 0.9742 acc_val: 0.8167 time: 0.7769s\n",
      "Epoch: 0101 loss_train: 0.9528 acc_train: 0.7786 loss_val: 0.9691 acc_val: 0.8167 time: 0.7801s\n",
      "Epoch: 0102 loss_train: 1.0008 acc_train: 0.7000 loss_val: 0.9641 acc_val: 0.8233 time: 0.7554s\n",
      "Epoch: 0103 loss_train: 0.8856 acc_train: 0.7786 loss_val: 0.9592 acc_val: 0.8233 time: 0.7789s\n",
      "Epoch: 0104 loss_train: 0.8131 acc_train: 0.8000 loss_val: 0.9544 acc_val: 0.8200 time: 0.7939s\n",
      "Epoch: 0105 loss_train: 0.8740 acc_train: 0.7500 loss_val: 0.9497 acc_val: 0.8200 time: 0.7560s\n",
      "Epoch: 0106 loss_train: 0.8483 acc_train: 0.7786 loss_val: 0.9448 acc_val: 0.8200 time: 0.7799s\n",
      "Epoch: 0107 loss_train: 0.9177 acc_train: 0.7643 loss_val: 0.9400 acc_val: 0.8233 time: 0.7759s\n",
      "Epoch: 0108 loss_train: 0.9773 acc_train: 0.7143 loss_val: 0.9352 acc_val: 0.8233 time: 0.7699s\n",
      "Epoch: 0109 loss_train: 0.8618 acc_train: 0.7786 loss_val: 0.9306 acc_val: 0.8233 time: 0.7779s\n",
      "Epoch: 0110 loss_train: 0.8757 acc_train: 0.7857 loss_val: 0.9257 acc_val: 0.8233 time: 0.7789s\n",
      "Epoch: 0111 loss_train: 0.9512 acc_train: 0.7571 loss_val: 0.9209 acc_val: 0.8233 time: 0.7669s\n",
      "Epoch: 0112 loss_train: 0.8352 acc_train: 0.8214 loss_val: 0.9161 acc_val: 0.8233 time: 0.7739s\n",
      "Epoch: 0113 loss_train: 0.8963 acc_train: 0.8071 loss_val: 0.9116 acc_val: 0.8233 time: 0.7829s\n",
      "Epoch: 0114 loss_train: 0.9109 acc_train: 0.7429 loss_val: 0.9073 acc_val: 0.8200 time: 0.7849s\n",
      "Epoch: 0115 loss_train: 0.8362 acc_train: 0.7714 loss_val: 0.9033 acc_val: 0.8200 time: 0.7679s\n",
      "Epoch: 0116 loss_train: 0.8712 acc_train: 0.7643 loss_val: 0.8995 acc_val: 0.8233 time: 0.7739s\n",
      "Epoch: 0117 loss_train: 0.8944 acc_train: 0.7857 loss_val: 0.8957 acc_val: 0.8233 time: 0.7829s\n",
      "Epoch: 0118 loss_train: 0.8215 acc_train: 0.8143 loss_val: 0.8920 acc_val: 0.8233 time: 0.7669s\n",
      "Epoch: 0119 loss_train: 0.8599 acc_train: 0.7643 loss_val: 0.8884 acc_val: 0.8233 time: 0.8168s\n",
      "Epoch: 0120 loss_train: 0.8968 acc_train: 0.7571 loss_val: 0.8849 acc_val: 0.8233 time: 0.7919s\n",
      "Epoch: 0121 loss_train: 0.9278 acc_train: 0.7500 loss_val: 0.8819 acc_val: 0.8200 time: 0.7907s\n",
      "Epoch: 0122 loss_train: 0.8864 acc_train: 0.7786 loss_val: 0.8789 acc_val: 0.8200 time: 0.7869s\n",
      "Epoch: 0123 loss_train: 0.8445 acc_train: 0.7786 loss_val: 0.8758 acc_val: 0.8200 time: 0.7799s\n",
      "Epoch: 0124 loss_train: 0.8061 acc_train: 0.7714 loss_val: 0.8727 acc_val: 0.8200 time: 0.7689s\n",
      "Epoch: 0125 loss_train: 0.8575 acc_train: 0.7857 loss_val: 0.8698 acc_val: 0.8167 time: 0.7610s\n",
      "Epoch: 0126 loss_train: 0.9303 acc_train: 0.7357 loss_val: 0.8670 acc_val: 0.8167 time: 0.7670s\n",
      "Epoch: 0127 loss_train: 0.9184 acc_train: 0.7571 loss_val: 0.8643 acc_val: 0.8167 time: 0.7819s\n",
      "Epoch: 0128 loss_train: 0.8472 acc_train: 0.8143 loss_val: 0.8617 acc_val: 0.8167 time: 0.7600s\n",
      "Epoch: 0129 loss_train: 0.8862 acc_train: 0.7643 loss_val: 0.8594 acc_val: 0.8167 time: 0.7939s\n",
      "Epoch: 0130 loss_train: 0.8762 acc_train: 0.7643 loss_val: 0.8571 acc_val: 0.8200 time: 0.8048s\n",
      "Epoch: 0131 loss_train: 0.8663 acc_train: 0.7714 loss_val: 0.8550 acc_val: 0.8200 time: 0.7610s\n",
      "Epoch: 0132 loss_train: 0.8084 acc_train: 0.8143 loss_val: 0.8527 acc_val: 0.8200 time: 0.7729s\n",
      "Epoch: 0133 loss_train: 0.8638 acc_train: 0.7357 loss_val: 0.8504 acc_val: 0.8200 time: 0.7759s\n",
      "Epoch: 0134 loss_train: 0.8403 acc_train: 0.7929 loss_val: 0.8478 acc_val: 0.8200 time: 0.7857s\n",
      "Epoch: 0135 loss_train: 0.7871 acc_train: 0.8071 loss_val: 0.8452 acc_val: 0.8200 time: 0.7669s\n",
      "Epoch: 0136 loss_train: 0.7706 acc_train: 0.8143 loss_val: 0.8426 acc_val: 0.8200 time: 0.7739s\n",
      "Epoch: 0137 loss_train: 0.7126 acc_train: 0.8286 loss_val: 0.8402 acc_val: 0.8200 time: 0.7929s\n",
      "Epoch: 0138 loss_train: 0.8118 acc_train: 0.8214 loss_val: 0.8378 acc_val: 0.8200 time: 0.7620s\n",
      "Epoch: 0139 loss_train: 0.8016 acc_train: 0.8214 loss_val: 0.8352 acc_val: 0.8233 time: 0.7739s\n",
      "Epoch: 0140 loss_train: 0.8033 acc_train: 0.7929 loss_val: 0.8327 acc_val: 0.8233 time: 0.7709s\n",
      "Epoch: 0141 loss_train: 0.8164 acc_train: 0.7643 loss_val: 0.8302 acc_val: 0.8233 time: 0.7630s\n",
      "Epoch: 0142 loss_train: 0.9391 acc_train: 0.7500 loss_val: 0.8279 acc_val: 0.8233 time: 0.7879s\n",
      "Epoch: 0143 loss_train: 0.7493 acc_train: 0.8357 loss_val: 0.8256 acc_val: 0.8233 time: 0.7702s\n",
      "Epoch: 0144 loss_train: 0.8524 acc_train: 0.7714 loss_val: 0.8232 acc_val: 0.8233 time: 0.7610s\n",
      "Epoch: 0145 loss_train: 0.8411 acc_train: 0.7714 loss_val: 0.8210 acc_val: 0.8233 time: 0.7799s\n",
      "Epoch: 0146 loss_train: 0.7727 acc_train: 0.8286 loss_val: 0.8186 acc_val: 0.8233 time: 0.7899s\n",
      "Epoch: 0147 loss_train: 0.8491 acc_train: 0.7714 loss_val: 0.8162 acc_val: 0.8233 time: 0.8029s\n",
      "Epoch: 0148 loss_train: 0.8424 acc_train: 0.7571 loss_val: 0.8138 acc_val: 0.8233 time: 0.8388s\n",
      "Epoch: 0149 loss_train: 0.7984 acc_train: 0.8071 loss_val: 0.8115 acc_val: 0.8233 time: 0.7949s\n",
      "Epoch: 0150 loss_train: 0.7936 acc_train: 0.8071 loss_val: 0.8093 acc_val: 0.8233 time: 0.8228s\n",
      "Epoch: 0151 loss_train: 0.7720 acc_train: 0.8071 loss_val: 0.8073 acc_val: 0.8233 time: 0.7809s\n",
      "Epoch: 0152 loss_train: 0.7847 acc_train: 0.7714 loss_val: 0.8051 acc_val: 0.8233 time: 0.8289s\n",
      "Epoch: 0153 loss_train: 0.7306 acc_train: 0.8143 loss_val: 0.8029 acc_val: 0.8233 time: 0.9764s\n",
      "Epoch: 0154 loss_train: 0.8914 acc_train: 0.7429 loss_val: 0.8008 acc_val: 0.8233 time: 0.8338s\n",
      "Epoch: 0155 loss_train: 0.7409 acc_train: 0.8071 loss_val: 0.7986 acc_val: 0.8233 time: 0.7799s\n",
      "Epoch: 0156 loss_train: 0.7680 acc_train: 0.8000 loss_val: 0.7966 acc_val: 0.8200 time: 0.8048s\n",
      "Epoch: 0157 loss_train: 0.7699 acc_train: 0.8143 loss_val: 0.7947 acc_val: 0.8200 time: 0.8288s\n",
      "Epoch: 0158 loss_train: 0.7758 acc_train: 0.8000 loss_val: 0.7930 acc_val: 0.8233 time: 0.8198s\n",
      "Epoch: 0159 loss_train: 0.7035 acc_train: 0.8357 loss_val: 0.7913 acc_val: 0.8200 time: 0.8258s\n",
      "Epoch: 0160 loss_train: 0.8310 acc_train: 0.7714 loss_val: 0.7896 acc_val: 0.8200 time: 0.8208s\n",
      "Epoch: 0161 loss_train: 0.8145 acc_train: 0.7786 loss_val: 0.7877 acc_val: 0.8200 time: 0.8258s\n",
      "Epoch: 0162 loss_train: 0.8858 acc_train: 0.7214 loss_val: 0.7864 acc_val: 0.8200 time: 0.7979s\n",
      "Epoch: 0163 loss_train: 0.7742 acc_train: 0.7857 loss_val: 0.7850 acc_val: 0.8200 time: 0.7689s\n",
      "Epoch: 0164 loss_train: 0.7542 acc_train: 0.7571 loss_val: 0.7837 acc_val: 0.8200 time: 0.8168s\n",
      "Epoch: 0165 loss_train: 0.8049 acc_train: 0.7714 loss_val: 0.7824 acc_val: 0.8200 time: 0.7654s\n",
      "Epoch: 0166 loss_train: 0.8252 acc_train: 0.7929 loss_val: 0.7809 acc_val: 0.8200 time: 0.7869s\n",
      "Epoch: 0167 loss_train: 0.7572 acc_train: 0.8071 loss_val: 0.7794 acc_val: 0.8200 time: 0.7769s\n",
      "Epoch: 0168 loss_train: 0.7137 acc_train: 0.8214 loss_val: 0.7780 acc_val: 0.8200 time: 0.7689s\n",
      "Epoch: 0169 loss_train: 0.7268 acc_train: 0.7786 loss_val: 0.7768 acc_val: 0.8200 time: 0.7839s\n",
      "Epoch: 0170 loss_train: 0.7067 acc_train: 0.8000 loss_val: 0.7756 acc_val: 0.8233 time: 0.7769s\n",
      "Epoch: 0171 loss_train: 0.8525 acc_train: 0.7357 loss_val: 0.7747 acc_val: 0.8233 time: 0.7659s\n",
      "Epoch: 0172 loss_train: 0.7565 acc_train: 0.8143 loss_val: 0.7737 acc_val: 0.8233 time: 0.7729s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0173 loss_train: 0.8669 acc_train: 0.7571 loss_val: 0.7727 acc_val: 0.8233 time: 0.7650s\n",
      "Epoch: 0174 loss_train: 0.8082 acc_train: 0.7714 loss_val: 0.7719 acc_val: 0.8233 time: 0.7757s\n",
      "Epoch: 0175 loss_train: 0.7470 acc_train: 0.7929 loss_val: 0.7710 acc_val: 0.8233 time: 0.7709s\n",
      "Epoch: 0176 loss_train: 0.7714 acc_train: 0.7714 loss_val: 0.7701 acc_val: 0.8233 time: 0.7669s\n",
      "Epoch: 0177 loss_train: 0.8433 acc_train: 0.7571 loss_val: 0.7692 acc_val: 0.8233 time: 0.7843s\n",
      "Epoch: 0178 loss_train: 0.7392 acc_train: 0.7857 loss_val: 0.7682 acc_val: 0.8233 time: 0.7699s\n",
      "Epoch: 0179 loss_train: 0.7736 acc_train: 0.7786 loss_val: 0.7670 acc_val: 0.8200 time: 0.7679s\n",
      "Epoch: 0180 loss_train: 0.7317 acc_train: 0.8143 loss_val: 0.7658 acc_val: 0.8200 time: 0.7909s\n",
      "Epoch: 0181 loss_train: 0.8399 acc_train: 0.7143 loss_val: 0.7649 acc_val: 0.8200 time: 0.7737s\n",
      "Epoch: 0182 loss_train: 0.7178 acc_train: 0.7929 loss_val: 0.7640 acc_val: 0.8200 time: 0.7759s\n",
      "Epoch: 0183 loss_train: 0.7985 acc_train: 0.7929 loss_val: 0.7632 acc_val: 0.8200 time: 0.7769s\n",
      "Epoch: 0184 loss_train: 0.7623 acc_train: 0.8143 loss_val: 0.7622 acc_val: 0.8200 time: 0.7732s\n",
      "Epoch: 0185 loss_train: 0.7174 acc_train: 0.8714 loss_val: 0.7611 acc_val: 0.8200 time: 0.7612s\n",
      "Epoch: 0186 loss_train: 0.6858 acc_train: 0.8214 loss_val: 0.7602 acc_val: 0.8200 time: 0.7739s\n",
      "Epoch: 0187 loss_train: 0.7009 acc_train: 0.7857 loss_val: 0.7592 acc_val: 0.8200 time: 0.8101s\n",
      "Epoch: 0188 loss_train: 0.7994 acc_train: 0.7857 loss_val: 0.7584 acc_val: 0.8200 time: 0.7600s\n",
      "Epoch: 0189 loss_train: 0.6947 acc_train: 0.8286 loss_val: 0.7574 acc_val: 0.8200 time: 0.7749s\n",
      "Epoch: 0190 loss_train: 0.7364 acc_train: 0.8000 loss_val: 0.7561 acc_val: 0.8200 time: 0.7789s\n",
      "Epoch: 0191 loss_train: 0.6567 acc_train: 0.8571 loss_val: 0.7548 acc_val: 0.8200 time: 0.7724s\n",
      "Epoch: 0192 loss_train: 0.7990 acc_train: 0.7929 loss_val: 0.7534 acc_val: 0.8200 time: 0.7735s\n",
      "Epoch: 0193 loss_train: 0.6710 acc_train: 0.8286 loss_val: 0.7520 acc_val: 0.8200 time: 0.7689s\n",
      "Epoch: 0194 loss_train: 0.7492 acc_train: 0.8429 loss_val: 0.7507 acc_val: 0.8200 time: 0.7869s\n",
      "Epoch: 0195 loss_train: 0.7049 acc_train: 0.7929 loss_val: 0.7494 acc_val: 0.8233 time: 0.7869s\n",
      "Epoch: 0196 loss_train: 0.7719 acc_train: 0.7857 loss_val: 0.7483 acc_val: 0.8233 time: 0.7653s\n",
      "Epoch: 0197 loss_train: 0.7763 acc_train: 0.7786 loss_val: 0.7472 acc_val: 0.8233 time: 0.7839s\n",
      "Epoch: 0198 loss_train: 0.6924 acc_train: 0.8500 loss_val: 0.7461 acc_val: 0.8233 time: 0.7640s\n",
      "Epoch: 0199 loss_train: 0.7913 acc_train: 0.7643 loss_val: 0.7451 acc_val: 0.8233 time: 0.7719s\n",
      "Epoch: 0200 loss_train: 0.7021 acc_train: 0.8143 loss_val: 0.7442 acc_val: 0.8233 time: 0.7859s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 165.1056s\n"
     ]
    }
   ],
   "source": [
    "t_total = time.time()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    loss_values.append(train(epoch))\n",
    "    \n",
    "    torch.save(model.state_dict(), \"{}.pkl\".format(epoch))\n",
    "    if loss_values[-1] < best:\n",
    "        best = loss_values[-1]\n",
    "        best_epoch = epoch\n",
    "        bad_counter = 0\n",
    "    else:\n",
    "        bad_counter += 1\n",
    "\n",
    "    if bad_counter == args.patience:\n",
    "        break\n",
    "        \n",
    "    files = glob.glob(\"*.pkl\")\n",
    "    for file in files:\n",
    "        epoch_nb = int(file.split(\".\")[0])\n",
    "        if epoch_nb < best_epoch:\n",
    "            os.remove(file)\n",
    "            \n",
    "files = glob.glob('*.pkl')\n",
    "for file in files:\n",
    "    epoch_nb = int(file.split('.')[0])\n",
    "    if epoch_nb > best_epoch:\n",
    "        os.remove(file)\n",
    "        \n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 199th epoch\n"
     ]
    }
   ],
   "source": [
    "# Restore best model\n",
    "print('Loading {}th epoch'.format(best_epoch))\n",
    "model.load_state_dict(torch.load('{}.pkl'.format(best_epoch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 112.00 MiB (GPU 0; 2.00 GiB total capacity; 1.37 GiB already allocated; 60.12 MiB free; 547.50 KiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-e330a2f0e087>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Testing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcompute_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-36-2a818e3e59bd>\u001b[0m in \u001b[0;36mcompute_test\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcompute_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mloss_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx_test\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx_test\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0macc_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx_test\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx_test\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sss\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-85effc925f77>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, adj)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0matt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madj\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0matt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattentions\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0melu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout_att\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-85effc925f77>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0matt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madj\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0matt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattentions\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0melu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mout_att\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\sss\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-982f94dfc492>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, adj)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0medge\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 112.00 MiB (GPU 0; 2.00 GiB total capacity; 1.37 GiB already allocated; 60.12 MiB free; 547.50 KiB cached)"
     ]
    }
   ],
   "source": [
    "# Testing\n",
    "compute_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
