{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 动态图表示\n",
    "- 由简到难分为四个等级【按时间粒度】\n",
    "- 1 Static: 不关注图中的动态性信息，而将其作为一张静态图同等处理\n",
    "- 2 Edge Weighted: 动态信息只是作为一张静态图中的节点或者边的labels而存在\n",
    "- 3 Discrete: 离散表示使用一组有序的图（快照）来表示动态图【--】\n",
    "- 4 Continuous Networks: 使用确切时间信息的表示形式【--】\n",
    "--------------------------------------------------------------------------\n",
    "- 由简到难分为四个等级【按链路持续时间】\n",
    "- 1 Interaction: 一种时间网络，其中的链接是瞬时事件，例如电子邮件发送\n",
    "- 2 Temporal Networks: 边有一定的持续时间，但比较短，例如人与人的谈话\n",
    "- 3 Evolving Networks: 链持续存在的时间长，例如雇佣关系\n",
    "- 4 Strictly Evolving Networks: 链接出现后会一直出现，例如引文网络\n",
    "--------------------------------------------------------------------------\n",
    "- 由简到难分为四个等级【按节点动态表示】\n",
    "- 1 Static: 节点数量在一段时间内保持不变\n",
    "- 2 Dynamic: 节点可能出现和消失\n",
    "- 3 Growing Networks: 只可能出现节点的网络"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DySAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import itertools\n",
    "import argparse\n",
    "import dill\n",
    "import scipy\n",
    "import random\n",
    "import copy\n",
    "\n",
    "from collections import defaultdict\n",
    "from itertools import islice, chain\n",
    "from typing import DefaultDict\n",
    "from torch.functional import Tensor\n",
    "from torch_geometric.data import Data\n",
    "import torch_geometric as tg\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from scipy.sparse import csr_matrix\n",
    "import scipy.sparse as sp\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import dateutil.parser\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.loss import BCEWithLogitsLoss\n",
    "\n",
    "from torch_geometric.utils import softmax\n",
    "from torch_scatter import scatter\n",
    "\n",
    "import copy\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from __future__ import division, print_function\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x24aa593f4e0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lines_per_n(f, n):\n",
    "    for line in f:\n",
    "        yield ''.join(chain([line], itertools.islice(f, n - 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDateTimeFromISO8601String(s):\n",
    "    d = dateutil.parser.parse(s)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_data = defaultdict(lambda: ())\n",
    "with open(r\"C:\\Users\\sss\\Desktop\\DySAT_pytorch-main\\raw_data\\Enron/vis.graph.nodeList.json\") as f:\n",
    "    for chunk in lines_per_n(f, 5):\n",
    "        chunk = chunk.split(\"\\n\")\n",
    "        id_string = chunk[1].split(\":\")[1]\n",
    "        x = [x.start() for x in re.finditer('\\\"', id_string)]\n",
    "        id = id_string[x[0] + 1: x[1]]\n",
    "        \n",
    "        name_string = chunk[2].split(\":\")[1]\n",
    "        x = [x.start() for x in re.finditer('\\\"', name_string)]\n",
    "        name = name_string[x[0] + 1: x[1]]\n",
    "        \n",
    "        idx_string = chunk[3].split(\":\")[1]\n",
    "        x1 = idx_string.find(\"(\")\n",
    "        x2 = idx_string.find(\")\")\n",
    "        idx = idx_string[x1 + 1: x2]\n",
    "        \n",
    "        # print(\"ID:{}, IDX:{:<4}, NAME:{}\".format(id, idx, name))\n",
    "        node_data[name] = (id, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1998-11-13 12:07:00+00:00 2002-06-21 22:40:19+00:00\n",
      "# interactions 22784\n"
     ]
    }
   ],
   "source": [
    "links = []\n",
    "ts = []\n",
    "with open(r\"C:\\Users\\sss\\Desktop\\DySAT_pytorch-main\\raw_data\\Enron/vis.digraph.allEdges.json\") as f:\n",
    "    for chunk in lines_per_n(f, 5):\n",
    "        chunk = chunk.split(\"\\n\")\n",
    "        \n",
    "        name_string = chunk[2].split(\":\")[1]\n",
    "        x = [x.start() for x in re.finditer('\\\"', name_string)]\n",
    "        from_id, to_id = name_string[x[0]+1:x[1]].split(\"_\")\n",
    "        \n",
    "        time_string = chunk[3].split(\"ISODate\")[1]\n",
    "        x = [x.start() for x in re.finditer('\\\"', time_string)]\n",
    "        timestamp = getDateTimeFromISO8601String(time_string[x[0] + 1: x[1]])\n",
    "        ts.append(timestamp)\n",
    "        links.append((from_id, to_id, timestamp))\n",
    "print (min(ts), max(ts))\n",
    "print (\"# interactions\", len(links))\n",
    "links.sort(key = lambda x: x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spliting Time Interval: \n",
      "Start Time : 1999-06-01 12:07:00+00:00, End Time : 2001-12-03 22:40:19+00:00\n"
     ]
    }
   ],
   "source": [
    "# split edges\n",
    "SLICE_MONTHS = 2\n",
    "START_DATE = min(ts) + timedelta(200)\n",
    "END_DATE = max(ts) - timedelta(200)\n",
    "print(\"Spliting Time Interval: \\nStart Time : {}, End Time : {}\".format(START_DATE, END_DATE))\n",
    "    \n",
    "slice_links = defaultdict(lambda: nx.MultiGraph())    \n",
    "for (a, b, time) in links:\n",
    "    datetime_object = time\n",
    "    if datetime_object == END_DATE:\n",
    "        months_diff = (END_DATE - START_DATE).days // 30\n",
    "    else:\n",
    "        months_diff = (datetime_object - START_DATE).days // 30\n",
    "    slice_id = months_diff // SLICE_MONTHS\n",
    "    slice_id = max(slice_id, 0)\n",
    "    \n",
    "    if slice_id not in slice_links.keys():\n",
    "        slice_links[slice_id] = nx.MultiGraph()\n",
    "        if slice_id > 0:\n",
    "            slice_links[slice_id].add_nodes_from(slice_links[slice_id-1].nodes(data=True))\n",
    "            assert (len(slice_links[slice_id].edges()) == 0)\n",
    "    slice_links[slice_id].add_edge(a,b, date=datetime_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print statics of each graph\n",
    "used_nodes = []\n",
    "for id, slice in slice_links.items():\n",
    "    # print(\"In snapshoot {:<2}, #Nodes={:<5}, #Edges={:<5}\".format(id, slice.number_of_nodes(), slice.number_of_edges()))\n",
    "    for node in slice.nodes():\n",
    "        if not node in used_nodes:\n",
    "            used_nodes.append(node)\n",
    "            \n",
    "# remap nodes in graphs. Cause start time is not zero, the node index is not consistent\n",
    "nodes_consistent_map = {node:idx for idx, node in enumerate(used_nodes)}\n",
    "for id, slice in slice_links.items():\n",
    "    slice_links[id] = nx.relabel_nodes(slice, nodes_consistent_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot features\n",
    "onehot = np.identity(slice_links[max(slice_links.keys())].number_of_nodes())\n",
    "graphs = []\n",
    "for id, slice in slice_links.items():\n",
    "    tmp_feature = []\n",
    "    for node in slice.nodes():\n",
    "        tmp_feature.append(onehot[node])\n",
    "    slice.graph[\"feature\"] = csr_matrix(tmp_feature)\n",
    "    graphs.append(slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Data Saved at C:\\Users\\sss\\Desktop\\DySAT_pytorch-main\\data\\Enron/graph.pkl\n"
     ]
    }
   ],
   "source": [
    "save_path = r\"C:\\Users\\sss\\Desktop\\DySAT_pytorch-main\\data\\Enron/graph.pkl\"\n",
    "with open(save_path, \"wb\") as f:\n",
    "    pkl.dump(graphs, f)\n",
    "print(\"Processed Data Saved at {}\".format(save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Walk\n",
    "- Random walk sampling code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph_RandomWalk():\n",
    "    def __init__(self, nx_G, is_directed, p, q):\n",
    "        self.G = nx_G\n",
    "        self.is_directed = is_directed\n",
    "        self.p = p\n",
    "        self.q = q\n",
    "        \n",
    "    def node2vec_walk(self, walk_length, start_node):\n",
    "        \"\"\" Simulate a random walk starting from start node. \"\"\"\n",
    "        G = self.G\n",
    "        alias_nodes = self.alias_nodes\n",
    "        alias_edges = self.alias_edges\n",
    "\n",
    "        walk = [start_node]\n",
    "\n",
    "        while len(walk) < walk_length:\n",
    "            cur = walk[-1]\n",
    "            cur_nbrs = sorted(G.neighbors(cur))\n",
    "            if len(cur_nbrs) > 0:\n",
    "                if len(walk) == 1:\n",
    "                    walk.append(cur_nbrs[alias_draw(alias_nodes[cur][0], alias_nodes[cur][1])])\n",
    "                else:\n",
    "                    prev = walk[-2]\n",
    "                    next = cur_nbrs[alias_draw(alias_edges[(prev, cur)][0],\n",
    "                        alias_edges[(prev, cur)][1])]\n",
    "                    walk.append(next)\n",
    "            else:\n",
    "                break\n",
    "        return walk\n",
    "    \n",
    "    def simulate_walks(self, num_walks, walk_length):\n",
    "        \"\"\" Repeatedly simulate random walks from each node. \"\"\"\n",
    "        G = self.G\n",
    "        walks = []\n",
    "        nodes = list(G.nodes())\n",
    "        for walk_iter in range(num_walks):\n",
    "            random.shuffle(nodes)\n",
    "            for node in nodes:\n",
    "                walks.append(self.node2vec_walk(walk_length=walk_length, start_node=node))\n",
    "\n",
    "        return walks\n",
    "    \n",
    "    def get_alias_edge(self, src, dst):\n",
    "        \"\"\" Get the alias edge setup lists for a given edge. \"\"\"\n",
    "        G = self.G\n",
    "        p = self.p\n",
    "        q = self.q\n",
    "\n",
    "        unnormalized_probs = []\n",
    "        for dst_nbr in sorted(G.neighbors(dst)):\n",
    "            if dst_nbr == src:\n",
    "                unnormalized_probs.append(G[dst][dst_nbr]['weight']/p)\n",
    "            elif G.has_edge(dst_nbr, src):\n",
    "                unnormalized_probs.append(G[dst][dst_nbr]['weight'])\n",
    "            else:\n",
    "                unnormalized_probs.append(G[dst][dst_nbr]['weight']/q)\n",
    "        norm_const = sum(unnormalized_probs)\n",
    "        normalized_probs =  [float(u_prob)/norm_const for u_prob in unnormalized_probs]\n",
    "\n",
    "        return alias_setup(normalized_probs)\n",
    "    \n",
    "    def preprocess_transition_probs(self):\n",
    "        \"\"\" Preprocessing of transition probabilities for guiding the random walks. \"\"\"\n",
    "        G = self.G\n",
    "        is_directed = self.is_directed\n",
    "\n",
    "        alias_nodes = {}\n",
    "        for node in G.nodes():\n",
    "            unnormalized_probs = [G[node][nbr]['weight'] for nbr in sorted(G.neighbors(node))]\n",
    "            norm_const = sum(unnormalized_probs)\n",
    "            normalized_probs =  [float(u_prob)/norm_const for u_prob in unnormalized_probs]\n",
    "            alias_nodes[node] = alias_setup(normalized_probs)\n",
    "\n",
    "        alias_edges = {}\n",
    "        triads = {}\n",
    "\n",
    "        if is_directed:\n",
    "            for edge in G.edges():\n",
    "                alias_edges[edge] = self.get_alias_edge(edge[0], edge[1])\n",
    "        else:\n",
    "            for edge in G.edges():\n",
    "                alias_edges[edge] = self.get_alias_edge(edge[0], edge[1])\n",
    "                alias_edges[(edge[1], edge[0])] = self.get_alias_edge(edge[1], edge[0])\n",
    "\n",
    "        self.alias_nodes = alias_nodes\n",
    "        self.alias_edges = alias_edges\n",
    "\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alias_setup(probs):\n",
    "    \"\"\" Compute utility lists for non-uniform sampling from discrete distributions. \"\"\"\n",
    "    K = len(probs)\n",
    "    q = np.zeros(K)\n",
    "    J = np.zeros(K, dtype=np.int)\n",
    "\n",
    "    smaller = []\n",
    "    larger = []\n",
    "    for kk, prob in enumerate(probs):\n",
    "        q[kk] = K*prob\n",
    "        if q[kk] < 1.0:\n",
    "            smaller.append(kk)\n",
    "        else:\n",
    "            larger.append(kk)\n",
    "\n",
    "    while len(smaller) > 0 and len(larger) > 0:\n",
    "        small = smaller.pop()\n",
    "        large = larger.pop()\n",
    "\n",
    "        J[small] = large\n",
    "        q[large] = q[large] + q[small] - 1.0\n",
    "        if q[large] < 1.0:\n",
    "            smaller.append(large)\n",
    "        else:\n",
    "            larger.append(large)\n",
    "\n",
    "    return J, q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alias_draw(J, q):\n",
    "    \"\"\" Draw sample from a non-uniform discrete distribution using alias sampling. \"\"\"\n",
    "    K = len(J)\n",
    "\n",
    "    kk = int(np.floor(np.random.rand()*K))\n",
    "    if np.random.rand() < q[kk]:\n",
    "        return kk\n",
    "    else:\n",
    "        return J[kk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_random_walks_n2v(graph, adj, num_walks, walk_len):\n",
    "    \"\"\" \n",
    "    In: Graph and list of nodes\n",
    "    Out: (target, context) pairs from random walk sampling using the sampling strategy of node2vec (deepwalk)\n",
    "    \"\"\"\n",
    "    nx_G = nx.Graph()\n",
    "    for e in graph.edges():\n",
    "        nx_G.add_edge(e[0], e[1])\n",
    "    for edge in graph.edges():\n",
    "        nx_G[edge[0]][edge[1]]['weight'] = adj[edge[0], edge[1]]\n",
    "\n",
    "    G = Graph_RandomWalk(nx_G, False, 1.0, 1.0)\n",
    "    G.preprocess_transition_probs()\n",
    "    walks = G.simulate_walks(num_walks, walk_len)\n",
    "    WINDOW_SIZE = 10\n",
    "    pairs = defaultdict(list)\n",
    "    pairs_cnt = 0\n",
    "    for walk in walks:\n",
    "        for word_index, word in enumerate(walk):\n",
    "            for nb_word in walk[max(word_index - WINDOW_SIZE, 0): min(word_index + WINDOW_SIZE, len(walk)) + 1]:\n",
    "                if nb_word != word:\n",
    "                    pairs[word].append(nb_word)\n",
    "                    pairs_cnt += 1\n",
    "    # print(\"# nodes with random walk samples: {}\".format(len(pairs)))\n",
    "    # print(\"# sampled pairs: {}\".format(pairs_cnt))\n",
    "    \n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixed_unigram_candidate_sampler(true_clasees, num_true, num_sampled, unique, distortion, unigrams):\n",
    "    \"\"\" TODO: implementate distortion to unigrams \"\"\"\n",
    "    assert true_clasees.shape[1] == num_true\n",
    "    samples = []\n",
    "    for i in range(true_clasees.shape[0]):\n",
    "        dist = copy.deepcopy(unigrams)\n",
    "        candidate = list(range(len(dist)))\n",
    "        taboo = true_clasees[i].cpu().tolist()\n",
    "        for tabo in sorted(taboo, reverse=True):\n",
    "            candidate.remove(tabo)\n",
    "            dist.pop(tabo)\n",
    "        sample = np.random.choice(candidate, size=num_sampled, replace=unique, p=dist/np.sum(dist))\n",
    "        samples.append(sample)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_device(batch, device):\n",
    "    feed_dict = copy.deepcopy(batch)\n",
    "    node_1, node_2, node_2_negative, graphs = feed_dict.values()\n",
    "    # to device\n",
    "    feed_dict[\"node_1\"] = [x.to(device) for x in node_1]\n",
    "    feed_dict[\"node_2\"] = [x.to(device) for x in node_2]\n",
    "    feed_dict[\"node_2_neg\"] = [x.to(device) for x in node_2_negative]\n",
    "    feed_dict[\"graphs\"] = [g.to(device) for g in graphs]\n",
    "\n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graphs(dataset_str):\n",
    "    \"\"\" Load graph snapshots given the name of dataset \"\"\"\n",
    "    with open(r\"C:\\Users\\sss\\Desktop\\DySAT_pytorch-main/data/{}/{}\".format(dataset_str, \"graph.pkl\"), \"rb\") as f:\n",
    "        graphs = pkl.load(f)\n",
    "    print(\"Loaded {} graphs \".format(len(graphs)))\n",
    "    adjs = [nx.adjacency_matrix(g) for g in graphs]\n",
    "    return graphs, adjs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_pairs(graphs, adjs):\n",
    "    \"\"\" Load/generate context pairs for each snapshot through random walk sampling.\"\"\"\n",
    "    print(\"Computing training pairs ...\")\n",
    "    context_pairs_train = []\n",
    "    for i in range(len(graphs)):\n",
    "        context_pairs_train.append(run_random_walks_n2v(graphs[i], adjs[i], num_walks=10, walk_len=20))\n",
    "\n",
    "    return context_pairs_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evaluation_data(graphs):\n",
    "    \"\"\" Load train/val/test examples to evaluate link prediction performance \"\"\"\n",
    "    eval_idx = len(graphs) - 2\n",
    "    eval_graph = graphs[eval_idx]\n",
    "    next_graph = graphs[eval_idx + 1]\n",
    "    print(\"Generating eval data ....\")\n",
    "    train_edges, train_edges_false, val_edges, val_edges_false, test_edges, test_edges_false = create_data_splits(eval_graph, next_graph, val_mask_fraction=0.2, test_mask_fraction=0.6)\n",
    "    \n",
    "    return train_edges, train_edges_false, val_edges, val_edges_false, test_edges, test_edges_false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_splits(graph, next_graph, val_mask_fraction=0.2, test_mask_fraction=0.6):\n",
    "    edges_next = np.array(list(nx.Graph(next_graph).edges()))\n",
    "    edges_positive = []   # Constraint to restrict new links to existing nodes.\n",
    "    for e in edges_next:\n",
    "        if graph.has_node(e[0]) and graph.has_node(e[1]):\n",
    "            edges_positive.append(e)\n",
    "    edges_positive = np.array(edges_positive) # [E, 2]\n",
    "    edges_negative = negative_sample(edges_positive, graph.number_of_nodes(), next_graph)\n",
    "    \n",
    "\n",
    "    train_edges_pos, test_pos, train_edges_neg, test_neg = train_test_split(edges_positive, edges_negative, test_size = val_mask_fraction + test_mask_fraction)\n",
    "    val_edges_pos, test_edges_pos, val_edges_neg, test_edges_neg = train_test_split(test_pos, test_neg, test_size = test_mask_fraction / (test_mask_fraction + val_mask_fraction))\n",
    "\n",
    "    return train_edges_pos, train_edges_neg, val_edges_pos, val_edges_neg, test_edges_pos, test_edges_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_sample(edges_pos, nodes_num, next_graph):\n",
    "    edges_neg = []\n",
    "    while len(edges_neg) < len(edges_pos):\n",
    "        idx_i = np.random.randint(0, nodes_num)\n",
    "        idx_j = np.random.randint(0, nodes_num)\n",
    "        if idx_i == idx_j:\n",
    "            continue\n",
    "        if next_graph.has_edge(idx_i, idx_j) or next_graph.has_edge(idx_j, idx_i):\n",
    "            continue\n",
    "        if edges_neg:\n",
    "            if [idx_i, idx_j] in edges_neg or [idx_j, idx_i] in edges_neg:\n",
    "                continue\n",
    "        edges_neg.append([idx_i, idx_j])\n",
    "    return edges_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, args, graphs, features, adjs,  context_pairs):\n",
    "        super(MyDataset, self).__init__()\n",
    "        self.args = args\n",
    "        self.graphs = graphs\n",
    "        self.features = [self._preprocess_features(feat) for feat in features]\n",
    "        self.adjs = [self._normalize_graph_gcn(a)  for a  in adjs]\n",
    "        self.time_steps = args.time_steps\n",
    "        self.context_pairs = context_pairs\n",
    "        self.max_positive = args.neg_sample_size\n",
    "        self.train_nodes = list(self.graphs[self.time_steps-1].nodes()) # all nodes in the graph.\n",
    "        self.min_t = max(self.time_steps - self.args.window - 1, 0) if args.window > 0 else 0\n",
    "        self.degs = self.construct_degs()\n",
    "        self.pyg_graphs = self._build_pyg_graphs()\n",
    "        self.__createitems__()\n",
    "\n",
    "    def _normalize_graph_gcn(self, adj):\n",
    "        \"\"\"GCN-based normalization of adjacency matrix (scipy sparse format). Output is in tuple format\"\"\"\n",
    "        adj = sp.coo_matrix(adj, dtype=np.float32)\n",
    "        adj_ = adj + sp.eye(adj.shape[0], dtype=np.float32)\n",
    "        rowsum = np.array(adj_.sum(1), dtype=np.float32)\n",
    "        degree_mat_inv_sqrt = sp.diags(np.power(rowsum, -0.5).flatten(), dtype=np.float32)\n",
    "        adj_normalized = adj_.dot(degree_mat_inv_sqrt).transpose().dot(degree_mat_inv_sqrt).tocoo()\n",
    "        return adj_normalized\n",
    "\n",
    "    def _preprocess_features(self, features):\n",
    "        \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
    "        features = np.array(features.todense())\n",
    "        rowsum = np.array(features.sum(1))\n",
    "        r_inv = np.power(rowsum, -1).flatten()\n",
    "        r_inv[np.isinf(r_inv)] = 0.\n",
    "        r_mat_inv = sp.diags(r_inv)\n",
    "        features = r_mat_inv.dot(features)\n",
    "        return features\n",
    "\n",
    "    def construct_degs(self):\n",
    "        \"\"\" Compute node degrees in each graph snapshot.\"\"\"\n",
    "        # different from the original implementation\n",
    "        # degree is counted using multi graph\n",
    "        degs = []\n",
    "        for i in range(self.min_t, self.time_steps):\n",
    "            G = self.graphs[i]\n",
    "            deg = []\n",
    "            for nodeid in G.nodes():\n",
    "                deg.append(G.degree(nodeid))\n",
    "            degs.append(deg)\n",
    "        return degs\n",
    "\n",
    "    def _build_pyg_graphs(self):\n",
    "        pyg_graphs = []\n",
    "        for feat, adj in zip(self.features, self.adjs):\n",
    "            x = torch.Tensor(feat)\n",
    "            edge_index, edge_weight = tg.utils.from_scipy_sparse_matrix(adj)\n",
    "            data = Data(x=x, edge_index=edge_index, edge_weight=edge_weight)\n",
    "            pyg_graphs.append(data)\n",
    "        return pyg_graphs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_nodes)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        node = self.train_nodes[index]\n",
    "        return self.data_items[node]\n",
    "    \n",
    "    def __createitems__(self):\n",
    "        self.data_items = {}\n",
    "        for node in list(self.graphs[self.time_steps-1].nodes()):\n",
    "            feed_dict = {}\n",
    "            node_1_all_time = []\n",
    "            node_2_all_time = []\n",
    "            for t in range(self.min_t, self.time_steps):\n",
    "                node_1 = []\n",
    "                node_2 = []\n",
    "                if len(self.context_pairs[t][node]) > self.max_positive:\n",
    "                    node_1.extend([node]* self.max_positive)\n",
    "                    node_2.extend(np.random.choice(self.context_pairs[t][node], self.max_positive, replace=False))\n",
    "                else:\n",
    "                    node_1.extend([node]* len(self.context_pairs[t][node]))\n",
    "                    node_2.extend(self.context_pairs[t][node])\n",
    "                assert len(node_1) == len(node_2)\n",
    "                node_1_all_time.append(node_1)\n",
    "                node_2_all_time.append(node_2)\n",
    "\n",
    "            node_1_list = [torch.LongTensor(node) for node in node_1_all_time]\n",
    "            node_2_list = [torch.LongTensor(node) for node in node_2_all_time]\n",
    "            node_2_negative = []\n",
    "            for t in range(len(node_2_list)):\n",
    "                degree = self.degs[t]\n",
    "                node_positive = node_2_list[t][:, None]\n",
    "                node_negative = fixed_unigram_candidate_sampler(\n",
    "                    true_clasees=node_positive,\n",
    "                    num_true=1,\n",
    "                    num_sampled=self.args.neg_sample_size,\n",
    "                    unique=False,\n",
    "                    distortion=0.75,\n",
    "                    unigrams=degree\n",
    "                )\n",
    "                node_2_negative.append(node_negative)\n",
    "            node_2_neg_list = [torch.LongTensor(node) for node in node_2_negative]\n",
    "            feed_dict['node_1']=node_1_list\n",
    "            feed_dict['node_2']=node_2_list\n",
    "            feed_dict['node_2_neg']=node_2_neg_list\n",
    "            feed_dict[\"graphs\"] = self.pyg_graphs\n",
    "        \n",
    "            self.data_items[node] = feed_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(samples):\n",
    "        batch_dict = {}\n",
    "        for key in [\"node_1\", \"node_2\", \"node_2_neg\"]:\n",
    "            data_list = []\n",
    "            for sample in samples:\n",
    "                data_list.append(sample[key])\n",
    "            concate = []\n",
    "            for t in range(len(data_list[0])):\n",
    "                concate.append(torch.cat([data[t] for data in data_list]))\n",
    "            batch_dict[key] = concate\n",
    "        batch_dict[\"graphs\"] = samples[0][\"graphs\"]\n",
    "        return batch_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(test_results, output_name, model_name, dataset, time_steps, mod='val'):\n",
    "    \"\"\"Output result scores to a csv file for result logging\"\"\"\n",
    "    with open(output_name, 'a+') as f:\n",
    "        for op in test_results:\n",
    "            print(\"{} results ({})\".format(model_name, mod), test_results[op])\n",
    "            _, best_auc = test_results[op]\n",
    "            f.write(\"{},{},{},{},{},{},{}\\n\".format(dataset, time_steps, model_name, op, mod, \"AUC\", best_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_link_score(fu, fv, operator):\n",
    "    \"\"\"Given a pair of embeddings, compute link feature based on operator (such as Hadammad product, etc.)\"\"\"\n",
    "    fu = np.array(fu)\n",
    "    fv = np.array(fv)\n",
    "    if operator == \"HAD\":\n",
    "        return np.multiply(fu, fv)\n",
    "    else:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_link_feats(links, source_embeddings, target_embeddings, operator):\n",
    "    \"\"\"Compute link features for a list of pairs\"\"\"\n",
    "    features = []\n",
    "    for l in links:\n",
    "        a, b = l[0], l[1]\n",
    "        f = get_link_score(source_embeddings[a], target_embeddings[b], operator)\n",
    "        features.append(f)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_split(train_pos, train_neg, val_pos, val_neg, test_pos, test_neg):\n",
    "    \"\"\" Randomly split a given set of train, val and test examples\"\"\"\n",
    "    all_data_pos = []\n",
    "    all_data_neg = []\n",
    "\n",
    "    all_data_pos.extend(train_pos)\n",
    "    all_data_neg.extend(train_neg)\n",
    "    all_data_pos.extend(test_pos)\n",
    "    all_data_neg.extend(test_neg)\n",
    "\n",
    "    # re-define train_pos, train_neg, test_pos, test_neg.\n",
    "    random.shuffle(all_data_pos)\n",
    "    random.shuffle(all_data_neg)\n",
    "\n",
    "    train_pos = all_data_pos[:int(0.2 * len(all_data_pos))]\n",
    "    train_neg = all_data_neg[:int(0.2 * len(all_data_neg))]\n",
    "\n",
    "    test_pos = all_data_pos[int(0.2 * len(all_data_pos)):]\n",
    "    test_neg = all_data_neg[int(0.2 * len(all_data_neg)):]\n",
    "    # print(\"# train :\", len(train_pos) + len(train_neg), \"# val :\", len(val_pos) + len(val_neg), \"#test :\", len(test_pos) + len(test_neg))\n",
    "    return train_pos, train_neg, val_pos, val_neg, test_pos, test_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_roc_score_t(edges_pos, edges_neg, source_emb, target_emb):\n",
    "    \"\"\"Given test examples, edges_pos: +ve edges, edges_neg: -ve edges, return ROC scores for a given snapshot\"\"\"\n",
    "    def sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # Predict on test set of edges\n",
    "    adj_rec = np.dot(source_emb, target_emb.T)\n",
    "    pred = []\n",
    "    pos = []\n",
    "    for e in edges_pos:\n",
    "        pred.append(sigmoid(adj_rec[e[0], e[1]]))\n",
    "        pos.append(1.0)\n",
    "\n",
    "    pred_neg = []\n",
    "    neg = []\n",
    "    for e in edges_neg:\n",
    "        pred_neg.append(sigmoid(adj_rec[e[0], e[1]]))\n",
    "        neg.append(0.0)\n",
    "\n",
    "    pred_all = np.hstack([pred, pred_neg])\n",
    "    labels_all = np.hstack([np.ones(len(pred)), np.zeros(len(pred_neg))])\n",
    "    roc_score = roc_auc_score(labels_all, pred_all)\n",
    "    return roc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(train_pos, train_neg, val_pos, val_neg, test_pos, test_neg, source_embeds, target_embeds):\n",
    "    \"\"\"Downstream logistic regression classifier to evaluate link prediction\"\"\"\n",
    "    test_results = defaultdict(lambda: [])\n",
    "    val_results = defaultdict(lambda: [])\n",
    "\n",
    "    test_auc = get_roc_score_t(test_pos, test_neg, source_embeds, target_embeds)\n",
    "    val_auc = get_roc_score_t(val_pos, val_neg, source_embeds, target_embeds)\n",
    "\n",
    "    # Compute AUC based on sigmoid(u^T v) without classifier training.\n",
    "    test_results['SIGMOID'].extend([test_auc, test_auc])\n",
    "    val_results['SIGMOID'].extend([val_auc, val_auc])\n",
    "\n",
    "    test_pred_true = defaultdict(lambda: [])\n",
    "    val_pred_true = defaultdict(lambda: [])\n",
    "\n",
    "    for operator in operatorTypes:\n",
    "        train_pos_feats = np.array(get_link_feats(train_pos, source_embeds, target_embeds, operator))\n",
    "        train_neg_feats = np.array(get_link_feats(train_neg, source_embeds, target_embeds, operator))\n",
    "        val_pos_feats = np.array(get_link_feats(val_pos, source_embeds, target_embeds, operator))\n",
    "        val_neg_feats = np.array(get_link_feats(val_neg, source_embeds, target_embeds, operator))\n",
    "        test_pos_feats = np.array(get_link_feats(test_pos, source_embeds, target_embeds, operator))\n",
    "        test_neg_feats = np.array(get_link_feats(test_neg, source_embeds, target_embeds, operator))\n",
    "\n",
    "        train_pos_labels = np.array([1] * len(train_pos_feats))\n",
    "        train_neg_labels = np.array([-1] * len(train_neg_feats))\n",
    "        val_pos_labels = np.array([1] * len(val_pos_feats))\n",
    "        val_neg_labels = np.array([-1] * len(val_neg_feats))\n",
    "\n",
    "        test_pos_labels = np.array([1] * len(test_pos_feats))\n",
    "        test_neg_labels = np.array([-1] * len(test_neg_feats))\n",
    "        train_data = np.vstack((train_pos_feats, train_neg_feats))\n",
    "        train_labels = np.append(train_pos_labels, train_neg_labels)\n",
    "\n",
    "        val_data = np.vstack((val_pos_feats, val_neg_feats))\n",
    "        val_labels = np.append(val_pos_labels, val_neg_labels)\n",
    "\n",
    "        test_data = np.vstack((test_pos_feats, test_neg_feats))\n",
    "        test_labels = np.append(test_pos_labels, test_neg_labels)\n",
    "\n",
    "        logistic = linear_model.LogisticRegression()\n",
    "        logistic.fit(train_data, train_labels)\n",
    "        test_predict = logistic.predict_proba(test_data)[:, 1]\n",
    "        val_predict = logistic.predict_proba(val_data)[:, 1]\n",
    "\n",
    "        test_roc_score = roc_auc_score(test_labels, test_predict)\n",
    "        val_roc_score = roc_auc_score(val_labels, val_predict)\n",
    "\n",
    "        val_results[operator].extend([val_roc_score, val_roc_score])\n",
    "        test_results[operator].extend([test_roc_score, test_roc_score])\n",
    "\n",
    "        val_pred_true[operator].extend(zip(val_predict, val_labels))\n",
    "        test_pred_true[operator].extend(zip(test_predict, test_labels))\n",
    "\n",
    "    return val_results, test_results, val_pred_true, test_pred_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StructuralAttentionLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                input_dim, \n",
    "                output_dim, \n",
    "                n_heads, \n",
    "                attn_drop, \n",
    "                ffd_drop,\n",
    "                residual):\n",
    "        super(StructuralAttentionLayer, self).__init__()\n",
    "        self.out_dim = output_dim // n_heads\n",
    "        self.n_heads = n_heads\n",
    "        self.act = nn.ELU()\n",
    "\n",
    "        self.lin = nn.Linear(input_dim, n_heads * self.out_dim, bias=False)\n",
    "        self.att_l = nn.Parameter(torch.Tensor(1, n_heads, self.out_dim))\n",
    "        self.att_r = nn.Parameter(torch.Tensor(1, n_heads, self.out_dim))\n",
    "        \n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2)\n",
    "\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.ffd_drop = nn.Dropout(ffd_drop)\n",
    "\n",
    "        self.residual = residual\n",
    "        if self.residual:\n",
    "            self.lin_residual = nn.Linear(input_dim, n_heads * self.out_dim, bias=False)\n",
    "\n",
    "\n",
    "    def forward(self, graph):\n",
    "        graph = copy.deepcopy(graph)\n",
    "        edge_index = graph.edge_index\n",
    "        edge_weight = graph.edge_weight.reshape(-1, 1)\n",
    "        H, C = self.n_heads, self.out_dim\n",
    "        x = self.lin(graph.x).view(-1, H, C) # [N, heads, out_dim]\n",
    "        # attention\n",
    "        alpha_l = (x * self.att_l).sum(dim=-1).squeeze() # [N, heads]\n",
    "        alpha_r = (x * self.att_r).sum(dim=-1).squeeze()\n",
    "        alpha_l = alpha_l[edge_index[0]] # [num_edges, heads]\n",
    "        alpha_r = alpha_r[edge_index[1]]\n",
    "        alpha = alpha_r + alpha_l\n",
    "        alpha = edge_weight * alpha\n",
    "        alpha = self.leaky_relu(alpha)\n",
    "        coefficients = softmax(alpha, edge_index[1]) # [num_edges, heads]\n",
    "\n",
    "        # dropout\n",
    "        if self.training:\n",
    "            coefficients = self.attn_drop(coefficients)\n",
    "            x = self.ffd_drop(x)\n",
    "        x_j = x[edge_index[0]]  # [num_edges, heads, out_dim]\n",
    "\n",
    "        # output\n",
    "        out = self.act(scatter(x_j * coefficients[:, :, None], edge_index[1], dim=0, reduce=\"sum\"))\n",
    "        out = out.reshape(-1, self.n_heads*self.out_dim) #[num_nodes, output_dim]\n",
    "        if self.residual:\n",
    "            out = out + self.lin_residual(graph.x)\n",
    "        graph.x = out\n",
    "        return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalAttentionLayer(nn.Module):\n",
    "    def __init__(self, \n",
    "                input_dim, \n",
    "                n_heads, \n",
    "                num_time_steps, \n",
    "                attn_drop, \n",
    "                residual):\n",
    "        super(TemporalAttentionLayer, self).__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.num_time_steps = num_time_steps\n",
    "        self.residual = residual\n",
    "\n",
    "        # define weights\n",
    "        self.position_embeddings = nn.Parameter(torch.Tensor(num_time_steps, input_dim))\n",
    "        self.Q_embedding_weights = nn.Parameter(torch.Tensor(input_dim, input_dim))\n",
    "        self.K_embedding_weights = nn.Parameter(torch.Tensor(input_dim, input_dim))\n",
    "        self.V_embedding_weights = nn.Parameter(torch.Tensor(input_dim, input_dim))\n",
    "        # ff\n",
    "        self.lin = nn.Linear(input_dim, input_dim, bias=True)\n",
    "        # dropout \n",
    "        self.attn_dp = nn.Dropout(attn_drop)\n",
    "        self.xavier_init()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"In:  attn_outputs (of StructuralAttentionLayer at each snapshot):= [N, T, F]\"\"\"\n",
    "        # 1: Add position embeddings to input\n",
    "        position_inputs = torch.arange(0,self.num_time_steps).reshape(1, -1).repeat(inputs.shape[0], 1).long().to(inputs.device)\n",
    "        temporal_inputs = inputs + self.position_embeddings[position_inputs] # [N, T, F]\n",
    "\n",
    "        # 2: Query, Key based multi-head self attention.\n",
    "        q = torch.tensordot(temporal_inputs, self.Q_embedding_weights, dims=([2],[0])) # [N, T, F]\n",
    "        k = torch.tensordot(temporal_inputs, self.K_embedding_weights, dims=([2],[0])) # [N, T, F]\n",
    "        v = torch.tensordot(temporal_inputs, self.V_embedding_weights, dims=([2],[0])) # [N, T, F]\n",
    "\n",
    "        # 3: Split, concat and scale.\n",
    "        split_size = int(q.shape[-1]/self.n_heads)\n",
    "        q_ = torch.cat(torch.split(q, split_size_or_sections=split_size, dim=2), dim=0) # [hN, T, F/h]\n",
    "        k_ = torch.cat(torch.split(k, split_size_or_sections=split_size, dim=2), dim=0) # [hN, T, F/h]\n",
    "        v_ = torch.cat(torch.split(v, split_size_or_sections=split_size, dim=2), dim=0) # [hN, T, F/h]\n",
    "        \n",
    "        outputs = torch.matmul(q_, k_.permute(0,2,1)) # [hN, T, T]\n",
    "        outputs = outputs / (self.num_time_steps ** 0.5)\n",
    "        # 4: Masked (causal) softmax to compute attention weights.\n",
    "        diag_val = torch.ones_like(outputs[0])\n",
    "        tril = torch.tril(diag_val)\n",
    "        masks = tril[None, :, :].repeat(outputs.shape[0], 1, 1) # [h*N, T, T]\n",
    "        padding = torch.ones_like(masks) * (-2**32+1)\n",
    "        outputs = torch.where(masks==0, padding, outputs)\n",
    "        outputs = F.softmax(outputs, dim=2)\n",
    "        self.attn_wts_all = outputs # [h*N, T, T]\n",
    "                \n",
    "        # 5: Dropout on attention weights.\n",
    "        if self.training:\n",
    "            outputs = self.attn_dp(outputs)\n",
    "        outputs = torch.matmul(outputs, v_)  # [hN, T, F/h]\n",
    "        outputs = torch.cat(torch.split(outputs, split_size_or_sections=int(outputs.shape[0]/self.n_heads), dim=0), dim=2) # [N, T, F]\n",
    "        \n",
    "        # 6: Feedforward and residual\n",
    "        outputs = self.feedforward(outputs)\n",
    "        if self.residual:\n",
    "            outputs = outputs + temporal_inputs\n",
    "        return outputs\n",
    "\n",
    "    def feedforward(self, inputs):\n",
    "        outputs = F.relu(self.lin(inputs))\n",
    "        return outputs + inputs\n",
    "\n",
    "\n",
    "    def xavier_init(self):\n",
    "        nn.init.xavier_uniform_(self.position_embeddings)\n",
    "        nn.init.xavier_uniform_(self.Q_embedding_weights)\n",
    "        nn.init.xavier_uniform_(self.K_embedding_weights)\n",
    "        nn.init.xavier_uniform_(self.V_embedding_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DySAT(nn.Module):\n",
    "    def __init__(self, args, num_features, time_length):\n",
    "        \"\"\"[summary]\n",
    "\n",
    "        Args:\n",
    "            args ([type]): [description]\n",
    "            time_length (int): Total timesteps in dataset.\n",
    "        \"\"\"\n",
    "        super(DySAT, self).__init__()\n",
    "        self.args = args\n",
    "        if args.window < 0:\n",
    "            self.num_time_steps = time_length\n",
    "        else:\n",
    "            self.num_time_steps = min(time_length, args.window + 1)  # window = 0 => only self.\n",
    "        self.num_features = num_features\n",
    "\n",
    "        self.structural_head_config = list(map(int, args.structural_head_config.split(\",\")))\n",
    "        self.structural_layer_config = list(map(int, args.structural_layer_config.split(\",\")))\n",
    "        self.temporal_head_config = list(map(int, args.temporal_head_config.split(\",\")))\n",
    "        self.temporal_layer_config = list(map(int, args.temporal_layer_config.split(\",\")))\n",
    "        self.spatial_drop = args.spatial_drop\n",
    "        self.temporal_drop = args.temporal_drop\n",
    "\n",
    "        self.structural_attn, self.temporal_attn = self.build_model()\n",
    "\n",
    "        self.bceloss = BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, graphs):\n",
    "\n",
    "        # Structural Attention forward\n",
    "        structural_out = []\n",
    "        for t in range(0, self.num_time_steps):\n",
    "            structural_out.append(self.structural_attn(graphs[t]))\n",
    "        structural_outputs = [g.x[:,None,:] for g in structural_out] # list of [Ni, 1, F]\n",
    "\n",
    "        # padding outputs along with Ni\n",
    "        maximum_node_num = structural_outputs[-1].shape[0]\n",
    "        out_dim = structural_outputs[-1].shape[-1]\n",
    "        structural_outputs_padded = []\n",
    "        for out in structural_outputs:\n",
    "            zero_padding = torch.zeros(maximum_node_num-out.shape[0], 1, out_dim).to(out.device)\n",
    "            padded = torch.cat((out, zero_padding), dim=0)\n",
    "            structural_outputs_padded.append(padded)\n",
    "        structural_outputs_padded = torch.cat(structural_outputs_padded, dim=1) # [N, T, F]\n",
    "        \n",
    "        # Temporal Attention forward\n",
    "        temporal_out = self.temporal_attn(structural_outputs_padded)\n",
    "        \n",
    "        return temporal_out\n",
    "\n",
    "    def build_model(self):\n",
    "        input_dim = self.num_features\n",
    "\n",
    "        # 1: Structural Attention Layers\n",
    "        structural_attention_layers = nn.Sequential()\n",
    "        for i in range(len(self.structural_layer_config)):\n",
    "            layer = StructuralAttentionLayer(input_dim=input_dim,\n",
    "                                             output_dim=self.structural_layer_config[i],\n",
    "                                             n_heads=self.structural_head_config[i],\n",
    "                                             attn_drop=self.spatial_drop,\n",
    "                                             ffd_drop=self.spatial_drop,\n",
    "                                             residual=self.args.residual)\n",
    "            structural_attention_layers.add_module(name=\"structural_layer_{}\".format(i), module=layer)\n",
    "            input_dim = self.structural_layer_config[i]\n",
    "        \n",
    "        # 2: Temporal Attention Layers\n",
    "        input_dim = self.structural_layer_config[-1]\n",
    "        temporal_attention_layers = nn.Sequential()\n",
    "        for i in range(len(self.temporal_layer_config)):\n",
    "            layer = TemporalAttentionLayer(input_dim=input_dim,\n",
    "                                           n_heads=self.temporal_head_config[i],\n",
    "                                           num_time_steps=self.num_time_steps,\n",
    "                                           attn_drop=self.temporal_drop,\n",
    "                                           residual=self.args.residual)\n",
    "            temporal_attention_layers.add_module(name=\"temporal_layer_{}\".format(i), module=layer)\n",
    "            input_dim = self.temporal_layer_config[i]\n",
    "\n",
    "        return structural_attention_layers, temporal_attention_layers\n",
    "\n",
    "    def get_loss(self, feed_dict):\n",
    "        node_1, node_2, node_2_negative, graphs = feed_dict.values()\n",
    "        # run gnn\n",
    "        final_emb = self.forward(graphs) # [N, T, F]\n",
    "        self.graph_loss = 0\n",
    "        for t in range(self.num_time_steps - 1):\n",
    "            emb_t = final_emb[:, t, :].squeeze() #[N, F]\n",
    "            source_node_emb = emb_t[node_1[t]]\n",
    "            tart_node_pos_emb = emb_t[node_2[t]]\n",
    "            tart_node_neg_emb = emb_t[node_2_negative[t]]\n",
    "            pos_score = torch.sum(source_node_emb * tart_node_pos_emb, dim=1)            \n",
    "            neg_score = -torch.sum(source_node_emb[:, None, :]*tart_node_neg_emb, dim=2).flatten()\n",
    "            pos_loss = self.bceloss(pos_score, torch.ones_like(pos_score))\n",
    "            neg_loss = self.bceloss(neg_score, torch.ones_like(neg_score))\n",
    "            graphloss = pos_loss + self.args.neg_weight*neg_loss\n",
    "            self.graph_loss += graphloss\n",
    "        return self.graph_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inductive_graph(graph_former, graph_later):\n",
    "    \"\"\"Create the adj_train so that it includes nodes from (t+1) \n",
    "       but only edges from t: this is for the purpose of inductive testing.\n",
    "\n",
    "    Args:\n",
    "        graph_former ([type]): [description]\n",
    "        graph_later ([type]): [description]\n",
    "    \"\"\"\n",
    "    newG = nx.MultiGraph()\n",
    "    newG.add_nodes_from(graph_later.nodes(data=True))\n",
    "    newG.add_edges_from(graph_former.edges(data=False))\n",
    "    return newG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parserDict():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    parser.add_argument('--time_steps', type=int, nargs='?', default=16, help=\"total time steps used for train, eval and test\")\n",
    "    # Experimental settings.\n",
    "    parser.add_argument('--dataset', type=str, nargs='?', default='Enron', help='dataset name')\n",
    "    parser.add_argument('--GPU_ID', type=int, nargs='?', default=0, help='GPU_ID (0/1 etc.)')\n",
    "    parser.add_argument('--epochs', type=int, nargs='?', default=200, help='# epochs')\n",
    "    parser.add_argument('--val_freq', type=int, nargs='?', default=1, help='Validation frequency (in epochs)')\n",
    "    parser.add_argument('--test_freq', type=int, nargs='?', default=1, help='Testing frequency (in epochs)')\n",
    "    parser.add_argument('--batch_size', type=int, nargs='?', default=512, help='Batch size (# nodes)')\n",
    "    parser.add_argument('--featureless', type=bool, nargs='?', default=True, help='True if one-hot encoding.')\n",
    "    parser.add_argument(\"--early_stop\", type=int, default=10, help=\"patient\")\n",
    "    \n",
    "    # 1-hot encoding is input as a sparse matrix - hence no scalability issue for large datasets.\n",
    "    # Tunable hyper-params\n",
    "    # TODO: Implementation has not been verified, performance may not be good.\n",
    "    parser.add_argument('--residual', type=bool, nargs='?', default=True, help='Use residual')\n",
    "    \n",
    "    # Number of negative samples per positive pair.\n",
    "    parser.add_argument('--neg_sample_size', type=int, nargs='?', default=10, help='# negative samples per positive')\n",
    "    \n",
    "    # Walk length for random walk sampling.\n",
    "    parser.add_argument('--walk_len', type=int, nargs='?', default=20, help='Walk length for random walk sampling')\n",
    "    \n",
    "    # Weight for negative samples in the binary cross-entropy loss function.\n",
    "    parser.add_argument('--neg_weight', type=float, nargs='?', default=1.0, help='Weightage for negative samples')\n",
    "    parser.add_argument('--learning_rate', type=float, nargs='?', default=0.01, help='Initial learning rate for self-attention model.')\n",
    "    parser.add_argument('--spatial_drop', type=float, nargs='?', default=0.1, help='Spatial (structural) attention Dropout (1 - keep probability).')\n",
    "    parser.add_argument('--temporal_drop', type=float, nargs='?', default=0.5, help='Temporal attention Dropout (1 - keep probability).')\n",
    "    parser.add_argument('--weight_decay', type=float, nargs='?', default=0.0005, help='Initial learning rate for self-attention model.')\n",
    "    \n",
    "    # Architecture params\n",
    "    parser.add_argument('--structural_head_config', type=str, nargs='?', default='16,8,8', help='Encoder layer config: # attention heads in each GAT layer')\n",
    "    parser.add_argument('--structural_layer_config', type=str, nargs='?', default='128', help='Encoder layer config: # units in each GAT layer')\n",
    "    parser.add_argument('--temporal_head_config', type=str, nargs='?', default='16', help='Encoder layer config: # attention heads in each Temporal layer')\n",
    "    parser.add_argument('--temporal_layer_config', type=str, nargs='?', default='128', help='Encoder layer config: # units in each Temporal layer')\n",
    "    parser.add_argument('--position_ffn', type=str, nargs='?', default='True', help='Position wise feedforward')\n",
    "    parser.add_argument('--window', type=int, nargs='?', default=-1, help='Window for temporal attention (default : -1 => full)')\n",
    "    args = parser.parse_args(args=[])\n",
    "    \n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    args = parserDict()\n",
    "\n",
    "    #graphs, feats, adjs = load_graphs(args.dataset)\n",
    "    graphs, adjs = load_graphs(args.dataset)\n",
    "    if args.featureless == True:\n",
    "        feats = [scipy.sparse.identity(adjs[args.time_steps - 1].shape[0]).tocsr()[range(0, x.shape[0]), :] for x in adjs if\n",
    "             x.shape[0] <= adjs[args.time_steps - 1].shape[0]]\n",
    "\n",
    "    assert args.time_steps <= len(adjs), \"Time steps is illegal\"\n",
    "\n",
    "    context_pairs_train = get_context_pairs(graphs, adjs)\n",
    "\n",
    "    # Load evaluation data for link prediction.\n",
    "    train_edges_pos, train_edges_neg, val_edges_pos, val_edges_neg, \\\n",
    "        test_edges_pos, test_edges_neg = get_evaluation_data(graphs)\n",
    "    # print(\"No. Train: Pos={}, Neg={} \\nNo. Val: Pos={}, Neg={} \\nNo. Test: Pos={}, Neg={}\".format(len(train_edges_pos), len(train_edges_neg), len(val_edges_pos), len(val_edges_neg), len(test_edges_pos), len(test_edges_neg)))\n",
    "\n",
    "    # Create the adj_train so that it includes nodes from (t+1) but only edges from t: this is for the purpose of\n",
    "    # inductive testing.\n",
    "    new_G = inductive_graph(graphs[args.time_steps-2], graphs[args.time_steps-1])\n",
    "    graphs[args.time_steps-1] = new_G\n",
    "    adjs[args.time_steps-1] = nx.adjacency_matrix(new_G)\n",
    "\n",
    "    # build dataloader and model\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    dataset = MyDataset(args, graphs, feats, adjs, context_pairs_train)\n",
    "    dataloader = DataLoader(dataset, \n",
    "                            batch_size=args.batch_size, \n",
    "                            shuffle=True, \n",
    "                            num_workers=0, \n",
    "                            collate_fn=MyDataset.collate_fn)\n",
    "    #dataloader = NodeMinibatchIterator(args, graphs, feats, adjs, context_pairs_train, device) \n",
    "    model = DySAT(args, feats[0].shape[1], args.time_steps).to(device)\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "    \n",
    "    print(\"--\" * 20)\n",
    "\n",
    "    # in training\n",
    "    best_epoch_val = 0\n",
    "    patient = 0\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = []\n",
    "        for idx, feed_dict in enumerate(dataloader):\n",
    "            feed_dict = to_device(feed_dict, device)\n",
    "            opt.zero_grad()\n",
    "            loss = model.get_loss(feed_dict)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            epoch_loss.append(loss.item())\n",
    "            if epoch % 5 == 0:\n",
    "                print(\"Epoch: {} | Loss: {}\".format(epoch, loss.item()))\n",
    "                \n",
    "#             model.eval()\n",
    "#             emb = model(feed_dict[\"graphs\"])[:, -2, :].detach().cpu().numpy()\n",
    "#             val_results, test_results, _, _ = evaluate_classifier(\n",
    "#                 train_edges_pos, train_edges_neg,\n",
    "#                 val_edges_pos, val_edges_neg, \n",
    "#                 test_edges_pos, test_edges_neg, \n",
    "#                 emb, emb\n",
    "#             )\n",
    "#             epoch_auc_val = val_results[\"HAD\"][1]\n",
    "#             epoch_auc_test = test_results[\"HAD\"][1]\n",
    "            \n",
    "#             if epoch_auc_val > best_epoch_val:\n",
    "#                 best_epoch_val = epoch_auc_val\n",
    "#                 torch.save(model.state_dict(), r\"C:\\Users\\sss\\Desktop\\DySAT_pytorch-main\\model_checkpoints/model.pt\")\n",
    "#                 patient = 0\n",
    "#             else:\n",
    "#                 patient += 1\n",
    "#                 if patient > args.early_stop:\n",
    "#                     break\n",
    "                    \n",
    "#             print(\"Epoch {:<3},  Loss = {:.3f}, Val AUC {:.3f} Test AUC {:.3f}\".format(epoch, np.mean(epoch_loss), epoch_auc_val, epoch_auc_test))\n",
    "            \n",
    "            \n",
    "#     Test Best Model\n",
    "#     model.load_state_dict(torch.load(r\"C:\\Users\\sss\\Desktop\\DySAT_pytorch-main\\model_checkpoints/model.pt\"))\n",
    "#     model.eval()\n",
    "#     emb = model(feed_dict[\"graphs\"])[:, -2, :].detach().cpu().numpy()\n",
    "#     val_results, test_results, _, _ = evaluate_classifier(\n",
    "#        train_edges_pos, train_edges_neg,\n",
    "#       val_edges_pos, val_edges_neg, \n",
    "#      test_edges_pos, test_edges_neg, \n",
    "#     emb, emb\n",
    "#     )\n",
    "#     auc_val = val_results[\"HAD\"][1]\n",
    "#     auc_test = test_results[\"HAD\"][1]\n",
    "#     print(\"Best Test AUC = {:.3f}\".format(auc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 19 graphs \n",
      "Computing training pairs ...\n",
      "Generating eval data ....\n",
      "----------------------------------------\n",
      "Epoch: 5 | Loss: 23.030160903930664\n",
      "Epoch: 10 | Loss: 19.204713821411133\n",
      "Epoch: 15 | Loss: 17.431804656982422\n",
      "Epoch: 20 | Loss: 16.741374969482422\n",
      "Epoch: 25 | Loss: 16.35283088684082\n",
      "Epoch: 30 | Loss: 16.03824234008789\n",
      "Epoch: 35 | Loss: 15.814580917358398\n",
      "Epoch: 40 | Loss: 15.65756607055664\n",
      "Epoch: 45 | Loss: 15.530989646911621\n",
      "Epoch: 50 | Loss: 15.41029167175293\n",
      "Epoch: 55 | Loss: 15.313873291015625\n",
      "Epoch: 60 | Loss: 15.253538131713867\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-37-b89792cd0329>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m             \u001b[0mopt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-4e37a0da6f2d>\u001b[0m in \u001b[0;36mget_loss\u001b[1;34m(self, feed_dict)\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[0mtart_node_pos_emb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0memb_t\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnode_2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m             \u001b[0mtart_node_neg_emb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0memb_t\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnode_2_negative\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m             \u001b[0mpos_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_node_emb\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtart_node_pos_emb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m             \u001b[0mneg_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msource_node_emb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtart_node_neg_emb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[0mpos_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbceloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
