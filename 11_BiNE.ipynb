{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用在二部图中\n",
    "- 【二部图：共两类节点，分为显式和隐式，显式是两点直接相连，隐式是两点通过中间节点相连】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "from io import open\n",
    "from os import path\n",
    "from glob import glob\n",
    "from random import shuffle\n",
    "from sklearn import preprocessing, metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import average_precision_score,auc,precision_recall_fscore_support\n",
    "\n",
    "from six.moves import range, zip, zip_longest\n",
    "from six import iterkeys\n",
    "from collections import defaultdict, Iterable\n",
    "from networkx.algorithms import bipartite as bi\n",
    "\n",
    "\n",
    "from multiprocessing import cpu_count\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from itertools import product, permutations\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "from scipy.io import loadmat\n",
    "from scipy.sparse import issparse\n",
    "\n",
    "from argparse import ArgumentParser, FileType, ArgumentDefaultsHelpFormatter\n",
    "from datasketch import MinHashLSHForest, MinHash, MinHashLSH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataUtils(object):\n",
    "    def __init__(self, model_path):\n",
    "        self.model_path = model_path\n",
    "        \n",
    "    def rename(self, datafile):\n",
    "        # 区分两种节点并重命名\n",
    "        with open(os.path.join(self.model_path, \"_ratings.dat\"), \"w\") as fw:\n",
    "            with open(datafile, \"r\", encoding=\"UTF-8\") as fin:\n",
    "                line = fin.readline()\n",
    "                while line:\n",
    "                    user, item, rating = line.strip().split(\"\\t\")\n",
    "                    fw.write(\"u\" + user + \"\\t\" + \"i\" + item + \"\\t\" + rating + \"\\n\")\n",
    "                    line = fin.readline()\n",
    "                    \n",
    "    def split_data(self, percent):\n",
    "        test_user, test_item, test_rate, rating = set(), set(), {}, {}\n",
    "        with open(os.path.join(self.model_path, \"ratings.dat\"), \"r\") as fin, open(os.path.join(self.model_path, \"ratings_train.dat\"), \"w\") as ftrain, open(os.path.join(self.model_path, \"ratings_test.dat\"), \"w\") as ftest:\n",
    "            for line in fin.readlines():\n",
    "                user, item, rate = line.strip().split(\"\\t\")\n",
    "                if rating.get(user) is None:\n",
    "                    rating[user] = {}\n",
    "                rating[user][item] = rate\n",
    "            for u in rating.keys():\n",
    "                item_list = rating[u].keys()\n",
    "                sample_list = random.sample(item_list, int(len(item_list) * percent))\n",
    "                for item in item_list:\n",
    "                    if item in sample_list:\n",
    "                        ftrain.write(u + \"\\t\" + item + \"\\t\" + rating[u][item] + \"\\n\")\n",
    "                    else:\n",
    "                        if test_rate.get(u) is None:\n",
    "                            test_rate[u] = {}\n",
    "                        test_rate[u][item] = float(rating[u][item])\n",
    "                        test_user.add(u)\n",
    "                        test_item.add(item)\n",
    "                        ftest.write(u + \"\\t\" + item + \"\\t\" + rating[u][item] + \"\\n\")\n",
    "                        \n",
    "        return test_user, test_item, test_rate\n",
    "    \n",
    "    def read_data(self, filename=None):\n",
    "        if filename is None:\n",
    "            filename = os.path.join(self.model_path, \"ratings_test.dat\")\n",
    "        users, items, rates = set(), set(), {}\n",
    "        with open(filename, \"r\", encoding=\"UTF-8\") as fin:\n",
    "            line = fin.readline()\n",
    "            while line:\n",
    "                user, item, rate = line.strip().split()\n",
    "                if rates.get(user) is None:\n",
    "                    rates[user] = {}\n",
    "                rates[user][item] = float(rate)\n",
    "                users.add(user)\n",
    "                items.add(item)\n",
    "                line = fin.readline()\n",
    "                \n",
    "        return users, items, rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 负采样操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_lsh(obj_dict):\n",
    "    lsh_0 = MinHashLSH(threshold=0, num_perm=128,params=None)\n",
    "    lsh_5 = MinHashLSH(threshold=0.6, num_perm=128,params=None)\n",
    "    # forest = MinHashLSHForest(num_perm=128)\n",
    "    keys = list(obj_dict.keys())\n",
    "    values = list(obj_dict.values())\n",
    "    ms = []\n",
    "    for i in range(len(keys)):\n",
    "        temp = MinHash(num_perm=128)\n",
    "        for d in values[i]:\n",
    "            temp.update(d.encode('utf8'))\n",
    "        ms.append(temp)\n",
    "        lsh_0.insert(keys[i], temp)\n",
    "        lsh_5.insert(keys[i], temp)\n",
    "    return lsh_0,lsh_5, keys, ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_get_negs_by_lsh(sample_num, obj_dict):\n",
    "    lsh_0,lsh_5, keys, ms = construct_lsh(obj_dict)\n",
    "    visited = []\n",
    "    negs_dict = {}\n",
    "    for i in range(len(keys)):\n",
    "        record = []\n",
    "        if i in visited:\n",
    "            continue\n",
    "        visited.append(i)\n",
    "        record.append(i)\n",
    "        total_list = set(keys)\n",
    "        sim_list = set(lsh_0.query(ms[i]))\n",
    "        high_sim_list = set(lsh_5.query(ms[i]))\n",
    "        total_list = list(total_list - sim_list)\n",
    "        for j in high_sim_list:\n",
    "            total_list = set(total_list)\n",
    "            ind = keys.index(j)\n",
    "            if ind not in visited:\n",
    "                visited.append(ind)\n",
    "                record.append(ind)\n",
    "            sim_list_child = set(lsh_0.query(ms[ind]))\n",
    "            total_list = list(total_list - sim_list_child)\n",
    "        total_list = random.sample(list(total_list), min(sample_num, len(total_list)))\n",
    "        for j in record:\n",
    "            key = keys[j]\n",
    "            negs_dict[key] = total_list\n",
    "    return negs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_negs_by_lsh(user_dict, item_dict, num_negs):\n",
    "    sample_num_u = max(300, int(len(user_dict) * 0.01 * num_negs))\n",
    "    sample_num_v = max(300, int(len(user_dict) * 0.01 * num_negs))\n",
    "    negs_u = call_get_negs_by_lsh(sample_num_u, user_dict)  # u类别下，负采样工作;每个节点进行600个负采样节点\n",
    "    negs_v = call_get_negs_by_lsh(sample_num_v, item_dict)  # v类别下，负采样工作;每个节点进行300个负采样节点\n",
    "    return negs_u, negs_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph(defaultdict):\n",
    "    act = {}\n",
    "    isWeight = False\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Graph, self).__init__(list)\n",
    "\n",
    "    def setIsWeight(self,isWeight):\n",
    "        self.isWeight = isWeight\n",
    "\n",
    "    def initAct(self):\n",
    "        for i in self.keys():\n",
    "            self.act[i] = 0\n",
    "\n",
    "    def nodes(self):\n",
    "        return self.keys()\n",
    "\n",
    "    def adjacency_iter(self):\n",
    "        return self.iteritems()\n",
    "\n",
    "    def subgraph(self, nodes={}):\n",
    "        subgraph = Graph()\n",
    "    \n",
    "        for n in nodes:\n",
    "            if n in self:\n",
    "                subgraph[n] = [x for x in self[n] if x in nodes]\n",
    "        \n",
    "        return subgraph\n",
    "\n",
    "    def make_undirected(self):\n",
    "\n",
    "        for v in self.keys():\n",
    "            for other in self[v]:\n",
    "                if v != other:\n",
    "                    self[other].append(v)\n",
    "                    \n",
    "        self.make_consistent()\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def make_consistent(self):\n",
    "\n",
    "        if self.isWeight == True:\n",
    "            for k in iterkeys(self):\n",
    "                self[k] = self.sortedDictValues(self[k])\n",
    "                self.remove_self_loops_dict()\n",
    "        \n",
    "        else:\n",
    "            for k in iterkeys(self):\n",
    "                self[k] = list(sorted(set(self[k])))\n",
    "\n",
    "        self.remove_self_loops()\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def sortedDictValues(self,adict):\n",
    "        keys = adict.keys()\n",
    "        keys.sort()\n",
    "\n",
    "        return map(adict.get, keys)\n",
    "\n",
    "    def make_consistent_dict(self):\n",
    "        for k in iterkeys(self):\n",
    "            self[k] = self.sortedDictValues(self[k])\n",
    "        self.remove_self_loops_dict()\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def remove_self_loops(self):\n",
    "        removed = 0\n",
    "        \n",
    "        if self.isWeight == True:\n",
    "            for x in self:\n",
    "                if x in self[x].keys():\n",
    "                    del self[x][x]\n",
    "                    removed += 1\n",
    "                    \n",
    "        else:\n",
    "            for x in self:\n",
    "                if x in self[x]:\n",
    "                    self[x].remove(x)\n",
    "                    removed += 1\n",
    "    \n",
    "        return self\n",
    "\n",
    "    def check_self_loops(self):\n",
    "        for x in self:\n",
    "            for y in self[x]:\n",
    "                if x == y:\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    def has_edge(self, v1, v2):\n",
    "        if v2 in self[v1] or v1 in self[v2]:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def degree(self, nodes=None):\n",
    "        if isinstance(nodes, Iterable):\n",
    "            return {v:len(self[v]) for v in nodes}\n",
    "        else:\n",
    "            return len(self[nodes])\n",
    "\n",
    "    def order(self):\n",
    "        \"Returns the number of nodes in the graph\"\n",
    "        return len(self)    \n",
    "\n",
    "    def number_of_edges(self):\n",
    "        \"Returns the number of nodes in the graph\"\n",
    "        return sum([self.degree(x) for x in self.keys()])/2\n",
    "\n",
    "    def number_of_nodes(self):\n",
    "        \"Returns the number of nodes in the graph\"\n",
    "        return self.order()\n",
    "\n",
    "    def random_walk(self, nodes, path_length, alpha=0, rand=random.Random(), start=None):\n",
    "        \"\"\" \n",
    "        Returns a truncated random walk.\n",
    "\n",
    "        path_length: Length of the random walk.\n",
    "        alpha: probability of restarts.\n",
    "        start: the start node of the random walk.\n",
    "        \"\"\"\n",
    "        G = self\n",
    "        if start:\n",
    "            path = [start]\n",
    "        else:  # Sampling is uniform w.r.t V, and not w.r.t E\n",
    "            path = [rand.choice(nodes)]\n",
    "            \n",
    "        while len(path) < path_length:\n",
    "            cur = path[-1]\n",
    "            if len(G[cur]) > 0:\n",
    "                if rand.random() >= alpha:\n",
    "                    add_node = rand.choice(G[rand.choice(G[cur])])\n",
    "                    while add_node == cur:\n",
    "                        add_node = rand.choice(G[rand.choice(G[cur])])\n",
    "                    path.append(add_node)\n",
    "                else:\n",
    "                    path.append(path[0])\n",
    "            else:\n",
    "                break\n",
    "        return path\n",
    "\n",
    "#     def spreading_activation(self, path_length, alpha=0, rand=random.Random(), start=None):\n",
    "#         \"\"\" \n",
    "#         Returns a truncated random walk.\n",
    "#         path_length: Length of the random walk.\n",
    "#         alpha: probability of restarts.\n",
    "#         start: the start node of the random walk.\n",
    "#         \"\"\"\n",
    "#         G = self\n",
    "#         if start:\n",
    "#             path = [str(start)]\n",
    "#         else:  # Sampling is uniform w.r.t V, and not w.r.t E\n",
    "#             path = [rand.choice(G.keys())]\n",
    "\n",
    "#         while len(path) < path_length:\n",
    "#             cur = path[-1]\n",
    "#             if len(G[cur]) > 0:\n",
    "#                 if rand.random() >= alpha:\n",
    "#                     temp = rand.choice(G[cur])\n",
    "#                     while\n",
    "#                     path.append(rand.choice(G[cur]))\n",
    "#                 else:\n",
    "#                     path.append(path[0])\n",
    "#             else:\n",
    "#                 break\n",
    "#         return path\n",
    "\n",
    "    def random_walk_restart(self, nodes, percentage, alpha=0, rand=random.Random(), start=None):\n",
    "        \"\"\"\n",
    "        Returns a truncated random walk.\n",
    "        percentage: probability of stopping walking\n",
    "        alpha: probability of restarts.\n",
    "        start: the start node of the random walk.\n",
    "        \"\"\"\n",
    "        G = self\n",
    "        if start:\n",
    "            path = [start]\n",
    "        else:  # Sampling is uniform w.r.t V, and not w.r.t E\n",
    "            path = [rand.choice(nodes)]\n",
    "        \n",
    "        while len(path) < 1 or random.random() > percentage:\n",
    "            cur = path[-1]\n",
    "            if len(G[cur]) > 0:  # 节点的邻居节点\n",
    "                if rand.random() >= alpha:\n",
    "                    add_node = rand.choice(G[cur])\n",
    "                    while add_node == cur:  # 如果是同一个节点，重新选择；前面我们已经去重自连接边了；\n",
    "                        add_node = rand.choice(G[cur])\n",
    "                    path.append(add_node)\n",
    "                else:\n",
    "                    path.append(path[0])\n",
    "            else:\n",
    "                break\n",
    "        return path\n",
    "  \n",
    "      # neighbors = []\n",
    "      # for n in G[cur]:\n",
    "      #   neighbors.extend(G[n])\n",
    "      # if len(G[cur]) > 0:\n",
    "      #   if rand.random() >= alpha:\n",
    "      #     add_node = rand.choice(neighbors)\n",
    "      #     path.append(add_node)\n",
    "      #   else:\n",
    "      #     path.append(path[0])\n",
    "      # else:\n",
    "      #   break\n",
    "    # return path\n",
    "\n",
    "    def random_walk_restart_for_large_bipartite_graph(self, nodes, percentage, alpha=0, rand=random.Random(), start=None):\n",
    "        \"\"\"\n",
    "        Returns a truncated random walk.\n",
    "        percentage: probability of stopping walking\n",
    "        alpha: probability of restarts.\n",
    "        start: the start node of the random walk.\n",
    "        \"\"\"\n",
    "        G = self\n",
    "        if start:\n",
    "            path = [start]\n",
    "        else:  # Sampling is uniform w.r.t V, and not w.r.t E\n",
    "            path = [rand.choice(nodes)]\n",
    "            \n",
    "        while len(path) < 1 or random.random() > percentage:\n",
    "            cur = path[-1]\n",
    "            neighbors = set([])\n",
    "            for nei in G[cur]:\n",
    "                neighbors = neighbors.union(set(G[nei]))\n",
    "            # print(len(neighbors))\n",
    "            neighbors = list(neighbors)\n",
    "            \n",
    "            if len(G[cur]) > 0:\n",
    "                if rand.random() >= alpha:\n",
    "                    add_node = rand.choice(neighbors)\n",
    "                    while add_node == cur and len(neighbors) > 1:\n",
    "                        add_node = rand.choice(neighbors)\n",
    "                    path.append(add_node)\n",
    "                else:\n",
    "                    path.append(path[0])\n",
    "            else:\n",
    "                break\n",
    "        return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateAct(self, node):\n",
    "    G = self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TODO add build_walks in here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_deepwalk_corpus(G, num_paths, path_length, alpha=0, rand=random.Random(), node_type=\"u\"):\n",
    "    walks = []\n",
    "    nodes_total = list(G.nodes())\n",
    "    nodes = []\n",
    "    \n",
    "    for obj in nodes_total:\n",
    "        if obj[0] == node_type:\n",
    "            nodes.append(obj)\n",
    "            \n",
    "    # nodes = list(G.nodes())\n",
    "    \n",
    "    for cnt in range(num_paths):\n",
    "        rand.shuffle(nodes)\n",
    "        for node in nodes:\n",
    "            walks.append(G.random_walk(nodes, path_length, alpha=alpha, rand=rand, start=node))\n",
    "            \n",
    "    random.shuffle(walks)\n",
    "    \n",
    "    return walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_deepwalk_corpus_random(G, hits_dict, percentage, maxT, minT, alpha=0, rand=random.Random()):\n",
    "    walks = []\n",
    "    nodes = list(G.nodes())\n",
    "    for node in nodes:\n",
    "        num_paths = max(int(math.ceil(maxT * hits_dict[node])), minT)  # hits确定随机游走的次数\n",
    "        # print(num_paths)\n",
    "        for cnt in range(num_paths):\n",
    "            walks.append(G.random_walk_restart(nodes, percentage,rand=rand, alpha=alpha, start=node))  # 按照stop_proba进行采样\n",
    "    \n",
    "    random.shuffle(walks)\n",
    "    return walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_deepwalk_corpus_random_for_large_bibartite_graph(G, hits_dict, percentage, maxT, minT, alpha=0, rand = random.Random(), node_type='u'):\n",
    "    walks = []\n",
    "    nodes_total = list(G.nodes())\n",
    "    nodes = []\n",
    "    # print(len(nodes), node_type)\n",
    "    for obj in nodes_total:\n",
    "        if obj[0] == node_type:\n",
    "            nodes.append(obj)\n",
    "    # cnt_0 = 1\n",
    "    # print(len(nodes))\n",
    "    for node in nodes:\n",
    "        \"\"\"\n",
    "        if cnt_0 % 1000 == 0:\n",
    "            print(cnt_0)\n",
    "            cnt_0 += 1\n",
    "        \"\"\"\n",
    "        num_paths = max(int(math.ceil(maxT * hits_dict[node])), minT)\n",
    "        # print(num_paths)\n",
    "        for cnt in range(num_paths):\n",
    "            walks.append(G.random_walk_restart_for_large_bipartite_graph(nodes, percentage,rand=rand, alpha=alpha, start=node))\n",
    "    random.shuffle(walks)\n",
    "    \n",
    "    return walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_deepwalk_corpus_iter(G, num_paths, path_length, alpha=0, rand=random.Random(0)):\n",
    "    walks = []\n",
    "    nodes = list(G.nodes())\n",
    "    \n",
    "    for cnt in range(num_paths):\n",
    "        rand.shuffle(nodes)\n",
    "        for node in nodes:\n",
    "            yield G.random_walk(path_length, rand=rand, alpha=alpha, start=node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clique(size):\n",
    "    return from_adjlist(permutations(range(1, size + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://stackoverflow.com/questions/312443/how-do-you-split-a-list-into-evenly-sized-chunks-in-python\n",
    "def grouper(n, iterable, padvalue=None):\n",
    "    \"grouper(3, 'abcdefg', 'x') --> ('a','b','c'), ('d','e','f'), ('g','x','x')\"\n",
    "    return zip_longest(*[iter(iterable)] * n, fillvalue=padvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_adjacencylist(f):\n",
    "    adjlist = []\n",
    "    for l in f:\n",
    "        if l and l[0] != \"#\":\n",
    "            introw = [int(x) for x in l.strip().split()]\n",
    "            row = [introw(0)]\n",
    "            row.extend(set(sorted(introw[1:])))\n",
    "            adjlist.extend([row])\n",
    "            \n",
    "    return adjlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_adjacencylist_unchecked(f):\n",
    "    adjlist = []\n",
    "    for l in f:\n",
    "        if l and l[0] != \"#\":\n",
    "            adjlist.extend([[int(x) for x in l.strip().split()]])\n",
    "            \n",
    "    return adjlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_adjlist_unchecked(adjlist):\n",
    "    G = Graph()\n",
    "    \n",
    "    for row in adjlist:\n",
    "        node = row[0]\n",
    "        neighbors = row[1:]\n",
    "        G[node] = neighbors\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_adjlist(adjlist):\n",
    "    G = Graph()\n",
    "    \n",
    "    for row in adjlist:\n",
    "        node = row[0]\n",
    "        neighbors = row[1:]\n",
    "        G[node] = list(sorted(set(neighbors)))\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_adjacencylist(file_, undirected=False, chunksize=10000, unchecked=True):\n",
    "    if unchecked:\n",
    "        parse_func = parse_adjacencylist_unchecked\n",
    "        convert_func = from_adjlist_unchecked\n",
    "    else:\n",
    "        parse_func = parse_adjacencylist\n",
    "        convert_func = from_adjlist\n",
    "    \n",
    "    adjlist = []\n",
    "    \n",
    "    with open(file_) as f:\n",
    "        with ProcessPoolExecutor(max_workers=cpu_count()) as executor:\n",
    "            total = 0\n",
    "            for idx, adj_chunk in enumerate(executor.map(parse_func, grouper(int(chunksize), f))):\n",
    "                adjlist.extend(adj_chunk)\n",
    "                total += len(adj_chunk)\n",
    "                    \n",
    "    G = convert_func(adjlist)\n",
    "    \n",
    "    if undirected:\n",
    "        G = G.make_undirected()\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_edgelist(file_, undirected=True):\n",
    "    G = Graph()\n",
    "    with open(file_,encoding=\"UTF-8\") as f:\n",
    "        for l in f:\n",
    "            x, y = l.strip().split()[:2]\n",
    "            G[x].append(y)\n",
    "            if undirected:\n",
    "                G[y].append(x)\n",
    "    G.make_consistent()\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_edgelist_from_matrix(matrix, undirected=True):\n",
    "    G = Graph()\n",
    "    for x in matrix.keys():\n",
    "        for y in matrix[x]:\n",
    "            G[x].append(y)\n",
    "            if undirected:\n",
    "                G[y].append(x)\n",
    "    G.make_consistent()\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_edgelist_w(file_, undirected=True):\n",
    "    G = Graph()\n",
    "    G.setIsWeight(True)\n",
    "    G.initAct()\n",
    "    with open(file_) as f:\n",
    "        for l in f:\n",
    "            x, y, w = l.strip().split()[:3]\n",
    "            x = int(x)\n",
    "            y = int(y)\n",
    "            w = float(w)\n",
    "            \n",
    "            if len(G[x]) == 0:\n",
    "                G[x] = {}\n",
    "            if len(G[y]) == 0:\n",
    "                G[y] = {}\n",
    "            G[x][y] = w\n",
    "            \n",
    "            if undirected:\n",
    "                G[y][x] = w\n",
    "                \n",
    "    G.make_consistent()\n",
    "    \n",
    "    return G    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_matfile(file_, variable_name=\"network\", undirected=True):\n",
    "    mat_varables = loadmat(file_)\n",
    "    mat_matrix = mat_varables[variable_name]\n",
    "\n",
    "    return from_numpy(mat_matrix, undirected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_networkx(G_input, undirected=True):\n",
    "    G = Graph()\n",
    "\n",
    "    for idx, x in enumerate(G_input.nodes_iter()):\n",
    "        for y in iterkeys(G_input[x]):\n",
    "            G[x].append(y)\n",
    "\n",
    "    if undirected:\n",
    "        G.make_undirected()\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_numpy(x, undirected=True):\n",
    "    G = Graph()\n",
    "\n",
    "    if issparse(x):\n",
    "        cx = x.tocoo()\n",
    "        for i,j,v in zip(cx.row, cx.col, cx.data):\n",
    "            G[i].append(j)\n",
    "    else:\n",
    "        raise Exception(\"Dense matrices not yet supported.\")\n",
    "\n",
    "    if undirected:\n",
    "        G.make_undirected()\n",
    "\n",
    "    G.make_consistent()\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphUtils(object):\n",
    "    def __init__(self, model_path):\n",
    "        self.model_path = model_path\n",
    "        self.G = nx.Graph()\n",
    "        self.edge_dict_u = {}\n",
    "        self.edge_dict_v = {}\n",
    "        self.edge_list = []\n",
    "        self.node_u = []\n",
    "        self.node_v = []\n",
    "        self.authority_u, self.authority_v = {}, {}\n",
    "        self.walks_u, self.walks_v = [], []\n",
    "        self.G_u, self.G_v = None, None\n",
    "        self.fw_u = os.path.join(self.model_path, \"homogeneous_u.dat\")\n",
    "        self.fw_v = os.path.join(self.model_path, \"homogeneous_v.dat\")\n",
    "        self.negs_u = {}\n",
    "        self.negs_v = {}\n",
    "        self.context_u = {}\n",
    "        self.context_v = {}\n",
    "        \n",
    "    def construct_training_graph(self, filename=None):  # 构造训练图\n",
    "        if filename is None:\n",
    "            filename = os.path.join(self.model_path, \"ratings_train.dat\")\n",
    "        edge_list_u_v = []\n",
    "        edge_list_v_u = []\n",
    "        with open(filename, encoding=\"UTF-8\") as fin:\n",
    "            line = fin.readline()\n",
    "            while line:\n",
    "                user, item, rating = line.strip().split(\"\\t\")\n",
    "                if self.edge_dict_u.get(user) is None:  # 构造u连接到的节点\n",
    "                    self.edge_dict_u[user] = {}\n",
    "                if self.edge_dict_v.get(item) is None:  # 构造v连接到的节点\n",
    "                    self.edge_dict_v[item] = {}\n",
    "                edge_list_u_v.append((user, item, float(rating)))  # list边\n",
    "                self.edge_dict_u[user][item] = float(rating)  # 记录点边\n",
    "                self.edge_dict_v[item][user] = float(rating)\n",
    "                edge_list_v_u.append((item, user, float(rating)))\n",
    "                line = fin.readline()\n",
    "        # create bipartite graph\n",
    "        self.node_u = self.edge_dict_u.keys()  # u类别的所有节点\n",
    "        self.node_v = self.edge_dict_v.keys()  # v类别的所有节点\n",
    "        # self.node_u.sort()\n",
    "        # self.node_v.sort()\n",
    "        self.G.add_nodes_from(self.node_u, bipartite=0)  # 构建二部图\n",
    "        self.G.add_nodes_from(self.node_v, bipartite=1)  \n",
    "        self.G.add_weighted_edges_from(edge_list_u_v + edge_list_v_u)  # 将异构图边信息导入\n",
    "        self.edge_list = edge_list_u_v\n",
    "        \n",
    "    def calculate_centrality(self, mode=\"hits\"):\n",
    "        if mode == \"degree_centrality\":\n",
    "            a = nx.degree_centrality(self.G)\n",
    "        else:\n",
    "            h, a = nx.hits(self.G)  # hub, authority\n",
    "            \n",
    "        max_a_u, min_a_u, max_a_v, min_a_v = 0, 100000, 0, 100000\n",
    "        \n",
    "        for node in self.G.nodes():  # u,v类别的authority的值\n",
    "            if node[0] == \"u\":\n",
    "                if max_a_u < a[node]:\n",
    "                    max_a_u = a[node]\n",
    "                if min_a_u > a[node]:\n",
    "                    min_a_u = a[node]\n",
    "            if node[0] == \"i\":\n",
    "                if max_a_v < a[node]:\n",
    "                    max_a_v = a[node]\n",
    "                if min_a_v > a[node]:\n",
    "                    min_a_v = a[node]\n",
    "        \n",
    "        for node in self.G.nodes():  # 计算每个节点归一化后的authority值\n",
    "            if node[0] == \"u\":\n",
    "                if max_a_u-min_a_u != 0:\n",
    "                    self.authority_u[node] = (float(a[node])-min_a_u) / (max_a_u-min_a_u)\n",
    "                else:\n",
    "                    self.authority_u[node] = 0\n",
    "            if node[0] == 'i':\n",
    "                if max_a_v-min_a_v != 0:\n",
    "                    self.authority_v[node] = (float(a[node])-min_a_v) / (max_a_v-min_a_v)\n",
    "                else:\n",
    "                    self.authority_v[node] = 0\n",
    "                    \n",
    "    def homogeneous_graph_random_walks(self, percentage, maxT, minT):\n",
    "        # print(len(self.node_u),len(self.node_v))\n",
    "        A = bi.biadjacency_matrix(self.G, self.node_u, self.node_v, dtype=np.float,weight='weight', format='csr')  # 构造二部图邻接矩阵\n",
    "        row_index = dict(zip(self.node_u, itertools.count()))  # 节点:编号 dictionary\n",
    "        col_index = dict(zip(self.node_v, itertools.count()))\n",
    "        index_row = dict(zip(row_index.values(), row_index.keys()))\n",
    "        index_item = dict(zip(col_index.values(), col_index.keys()))\n",
    "        AT = A.transpose()\n",
    "        self.save_homogenous_graph_to_file(A.dot(AT),self.fw_u, index_row,index_row)  # 将矩阵保留下来\n",
    "        self.save_homogenous_graph_to_file(AT.dot(A),self.fw_v, index_item,index_item)\n",
    "        self.G_u, self.walks_u = self.get_random_walks_restart(self.fw_u, self.authority_u, percentage=percentage, maxT=maxT, minT=minT)  # 每个类别节点下的随机游走\n",
    "        self.G_v, self.walks_v = self.get_random_walks_restart(self.fw_v, self.authority_v, percentage=percentage, maxT=maxT, minT=minT)\n",
    "        \n",
    "    def get_random_walks_restart(self, datafile, hits_dict, percentage, maxT, minT):\n",
    "        if datafile is None:\n",
    "            datafile = os.path.join(self.model_path, \"rating_train.dat\")\n",
    "        G = load_edgelist(datafile, undirected=True)  # 导入点边信息\n",
    "        print(\"number of nodes: {}\".format(len(G.nodes())))\n",
    "        print(\"walking...\")\n",
    "        walks = build_deepwalk_corpus_random(G, hits_dict, percentage=percentage, maxT = maxT, minT = minT, alpha=0)\n",
    "        print(\"walking...ok\")\n",
    "        return G, walks\n",
    "    \n",
    "    def homogeneous_graph_random_walks_for_large_bipartite_graph(self, percentage, maxT, minT):\n",
    "        A = bi.biadjacency_matrix(self.G, self.node_u, self.node_v, dtype=np.float,weight='weight', format='csr')\n",
    "        row_index = dict(zip(self.node_u, itertools.count()))\n",
    "        col_index = dict(zip(self.node_v, itertools.count()))\n",
    "        index_row = dict(zip(row_index.values(), row_index.keys()))\n",
    "        index_item = dict(zip(col_index.values(), col_index.keys()))\n",
    "        AT = A.transpose()\n",
    "        matrix_u = self.get_homogenous_graph(A.dot(AT), self.fw_u, index_row, index_row)\n",
    "        matrix_v = self.get_homogenous_graph(AT.dot(A), self.fw_v, index_item, index_item)\n",
    "        self.G_u, self.walks_u = self.get_random_walks_restart_for_large_bipartite_graph(matrix_u, self.authority_u, percentage=percentage, maxT=maxT, minT=minT)\n",
    "        self.G_v, self.walks_v = self.get_random_walks_restart_for_large_bipartite_graph(matrix_v, self.authority_v, percentage=percentage, maxT=maxT, minT=minT)\n",
    "        \n",
    "    def homogeneous_graph_random_walks_for_large_bipartite_graph_without_generating(self, datafile, percentage, maxT, minT):\n",
    "        self.G_u, self.walks_u = self.get_random_walks_restart_for_large_bipartite_graph_without_generating(datafile, self.authority_u, percentage=percentage, maxT=maxT, minT=minT, node_type='u')\n",
    "        self.G_v, self.walks_v = self.get_random_walks_restart_for_large_bipartite_graph_without_generating(datafile, self.authority_v, percentage=percentage, maxT=maxT, minT=minT,node_type='i')\n",
    "        \n",
    "    def get_random_walks_restart_for_large_bipartite_graph(self, matrix, hits_dict, percentage, maxT, minT):\n",
    "        G = load_edgelist_from_matrix(matrix, undirected=True)\n",
    "        print(\"number of nodes: {}\".format(len(G.nodes())))\n",
    "        print(\"walking...\")\n",
    "        walks = build_deepwalk_corpus_random(G, hits_dict, percentage=percentage, maxT = maxT, minT = minT, alpha=0)\n",
    "        print(\"walking...ok\")\n",
    "        return G, walks       \n",
    "        \n",
    "    def get_random_walks_restart_for_large_bipartite_graph_without_generating(self, datafile, hits_dict, percentage, maxT, minT, node_type='u'):\n",
    "        if datafile is None:\n",
    "            datafile = os.path.join(self.model_path,\"rating_train.dat\")\n",
    "        G = load_edgelist(datafile, undirected=True)\n",
    "        cnt = 0\n",
    "        for n in G.nodes():\n",
    "            if n[0] == node_type:\n",
    "                cnt += 1\n",
    "        print(\"number of nodes: {}\".format(cnt))\n",
    "        print(\"walking...\")\n",
    "        walks = build_deepwalk_corpus_random_for_large_bibartite_graph(G, hits_dict, percentage=percentage, maxT = maxT, minT = minT, alpha=0,node_type=node_type)\n",
    "        # print(walks)\n",
    "        print(\"walking...ok\")\n",
    "        return G, walks\n",
    "    \n",
    "    def save_words_and_sentences_to_file(self, filenodes, filesentences):\n",
    "        with open(filenodes, \"w\") as fw:\n",
    "            for node in self.G.keys():\n",
    "                fw.write(node + \"\\n\")\n",
    "\n",
    "        with open(filesentences, \"w\") as fs:\n",
    "            for nodes in self.walks:\n",
    "                for index in range(0, len(nodes)):\n",
    "                    if index == len(nodes) - 1:\n",
    "                        fs.write(nodes[index] + \"\\n\")\n",
    "                    else:\n",
    "                        fs.write(nodes[index] + \" \")\n",
    "                        \n",
    "    def get_negs(self, num_negs):\n",
    "        self.negs_u, self.negs_v = get_negs_by_lsh(self.edge_dict_u, self.edge_dict_v, num_negs)\n",
    "        # print(len(self.negs_u), len(self.negs_v))\n",
    "        return self.negs_u, self.negs_v\n",
    "    \n",
    "        \n",
    "    def get_context_and_fnegatives(self, G, walks,win_size, num_negs,table):\n",
    "        # generate context and negatives\n",
    "        if isinstance(G, Graph):\n",
    "            node_list = G.nodes()\n",
    "        elif isinstance(G, list):\n",
    "            node_list = G\n",
    "        word2id = {}\n",
    "        for i in range(len(node_list)):\n",
    "            word2id[node_list[i]] = i + 1\n",
    "        walk_list = walks\n",
    "        print(\"context...\")\n",
    "        context_dict = {}\n",
    "        new_neg_dict = {}\n",
    "        for step in range(len(walk_list)):\n",
    "\n",
    "            walk = walk_list[step % len(walk_list)]\n",
    "            # print(walk)\n",
    "            batch_labels = []\n",
    "            # travel each walk\n",
    "            for iter in range(len(walk)):\n",
    "                start = max(0, iter - win_size)\n",
    "                end = min(len(walk), iter + win_size + 1)\n",
    "                # index: index in window\n",
    "                if context_dict.get(walk[iter]) is None:\n",
    "                    context_dict[walk[iter]] = []\n",
    "                    new_neg_dict[walk[iter]] = []\n",
    "                labels_list = []\n",
    "                neg_sample = []\n",
    "                for index in range(start, end):\n",
    "                    labels_list.append(walk[index])\n",
    "                while len(neg_sample) < num_negs:\n",
    "                    sa = random.choice(range(len(node_list)))\n",
    "                    if table[sa] in labels_list:\n",
    "                        continue\n",
    "                    neg_sample.append(table[sa])\n",
    "                context_dict[walk[iter]].append(labels_list)\n",
    "                new_neg_dict[walk[iter]].append(neg_sample)\n",
    "            if len(batch_labels) == 0:\n",
    "                continue\n",
    "        print(\"context...ok\")\n",
    "        return context_dict, new_neg_dict\n",
    "        \n",
    "    def get_context_and_negatives(self,G,walks,win_size,num_negs,negs_dict):\n",
    "        # generate context and negatives\n",
    "        if isinstance(G, Graph):\n",
    "            node_list = G.nodes()\n",
    "        elif isinstance(G, list):\n",
    "            node_list = G\n",
    "        node_list = list(node_list)\n",
    "        word2id = {}\n",
    "        for i in range(len(node_list)):  # 每个节点给一个编号\n",
    "            word2id[node_list[i]] = i + 1\n",
    "        walk_list = walks\n",
    "        print(\"context...\")\n",
    "        context_dict = {}  # 上下文\n",
    "        new_neg_dict = {}  # 负采样\n",
    "        for step in range(len(walk_list)):\n",
    "            walk = walk_list[step % len(walk_list)]  # 多余的步骤; 选择游走序列\n",
    "            # print(walk)\n",
    "            # travel each walk\n",
    "            for iter in range(len(walk)):  # walk[iter] 中心词\n",
    "                start = max(0, iter - win_size)\n",
    "                end = min(len(walk), iter + win_size + 1)\n",
    "                # index: index in window\n",
    "                if context_dict.get(walk[iter]) is None:\n",
    "                    context_dict[walk[iter]] = []\n",
    "                    new_neg_dict[walk[iter]] = []\n",
    "                labels_list = []\n",
    "                negs = negs_dict[walk[iter]]   # 负采样词\n",
    "                for index in range(start, end):\n",
    "                    if walk[index] in negs:\n",
    "                        negs.remove(walk[index])  # 在负采样中，移除负采样中该节点\n",
    "                    if walk[index] == walk[iter]:  # 中心词本身，不是上线文节点;\n",
    "                        continue\n",
    "                    else:\n",
    "                        labels_list.append(walk[index])  # 选择上线文节点\n",
    "                neg_sample = random.sample(negs, min(num_negs, len(negs)))  # 随机负采样4个节点\n",
    "                context_dict[walk[iter]].append(labels_list)  # 上线文节点，训练样本\n",
    "                new_neg_dict[walk[iter]].append(neg_sample)  # 负采样节点，负样本\n",
    "        print(\"context...ok\")\n",
    "        return context_dict, new_neg_dict  # 上下文节点; 负采样节点;\n",
    "    \n",
    "    def save_homogenous_graph_to_file(self, A, datafile, index_row, index_item):\n",
    "        (M, N) = A.shape\n",
    "        csr_dict = A.__dict__\n",
    "        data = csr_dict.get(\"data\")\n",
    "        indptr = csr_dict.get(\"indptr\")\n",
    "        indices = csr_dict.get(\"indices\")\n",
    "        col_index = 0\n",
    "        with open(datafile, 'w') as fw:\n",
    "            for row in range(M):\n",
    "                for col in range(indptr[row], indptr[row + 1]):\n",
    "                    r = row\n",
    "                    c = indices[col]\n",
    "                    fw.write(index_row.get(r) + \"\\t\" + index_item.get(c) + \"\\t\" + str(data[col_index]) + \"\\n\")\n",
    "                    col_index += 1\n",
    "                    \n",
    "    def get_homogenous_graph(self, A, datafile, index_row, index_item):\n",
    "        (M, N) = A.shape\n",
    "        csr_dict = A.__dict__\n",
    "        data = csr_dict.get(\"data\")\n",
    "        indptr = csr_dict.get(\"indptr\")\n",
    "        indices = csr_dict.get(\"indices\")\n",
    "        col_index = 0\n",
    "        matrix = {}\n",
    "        with open(datafile, 'w') as fw:\n",
    "            for row in range(M):\n",
    "                for col in range(indptr[row], indptr[row+1]):\n",
    "                    r = index_row.get(row)\n",
    "                    c = index_item.get(indices[col])\n",
    "                    if matrix.get(r) is None:\n",
    "                        matrix[r] = []\n",
    "                    matrix[r].append(c)\n",
    "                    col_index += 1\n",
    "\n",
    "        return matrix\n",
    "    \n",
    "    def read_sentences_and_homogeneous_graph(self, filesentences=None, datafile=None):\n",
    "        G = load_edgelist(datafile, undirected=True)\n",
    "        walks = []\n",
    "        with open(filesentences, \"r\") as fin:\n",
    "            for line in fin.readlines():\n",
    "                walk = line.strip().split(\" \")\n",
    "                walks.append(walk)\n",
    "        return G, walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_embedding_vectors(node_u, node_v, node_list_u, node_list_v, args):\n",
    "    # user\n",
    "    for i in node_u:\n",
    "        vectors = np.random.random([1, args.d])\n",
    "        help_vectors = np.random.random([1, args.d])\n",
    "        node_list_u[i] = {}\n",
    "        node_list_u[i]['embedding_vectors'] = preprocessing.normalize(vectors, norm='l2')\n",
    "        node_list_u[i]['context_vectors'] = preprocessing.normalize(help_vectors, norm='l2')\n",
    "    # item\n",
    "    for i in node_v:\n",
    "        vectors = np.random.random([1, args.d])\n",
    "        help_vectors = np.random.random([1, args.d])\n",
    "        node_list_v[i] = {}\n",
    "        node_list_v[i]['embedding_vectors'] = preprocessing.normalize(vectors, norm='l2')\n",
    "        node_list_v[i]['context_vectors'] = preprocessing.normalize(help_vectors, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_generator(gul, args):\n",
    "    gul.calculate_centrality(args.mode)  # HITS计算节点的中心性\n",
    "    if args.large == 0:  # percentage:stop proba; maxT:最大长度; minT:最短长度\n",
    "        gul.homogeneous_graph_random_walks(percentage=args.p, maxT=args.maxT, minT=args.minT)\n",
    "    elif args.large == 1:\n",
    "        gul.homogeneous_graph_random_walks_for_large_bipartite_graph(percentage=args.p, maxT=args.maxT, minT=args.minT)\n",
    "    elif args.large == 2:\n",
    "        gul.homogeneous_graph_random_walks_for_large_bipartite_graph_without_generating(datafile=args.train_data,percentage=args.p,maxT=args.maxT, minT=args.minT)\n",
    "        \n",
    "    return gul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_and_negative_samples(gul, args):\n",
    "    if args.large == 0:\n",
    "        neg_dict_u, neg_dict_v = gul.get_negs(args.ns)  # 负采样方法\n",
    "        print(\"negative samples is ok...\")\n",
    "        context_dict_u, neg_dict_u = gul.get_context_and_negatives(gul.G_u, gul.walks_u, args.ws, args.ns, neg_dict_u)\n",
    "        context_dict_v, neg_dict_v = gul.get_context_and_negatives(gul.G_v, gul.walks_v, args.ws, args.ns, neg_dict_v)\n",
    "    else:\n",
    "        neg_dict_u, neg_dict_v = gul.get_negs(args.ns)  # 负采样方法\n",
    "        # print(len(gul.walks_u), len(gul.walks_v))\n",
    "        print(\"negative samples is ok...\")\n",
    "        context_dict_u, neg_dict_u = gul.get_context_and_negatives(gul.node_u, gul.walks_u, args.ws, args.ns, neg_dict_u)\n",
    "        context_dict_v, neg_dict_v = gul.get_context_and_negatives(gul.node_v, gul.walks_v, args.ws, args.ns, neg_dict_v)\n",
    "        \n",
    "    # u类别context和neg; v类别context和neg; u节点; v节点;\n",
    "    return context_dict_u, neg_dict_u, context_dict_v, neg_dict_v, gul.node_u, gul.node_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skip_gram(center, contexts, negs, node_list, lam, pa):\n",
    "    loss = 0\n",
    "    I_z = {contexts: 1}  # indication function\n",
    "    \n",
    "    for node in negs:\n",
    "        I_z[node] = 0  # 构造负样本\n",
    "        \n",
    "    V = np.array(node_list[center][\"embedding_vectors\"])  # 获取contexts节点的embedding\n",
    "    update = [[0] * V.size]\n",
    "    \n",
    "    for u in I_z.keys():\n",
    "        if node_list.get(u) is None:\n",
    "            pass\n",
    "        Theta = np.array(node_list[u][\"context_vectors\"])  # theta - context_embedding\n",
    "        X = float(V.dot(Theta.T))  # v * c\n",
    "        sigmod = 1.0 / (1 + (math.exp(-X * 1.0)))\n",
    "        update += pa * lam * (I_z[u] - sigmod) * Theta\n",
    "        node_list[u][\"context_vectors\"] += pa * lam * (I_z[u] - sigmod) * V\n",
    "        try:\n",
    "            loss += pa * (I_z[u] * math.log(sigmod) + (1 - I_z[u]) * math.log(1 - sigmod))  # cross-entropy\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return update, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL_divergence(edge_dict_u, u, v, node_list_u, node_list_v, lam, gamma):\n",
    "    loss = 0\n",
    "    e_ij = edge_dict_u[u][v]\n",
    "    \n",
    "    update_u = 0\n",
    "    update_v = 0\n",
    "    U = np.array(node_list_u[u]['embedding_vectors'])\n",
    "    V = np.array(node_list_v[v]['embedding_vectors'])\n",
    "    X = float(U.dot(V, T))\n",
    "    \n",
    "    sigmod = 1.0 / (1 + (math.exp(-X * 1.0)))\n",
    "    \n",
    "    update_u += gamma * lam * ((e_ij * (1 - sigmod)) * 1.0 / math.log(math.e, math.e)) * V\n",
    "    update_v += gamma * lam * ((e_ij * (1 - sigmod)) * 1.0 / math.log(math.e, math.e)) * U\n",
    "    \n",
    "    try:\n",
    "        loss += gamma * e_ij * math.log(sigmod)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return update_u, update_v, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_and_recall(ranked_list,ground_list):\n",
    "    hits = 0\n",
    "    for i in range(len(ranked_list)):\n",
    "        id = ranked_list[i]\n",
    "        if id in ground_list:\n",
    "            hits += 1\n",
    "    pre = hits/(1.0 * len(ranked_list))\n",
    "    rec = hits/(1.0 * len(ground_list))\n",
    "    return pre, rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AP(ranked_list, ground_truth):\n",
    "    hits, sum_precs = 0, 0.0\n",
    "    for i in range(len(ranked_list)):\n",
    "        id = ranked_list[i]\n",
    "        if id in ground_truth:\n",
    "            hits += 1\n",
    "            sum_precs += hits / (i+1.0)\n",
    "    if hits > 0:\n",
    "        return sum_precs / len(ground_truth)\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RR(ranked_list, ground_list):\n",
    "    for i in range(len(ranked_list)):\n",
    "        id = ranked_list[i]\n",
    "        if id in ground_list:\n",
    "            return 1 / (i + 1.0)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IDCG(n):\n",
    "    idcg = 0\n",
    "    for i in range(n):\n",
    "        idcg += 1 / math.log(i+2, 2)\n",
    "    return idcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nDCG(ranked_list, ground_truth):\n",
    "    dcg = 0\n",
    "    idcg = IDCG(len(ground_truth))\n",
    "    for i in range(len(ranked_list)):\n",
    "        id = ranked_list[i]\n",
    "        if id not in ground_truth:\n",
    "            continue\n",
    "        rank = i+1\n",
    "        dcg += 1/ math.log(rank+1, 2)\n",
    "    return dcg / idcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_N(test_u, test_v, test_rate, node_list_u, node_list_v, top_n):\n",
    "    recommend_dict = {}\n",
    "    for u in test_u:\n",
    "        recommend_dict[u] = {}\n",
    "        for v in test_v:\n",
    "            if node_list_u.get(u) is None:\n",
    "                pre = 0\n",
    "            else:\n",
    "                U = np.array(node_list_u[u][\"embedding_vectors\"])\n",
    "                if node_list_v.get(v) is None:\n",
    "                    pre = 0\n",
    "                else:\n",
    "                    V = np.array(node_list_v[v][\"embedding_vectors\"])\n",
    "                    pre = U.dot(V.T)[0][0]\n",
    "            recommend_dict[u][v] = float(pre)\n",
    "            \n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    ap_list = []\n",
    "    ndcg_list = []\n",
    "    rr_list = []\n",
    "    \n",
    "    for u in test_u:\n",
    "        tmp_r = sorted(recommend_dict[u].items(), lambda x, y: cmp(x[1], y[1]), reverse=True)[0: min(len(recommend_dict[u]), top_n)]\n",
    "        tmp_t = sorted(test_rate[u].items(), lambda x, y: cmp(x[1], y[1]), reverse=True)[0: min(len(test_rate[u]), top_n)]\n",
    "        tmp_r_list = []\n",
    "        tmp_t_list = []\n",
    "        \n",
    "        for (item, rate) in tmp_r:\n",
    "            tmp_r_list.append(item)\n",
    "            \n",
    "        for (item, rate) in tmp_t:\n",
    "            tmp_t_list.append(item)\n",
    "        \n",
    "        pre, rec = precision_and_recall(tmp_r_list, tmp_t_list)\n",
    "        ap = AP(tmp_r_list,tmp_t_list)\n",
    "        rr = RR(tmp_r_list,tmp_t_list)\n",
    "        ndcg = nDCG(tmp_r_list,tmp_t_list)\n",
    "        precision_list.append(pre)\n",
    "        recall_list.append(rec)\n",
    "        ap_list.append(ap)\n",
    "        rr_list.append(rr)\n",
    "        ndcg_list.append(ndcg)\n",
    "    \n",
    "    precison = sum(precision_list) / len(precision_list)\n",
    "    recall = sum(recall_list) / len(recall_list)\n",
    "    # print(precison, recall)\n",
    "    f1 = 2 * precison * recall / (precison + recall)\n",
    "    map = sum(ap_list) / len(ap_list)\n",
    "    mrr = sum(rr_list) / len(rr_list)\n",
    "    mndcg = sum(ndcg_list) / len(ndcg_list)\n",
    "    return f1, map, mrr, mndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateFeatureFile(filecase, filevector_u, filevector_v, fileout, factors):\n",
    "    vectors_u = {}\n",
    "    vectors_v = {}\n",
    "    \n",
    "    with open(filevector_u, \"r\") as fu:\n",
    "        for line in fu.readlines():\n",
    "            items = line.strip().split(\" \")\n",
    "            vectors_u[items[0]] = items[1:]\n",
    "    \n",
    "    with open(filevector_v, \"r\") as fv:\n",
    "        for line in fv.readlines():\n",
    "            items = line.strip().split(\" \")\n",
    "            vectors_v[items[0]] = items[1:]\n",
    "            \n",
    "    with open(filecase,'r') as fc, open(fileout,'w') as fo:\n",
    "        for line in fc.readlines():\n",
    "            items = line.strip().split(\"\\t\")\n",
    "            if vectors_u.get(items[0]) == None:\n",
    "                vectors_u[items[0]] = ['0'] * factors\n",
    "            if vectors_v.get(items[1]) == None:\n",
    "                vectors_v[items[1]] = ['0'] * factors\n",
    "            if items[-1] == '1':\n",
    "                fo.write('{}\\t{}\\t{}\\n'.format('\\t'.join(vectors_u[items[0]]),'\\t'.join(vectors_v[items[1]]), 1))\n",
    "            else:\n",
    "                fo.write('{}\\t{}\\t{}\\n'.format('\\t'.join(vectors_u[items[0]]),'\\t'.join(vectors_v[items[1]]), 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_prediction(args):\n",
    "    filecase_a = args.case_train\n",
    "    filecase_e = args.case_test\n",
    "    filevector_u = args.vectors_u\n",
    "    filevector_v = args.vectors_v\n",
    "    filecase_a_c = r'C:\\Users\\sss\\Desktop\\BiNE\\data/features_train.dat'\n",
    "    filecase_e_c = r'C:\\Users\\sss\\Desktop\\BiNE\\data/features_test.dat'\n",
    "    generateFeatureFile(filecase_a, filevector_u, filevector_v, filecase_a_c, args.d)\n",
    "    generateFeatureFile(filecase_e, filevector_u, filevector_v, filecase_e_c, args.d)\n",
    "    \n",
    "    df_data_train = pd.read_csv(filecase_a_c, header=None, sep='\\t', encoding='utf-8')\n",
    "    X_train = df_data_train.drop(len(df_data_train.keys()) - 1, axis=1)\n",
    "    y_train = df_data_train[len(df_data_train.keys()) - 1]\n",
    "    \n",
    "    df_data_test = pd.read_csv(filecase_e_c, header=None, sep='\\t', encoding='utf-8')\n",
    "    X_test = df_data_test.drop(len(df_data_train.keys()) - 1, axis=1)\n",
    "    X_test = X_test.fillna(X_test.mean())\n",
    "    y_test = df_data_test[len(df_data_test.keys()) - 1]\n",
    "    y_test_list = list(y_test) \n",
    "    \n",
    "    lg = LogisticRegression(penalty='l2', C=0.001)\n",
    "    lg.fit(X_train, y_train)\n",
    "    lg_y_pred_est = lg.predict_proba(X_test)[:,1]\n",
    "    fpr,tpr,thresholds = metrics.roc_curve(y_test, lg_y_pred_est)\n",
    "    average_precision = average_precision_score(y_test, lg_y_pred_est)\n",
    "    os.remove(filecase_a_c)\n",
    "    os.remove(filecase_e_c)\n",
    "    \n",
    "    return metrics.auc(fpr,tpr), average_precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_by_sampling(args):\n",
    "    model_path = os.path.join(r'C:\\Users\\sss\\Desktop\\BiNE/', args.model_name)\n",
    "    if os.path.exists(model_path) is False:\n",
    "        os.makedirs(model_path)\n",
    "        \n",
    "    alpha, beta, gamma, lam = args.alpha, args.beta, args.gamma, args.lam\n",
    "    print('======== experiment settings =========')\n",
    "    print('alpha : %0.4f, beta : %0.4f, gamma : %0.4f, lam : %0.4f, p : %0.4f, ws : %d, ns : %d, maxT : % d, minT : %d, max_iter : %d, d : %d' % (alpha, beta, gamma, lam, args.p, args.ws, args.ns,args.maxT,args.minT,args.max_iter, args.d))\n",
    "    \n",
    "    print('========== processing data ===========')\n",
    "    dul = DataUtils(model_path)\n",
    "    if args.rec:\n",
    "        test_user, test_item, test_rate = dul.read_data(args.test_data)\n",
    "        \n",
    "    print(\"constructing graph....\")\n",
    "    gul = GraphUtils(model_path)\n",
    "    gul.construct_training_graph(args.train_data)  # train_data='../data/wiki/rating_train.dat'\n",
    "    edge_dict_u = gul.edge_dict_u  # dict形式的点边关系\n",
    "    edge_list = gul.edge_list  # list形式的点边关系\n",
    "    walk_generator(gul,args)  # 生成随机游走\n",
    "    \n",
    "    print(\"getting context and negative samples....\")\n",
    "    context_dict_u, neg_dict_u, context_dict_v, neg_dict_v, node_u, node_v = get_context_and_negative_samples(gul, args)\n",
    "    node_list_u, node_list_v = {}, {}\n",
    "    init_embedding_vectors(node_u, node_v, node_list_u, node_list_v, args)  # 初始化节点embedding\n",
    "    last_loss, count, epsilon = 0, 0, 1e-3\n",
    " \n",
    "    print(\"============== training ==============\")\n",
    "    for iter in range(0, args.max_iter):\n",
    "        s1 = \"\\r[%s%s]%0.2f%%\"%(\"*\"* iter,\" \"*(args.max_iter-iter),iter*100.0/(args.max_iter-1))\n",
    "        loss = 0\n",
    "        visited_u = dict(zip(node_list_u.keys(), [0] * len(node_list_u.keys())))  # u类别初始为0\n",
    "        visited_v = dict(zip(node_list_v.keys(), [0] * len(node_list_v.keys())))  # v类别初始为0\n",
    "        random.shuffle(edge_list)  # edge_list: 点边信息\n",
    "        for i in range(len(edge_list)):\n",
    "            u, v, w = edge_list[i]\n",
    "\n",
    "            length = len(context_dict_u[u])  # 周围邻居的数量\n",
    "            random.shuffle(context_dict_u[u])\n",
    "            if visited_u.get(u) < length:\n",
    "                # print(u)\n",
    "                index_list = list(range(visited_u.get(u),min(visited_u.get(u)+1,length)))\n",
    "                for index in index_list:\n",
    "                    context_u = context_dict_u[u][index]  # 选择节点的一个邻居\n",
    "                    neg_u = neg_dict_u[u][index]  # 选择节点的负采样信息; 负采样本身就是随机的，所以只需打乱context即可，并且多个epoch训练时，负采样样本也不同\n",
    "                    # center,context,neg,node_list,eta\n",
    "                    for z in context_u:  # 每一个邻居节点，都进行skip-gram更新embedding\n",
    "                        tmp_z, tmp_loss = skip_gram(u, z, neg_u, node_list_u, lam, alpha)\n",
    "                        node_list_u[u]['embedding_vectors'] += tmp_z  # 更新节点embedding\n",
    "                        loss += tmp_loss\n",
    "                visited_u[u] = index_list[-1]+3\n",
    "\n",
    "            length = len(context_dict_v[v])\n",
    "            random.shuffle(context_dict_v[v])\n",
    "            if visited_v.get(v) < length:\n",
    "                # print(v)\n",
    "                index_list = list(range(visited_v.get(v),min(visited_v.get(v)+1,length)))\n",
    "                for index in index_list:\n",
    "                    context_v = context_dict_v[v][index]\n",
    "                    neg_v = neg_dict_v[v][index]\n",
    "                    # center,context,neg,node_list,eta\n",
    "                    for z in context_v:\n",
    "                        tmp_z, tmp_loss = skip_gram(v, z, neg_v, node_list_v, lam, beta)\n",
    "                        node_list_v[v]['embedding_vectors'] += tmp_z\n",
    "                        loss += tmp_loss\n",
    "                visited_v[v] = index_list[-1]+3\n",
    "            # edge_dict_u:边连接的信息\n",
    "            update_u, update_v, tmp_loss = KL_divergence(edge_dict_u, u, v, node_list_u, node_list_v, lam, gamma)  # 计算KL-deversion\n",
    "            loss += tmp_loss\n",
    "            node_list_u[u]['embedding_vectors'] += update_u\n",
    "            node_list_v[v]['embedding_vectors'] += update_v\n",
    "        # 求的是梯度上升，loss越大越好\n",
    "        delta_loss = abs(loss - last_loss)\n",
    "        if last_loss > loss:\n",
    "            lam *= 1.05\n",
    "        else:\n",
    "            lam *= 0.95\n",
    "        last_loss = loss\n",
    "        if delta_loss < epsilon:\n",
    "            break\n",
    "        sys.stdout.write(s1)\n",
    "        sys.stdout.flush()\n",
    "    save_to_file(node_list_u,node_list_v,model_path,args)\n",
    "    print(\"\")\n",
    "    if args.rec:\n",
    "        print(\"============== testing ===============\")\n",
    "        f1, map, mrr, mndcg = top_N(test_user,test_item,test_rate,node_list_u,node_list_v,args.top_n)\n",
    "        print('recommendation metrics: F1 : %0.4f, MAP : %0.4f, MRR : %0.4f, NDCG : %0.4f' % (round(f1,4), round(map,4), round(mrr,4), round(mndcg,4)))\n",
    "    if args.lip:\n",
    "        print(\"============== testing ===============\")\n",
    "        auc_roc, auc_pr = link_prediction(args)\n",
    "        print('link prediction metrics: AUC_ROC : %0.4f, AUC_PR : %0.4f' % (round(auc_roc,4), round(auc_pr,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== experiment settings =========\n",
      "alpha : 0.0100, beta : 0.0100, gamma : 0.1000, lam : 0.0100, p : 0.1500, ws : 5, ns : 4, maxT :  32, minT : 1, max_iter : 50, d : 128\n",
      "========== processing data ===========\n",
      "constructing graph....\n"
     ]
    }
   ],
   "source": [
    "train_by_sampling(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    model_path = os.path.join(r'C:\\Users\\sss\\Desktop\\BiNE/', args.model_name)\n",
    "    if os.path.exists(model_path) is False:\n",
    "        os.makedirs(model_path)\n",
    "    alpha, beta, gamma, lam = args.alpha, args.beta, args.gamma, args.lam\n",
    "    print('======== experiment settings =========')\n",
    "    print('alpha : %0.4f, beta : %0.4f, gamma : %0.4f, lam : %0.4f, p : %0.4f, ws : %d, ns : %d, maxT : % d, minT : %d, max_iter : %d, d : %d' % (alpha, beta, gamma, lam, args.p, args.ws, args.ns,args.maxT,args.minT,args.max_iter, args.d))\n",
    "    print('========== processing data ===========')\n",
    "    dul = DataUtils(model_path)\n",
    "    if args.rec:\n",
    "        test_user, test_item, test_rate = dul.read_data(args.test_data)\n",
    "    print(\"constructing graph....\")\n",
    "    gul = GraphUtils(model_path)\n",
    "    gul.construct_training_graph(args.train_data)\n",
    "    edge_dict_u = gul.edge_dict_u\n",
    "    edge_list = gul.edge_list\n",
    "    walk_generator(gul,args)\n",
    "\n",
    "    print(\"getting context and negative samples....\")\n",
    "    context_dict_u, neg_dict_u, context_dict_v, neg_dict_v, node_u, node_v = get_context_and_negative_samples(gul, args)\n",
    "    node_list_u, node_list_v = {}, {}\n",
    "    init_embedding_vectors(node_u, node_v, node_list_u, node_list_v, args)\n",
    "\n",
    "    last_loss, count, epsilon = 0, 0, 1e-3\n",
    "    print(\"============== training ==============\")\n",
    "    for iter in range(0, args.max_iter):\n",
    "        s1 = \"\\r[%s%s]%0.2f%%\"%(\"*\"* iter,\" \"*(args.max_iter-iter),iter*100.0/(args.max_iter-1))\n",
    "        loss = 0\n",
    "        num = 0\n",
    "        visited_u = dict(zip(node_list_u.keys(), [0] * len(node_list_u.keys())))\n",
    "        visited_v = dict(zip(node_list_v.keys(), [0] * len(node_list_v.keys())))\n",
    "\n",
    "        random.shuffle(edge_list)\n",
    "        for (u, v, w) in edge_list:\n",
    "            if visited_u.get(u) == 0 or random.random() > 0.95:\n",
    "                # print(u)\n",
    "                length = len(context_dict_u[u])\n",
    "                index_list = random.sample(list(range(length)), min(length, 1))\n",
    "                for index in index_list:\n",
    "                    context_u = context_dict_u[u][index]\n",
    "                    neg_u = neg_dict_u[u][index]\n",
    "                    # center,context,neg,node_list,eta\n",
    "                    for k, z in enumerate(context_u):\n",
    "                        tmp_z, tmp_loss = skip_gram(u, z, neg_u, node_list_u, lam, alpha)\n",
    "                        node_list_u[z]['embedding_vectors'] += tmp_z\n",
    "                        loss += tmp_loss\n",
    "                visited_u[u] = 1\n",
    "            if visited_v.get(v) == 0 or random.random() > 0.95:\n",
    "                # print(v)\n",
    "                length = len(context_dict_v[v])\n",
    "                index_list = random.sample(list(range(length)), min(length, 1))\n",
    "                for index in index_list:\n",
    "                    context_v = context_dict_v[v][index]\n",
    "                    neg_v = neg_dict_v[v][index]\n",
    "                    # center,context,neg,node_list,eta\n",
    "                    for k,z in enumerate(context_v):\n",
    "                        tmp_z, tmp_loss = skip_gram(v, z, neg_v, node_list_v, lam, beta)\n",
    "                        node_list_v[z]['embedding_vectors'] += tmp_z\n",
    "                        loss += tmp_loss\n",
    "                visited_v[v] = 1\n",
    "            # print(len(edge_dict_u))\n",
    "            update_u, update_v, tmp_loss = KL_divergence(edge_dict_u, u, v, node_list_u, node_list_v, lam, gamma)\n",
    "            loss += tmp_loss\n",
    "            node_list_u[u]['embedding_vectors'] += update_u\n",
    "            node_list_v[v]['embedding_vectors'] += update_v\n",
    "            count = iter\n",
    "            num += 1\n",
    "        delta_loss = abs(loss - last_loss)\n",
    "        if last_loss > loss:\n",
    "            lam *= 1.05\n",
    "        else:\n",
    "            lam *= 0.95\n",
    "        last_loss = loss\n",
    "        if delta_loss < epsilon:\n",
    "            break\n",
    "        sys.stdout.write(s1)\n",
    "        sys.stdout.flush()\n",
    "    save_to_file(node_list_u,node_list_v,model_path,args)\n",
    "    print(\"\")\n",
    "    if args.rec:\n",
    "        print(\"============== testing ===============\")\n",
    "        f1, map, mrr, mndcg = top_N(test_user,test_item,test_rate,node_list_u,node_list_v,args.top_n)\n",
    "        print('recommendation metrics: F1 : %0.4f, MAP : %0.4f, MRR : %0.4f, NDCG : %0.4f' % (round(f1,4), round(map,4), round(mrr,4), round(mndcg,4)))\n",
    "    if args.lip:\n",
    "        print(\"============== testing ===============\")\n",
    "        auc_roc, auc_pr = link_prediction(args)\n",
    "        print('link prediction metrics: AUC_ROC : %0.4f, AUC_PR : %0.4f' % (round(auc_roc,4), round(auc_pr,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndarray_tostring(array):\n",
    "    string = \"\"\n",
    "    for item in array[0]:\n",
    "        string += str(item).strip()+\" \"\n",
    "    return string+\"\\n\"\n",
    "\n",
    "def save_to_file(node_list_u,node_list_v,model_path,args):\n",
    "    with open(args.vectors_u,\"w\") as fw_u:\n",
    "        for u in node_list_u.keys():\n",
    "            fw_u.write(u+\" \"+ ndarray_tostring(node_list_u[u]['embedding_vectors']))\n",
    "    with open(args.vectors_v,\"w\") as fw_v:\n",
    "        for v in node_list_v.keys():\n",
    "            fw_v.write(v+\" \"+ndarray_tostring(node_list_v[v]['embedding_vectors']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = ArgumentParser(\"BiNE\", formatter_class=ArgumentDefaultsHelpFormatter, conflict_handler='resolve')\n",
    "\n",
    "parser.add_argument('--train-data', default=r'../data/rating_train.dat', help='Input graph file.')\n",
    "parser.add_argument('--test-data', default=r'../data/rating_test.dat')\n",
    "parser.add_argument('--vectors-u', default=r'../data/vectors_u.dat', help=\"file of embedding vectors of U\")\n",
    "parser.add_argument('--vectors-v', default=r'../data/vectors_v.dat', help=\"file of embedding vectors of V\")\n",
    "parser.add_argument('--case-train', default=r'../data/wiki/case_train.dat', help=\"file of training data for LR\")\n",
    "parser.add_argument('--case-test', default=r'../data/wiki/case_test.dat', help=\"file of testing data for LR\")\n",
    "\n",
    "parser.add_argument('--model-name', default='default', help='name of model.')\n",
    "parser.add_argument('--ws', default=5, type=int, help='window size.')\n",
    "parser.add_argument('--ns', default=4, type=int, help='number of negative samples.')\n",
    "parser.add_argument('--d', default=128, type=int, help='embedding size.')\n",
    "parser.add_argument('--maxT', default=32, type=int, help='maximal walks per vertex.')\n",
    "parser.add_argument('--minT', default=1, type=int, help='minimal walks per vertex.')\n",
    "parser.add_argument('--p', default=0.15, type=float, help='walk stopping probability.')\n",
    "parser.add_argument('--alpha', default=0.01, type=float, help='trade-off parameter alpha.')\n",
    "parser.add_argument('--beta', default=0.01, type=float, help='trade-off parameter beta.')\n",
    "parser.add_argument('--gamma', default=0.1, type=float, help='trade-off parameter gamma.')\n",
    "parser.add_argument('--lam', default=0.01, type=float, help='learning rate lambda.')\n",
    "parser.add_argument('--max-iter', default=50, type=int, help='maximal number of iterations.')\n",
    "parser.add_argument('--top-n', default=10, type=int, help='recommend top-n items for each user.')\n",
    "parser.add_argument('--rec', default=0, type=int, help='calculate the recommendation metrics.')\n",
    "parser.add_argument('--lip', default=0, type=int, help='calculate the link prediction metrics.')\n",
    "parser.add_argument('--large', default=0, type=int, help='for large bipartite, 1 do not generate homogeneous graph file; 2 do not generate homogeneous graph')\n",
    "parser.add_argument('--mode', default='hits', type=str, help='metrics of centrality')\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "root = r\"C:\\Users\\sss\\Desktop\\BiNE/data/wiki/\"\n",
    "args.train_data, args.test_data = root + 'rating_train.dat', root + 'rating_test.dat'\n",
    "args.vectors_u, args.vectors_v = root + 'vectors_u.dat', root + 'vectors_v.dat'\n",
    "args.case_train, args.case_test = root + 'case_train.dat', root + 'case_test.dat'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_by_sampling(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
