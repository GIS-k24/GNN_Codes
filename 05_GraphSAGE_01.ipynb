{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GraphSAGE -- GNN范轴 -- Embedding\n",
    "- 是一种能够利用顶点的属性信息高效产生未知顶点embedding的一种归纳式(inductive)学习的框架\n",
    "- 只利用节点之间的连接关系 -- 对点进行编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import argparse\n",
    "import pyhocon\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='pytorch version of GraphSAGE')\n",
    "\n",
    "parser.add_argument('--dataSet', type=str, default='cora')\n",
    "parser.add_argument('--agg_func', type=str, default='MEAN')\n",
    "parser.add_argument('--epochs', type=int, default=2)\n",
    "parser.add_argument('--b_sz', type=int, default=20)\n",
    "parser.add_argument('--seed', type=int, default=824)\n",
    "parser.add_argument('--gcn', action='store_true')\n",
    "parser.add_argument('--learn_method', type=str, default='sup')\n",
    "parser.add_argument('--unsup_loss', type=str, default='normal')\n",
    "parser.add_argument('--max_vali_f1', type=float, default=0)\n",
    "parser.add_argument('--name', type=str, default='debug')\n",
    "\n",
    "# attention\n",
    "parser.add_argument('--config', type=str, default=r'C:\\Users\\sss\\Desktop/experiments.conf')  #/src\n",
    "\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed_all(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = pyhocon.ConfigFactory.parse_file(args.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConfigTree([('file_path',\n",
       "             ConfigTree([('workdir', '../'),\n",
       "                         ('cora_content',\n",
       "                          'C:\\\\Users\\\\sss\\\\Desktop\\\\cora/cora.content'),\n",
       "                         ('cora_cite',\n",
       "                          'C:\\\\Users\\\\sss\\\\Desktop\\\\cora/cora.cites'),\n",
       "                         ('pubmed_paper',\n",
       "                          '../pubmed-data/Pubmed-Diabetes.NODE.paper.tab'),\n",
       "                         ('pubmed_cites',\n",
       "                          '../pubmed-data/Pubmed-Diabetes.DIRECTED.cites.tab')])),\n",
       "            ('setting',\n",
       "             ConfigTree([('num_layers', 2), ('hidden_emb_size', 128)]))])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class DataCenter(object):\n",
    "    \"\"\"docstring for DataCenter\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super(DataCenter, self).__init__()\n",
    "        self.config = config\n",
    "        \n",
    "    def load_dataSet(self, dataSet='cora'):\n",
    "        if dataSet == 'cora':\n",
    "            cora_content_file = self.config['file_path.cora_content']\n",
    "            cora_cite_file = self.config['file_path.cora_cite']\n",
    "\n",
    "            feat_data = []\n",
    "            labels = [] # label sequence of node\n",
    "            node_map = {} # map node to Node_ID\n",
    "            label_map = {} # map label to Label_ID\n",
    "            with open(cora_content_file) as fp:\n",
    "                for i, line in enumerate(fp):\n",
    "                    info = line.strip().split()\n",
    "                    feat_data.append([float(x) for x in info[1: -1]])\n",
    "                    node_map[info[0]] = i  # 节点的index\n",
    "                    if not info[-1] in label_map:\n",
    "                        label_map[info[-1]] = len(label_map)\n",
    "                    labels.append(label_map[info[-1]])\n",
    "            feat_data = np.asarray(feat_data)\n",
    "            labels = np.asarray(labels, dtype=np.int64)\n",
    "                \n",
    "            adj_lists = defaultdict(set)\n",
    "            with open(cora_cite_file) as fp:\n",
    "                for i, line in enumerate(fp):\n",
    "                    info = line.strip().split()\n",
    "                    assert len(info) == 2\n",
    "                    paper1 = node_map[info[0]]\n",
    "                    paper2 = node_map[info[1]]\n",
    "                    adj_lists[paper1].add(paper2)\n",
    "                    adj_lists[paper2].add(paper1)\n",
    "\n",
    "            assert len(feat_data) == len(labels) == len(adj_lists)\n",
    "            test_indexs, val_indexs, train_indexs = self._split_data(feat_data.shape[0])  # 选择data中训练，验证，测试集\n",
    "\n",
    "            setattr(self, dataSet + '_test', test_indexs)\n",
    "            setattr(self, dataSet + '_val', val_indexs)\n",
    "            setattr(self, dataSet + '_train', train_indexs)\n",
    "\n",
    "            setattr(self, dataSet + '_feats', feat_data)\n",
    "            setattr(self, dataSet + '_labels', labels)\n",
    "            setattr(self, dataSet + '_adj_lists', adj_lists)\n",
    "\n",
    "        elif dataSet == 'pubmed':\n",
    "            pubmed_content_file = self.config['file_path.pubmed_paper']\n",
    "            pubmed_cite_file = self.config['file_path.pubmed_cites']\n",
    "\n",
    "            feat_data = []\n",
    "            labels = [] # label sequence of node\n",
    "            node_map = {} # map node to Node_ID\n",
    "            with open(pubmed_content_file) as fp:\n",
    "                fp.readline()\n",
    "                feat_map = {entry.split(\":\")[1]:i-1 for i,entry in enumerate(fp.readline().split(\"\\t\"))}\n",
    "                for i, line in enumerate(fp):\n",
    "                    info = line.split(\"\\t\")\n",
    "                    node_map[info[0]] = i\n",
    "                    labels.append(int(info[1].split(\"=\")[1])-1)\n",
    "                    tmp_list = np.zeros(len(feat_map)-2)\n",
    "                    for word_info in info[2:-1]:\n",
    "                        word_info = word_info.split(\"=\")\n",
    "                        tmp_list[feat_map[word_info[0]]] = float(word_info[1])\n",
    "                    feat_data.append(tmp_list)\n",
    "                \n",
    "            feat_data = np.asarray(feat_data)\n",
    "            labels = np.asarray(labels, dtype=np.int64)\n",
    "                \n",
    "            adj_lists = defaultdict(set)\n",
    "            with open(pubmed_cite_file) as fp:\n",
    "                fp.readline()\n",
    "                fp.readline()\n",
    "                for line in fp:\n",
    "                    info = line.strip().split(\"\\t\")\n",
    "                    paper1 = node_map[info[1].split(\":\")[1]]\n",
    "                    paper2 = node_map[info[-1].split(\":\")[1]]\n",
    "                    adj_lists[paper1].add(paper2)\n",
    "                    adj_lists[paper2].add(paper1)\n",
    "                \n",
    "            assert len(feat_data) == len(labels) == len(adj_lists)\n",
    "            test_indexs, val_indexs, train_indexs = self._split_data(feat_data.shape[0])\n",
    "\n",
    "            setattr(self, dataSet + '_test', test_indexs)\n",
    "            setattr(self, dataSet + '_val', val_indexs)\n",
    "            setattr(self, dataSet + '_train', train_indexs)\n",
    "\n",
    "            setattr(self, dataSet + '_feats', feat_data)\n",
    "            setattr(self, dataSet + '_labels', labels)\n",
    "            setattr(self, dataSet + '_adj_lists', adj_lists)\n",
    "\n",
    "\n",
    "    def _split_data(self, num_nodes, test_split = 3, val_split = 6):\n",
    "        rand_indices = np.random.permutation(num_nodes)\n",
    "\n",
    "        test_size = num_nodes // test_split\n",
    "        val_size = num_nodes // val_split\n",
    "        train_size = num_nodes - (test_size + val_size)\n",
    "\n",
    "        test_indexs = rand_indices[:test_size]\n",
    "        val_indexs = rand_indices[test_size:(test_size+val_size)]\n",
    "        train_indexs = rand_indices[(test_size+val_size):]\n",
    "            \n",
    "        return test_indexs, val_indexs, train_indexs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "ds = args.dataSet\n",
    "dataCenter = DataCenter(config)\n",
    "dataCenter.load_dataSet(ds)  # 读取数据\n",
    "\n",
    "features = torch.FloatTensor(getattr(dataCenter, ds + '_feats')).to(device)\n",
    "labels = torch.FloatTensor(getattr(dataCenter, ds + '_labels')).to(device)\n",
    "adj = getattr(dataCenter, ds + '_adj_lists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([2708, 1433]), torch.Size([2708]), 2708]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[features.shape, labels.shape, len(adj)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型一：GraphSAGE\n",
    "    第一层：[2708, 1433] -> [2708, 128]\n",
    "    第二层：[2708, 128] -> [2708, 128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SageLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes a node's using 'convolutional' GraphSage approach\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, out_size, gcn=False): \n",
    "        super(SageLayer, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.out_size = out_size\n",
    "        self.gcn = gcn\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_size, self.input_size if self.gcn else 2 * self.input_size)) # 创建weight\n",
    "\n",
    "        self.init_params()  # 初始化参数\n",
    "\n",
    "    def init_params(self):\n",
    "        for param in self.parameters():\n",
    "            nn.init.xavier_uniform_(param)\n",
    "\n",
    "    def forward(self, self_feats, aggregate_feats, neighs=None):\n",
    "        \"\"\"\n",
    "        Generates embeddings for a batch of nodes\n",
    "        nodes -- list of nodes\n",
    "        \"\"\"\n",
    "        if not self.gcn:\n",
    "            combined = torch.cat([self_feats, aggregate_feats], dim=1)   # concat自己信息和邻居信息\n",
    "        else:\n",
    "            combined = aggregate_feats\n",
    "        combined = F.relu(self.weight.mm(combined.t())).t()\n",
    "        return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSage(nn.Module):\n",
    "    \"\"\"docstring for GraphSage\"\"\"\n",
    "    def __init__(self, num_layers, input_size, out_size, raw_features, adj_lists, device, gcn=False, agg_func='MEAN'):\n",
    "        super(GraphSage, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.out_size = out_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gcn = gcn\n",
    "        self.device = device\n",
    "        self.agg_func = agg_func\n",
    "\n",
    "        self.raw_features = raw_features  # 点的特征\n",
    "        self.adj_lists = adj_lists  # 边的连接\n",
    "\n",
    "        for index in range(1, num_layers + 1):\n",
    "            layer_size = out_size if index != 1 else input_size\n",
    "            setattr(self, 'sage_layer' + str(index), SageLayer(layer_size, out_size, gcn=self.gcn))\n",
    "\n",
    "    def forward(self, nodes_batch):\n",
    "        \"\"\"\n",
    "        Generates embeddings for a batch of nodes.\n",
    "        nodes_batch -- batch of nodes to learn the embeddings.    《minbatch 过程，涉及到的所有节点》\n",
    "        \"\"\"\n",
    "        lower_layer_nodes = list(nodes_batch)\n",
    "        nodes_batch_layers = [(lower_layer_nodes,)]  # 第一次放入的节点，batch节点\n",
    "        # self.dc.logger.info('get_unique_neighs.')\n",
    "        for i in range(self.num_layers):  # 每层的Sage\n",
    "            lower_samp_neighs, lower_layer_nodes_dict, lower_layer_nodes = self._get_unique_neighs_list(lower_layer_nodes)  # 获得neighbors。 聚合自己和邻居节点，点的dict，涉及到的所有节点\n",
    "            nodes_batch_layers.insert(0, (lower_layer_nodes, lower_samp_neighs, lower_layer_nodes_dict))  # 聚合自己和邻居节点，点的dict，涉及到的所有节点\n",
    "            # insert,0 从最外层开始聚合\n",
    "        assert len(nodes_batch_layers) == self.num_layers + 1\n",
    "\n",
    "        pre_hidden_embs = self.raw_features\n",
    "        \n",
    "        for index in range(1, self.num_layers + 1):\n",
    "            nb = nodes_batch_layers[index][0]   # 聚合自己和周围的节点\n",
    "            pre_neighs = nodes_batch_layers[index - 1]  # 这层节点的上层邻居的所有信息。聚合自己和邻居节点，点的dict，涉及到的所有节点\n",
    "            # self.dc.logger.info('aggregate_feats.') aggrefate_feats=>输出GraphSAGE聚合后的信息\n",
    "            aggregate_feats = self.aggregate(nb, pre_hidden_embs, pre_neighs)  # 聚合函数。nb-这一层的节点， pre_hidden_embs-feature，pre_neighs-上一层节点\n",
    "            sage_layer = getattr(self, 'sage_layer' + str(index))\n",
    "            if index > 1:\n",
    "                nb = self._nodes_map(nb, pre_hidden_embs, pre_neighs)   # 第一层的batch节点，没有进行转换\n",
    "            # self.dc.logger.info('sage_layer.')\n",
    "            cur_hidden_embs = sage_layer(\n",
    "                self_feats=pre_hidden_embs[nb],\n",
    "                aggregate_feats=aggregate_feats\n",
    "            )  # 进入SageLayer。weight*concat(node,neighbors)\n",
    "            pre_hidden_embs = cur_hidden_embs\n",
    "\n",
    "        return pre_hidden_embs\n",
    "\n",
    "    def _nodes_map(self, nodes, hidden_embs, neighs):\n",
    "        layer_nodes, samp_neighs, layer_nodes_dict = neighs\n",
    "        assert len(samp_neighs) == len(nodes)\n",
    "        index = [layer_nodes_dict[x] for x in nodes]  # 记录将上一层的节点编号。\n",
    "        return index\n",
    "\n",
    "    def _get_unique_neighs_list(self, nodes, num_sample=10):\n",
    "        _set = set\n",
    "        to_neighs = [self.adj_lists[int(node)] for node in nodes]    # self.adj_lists边矩阵，获取节点的邻居\n",
    "        if not num_sample is None:  # 对邻居节点进行采样，如果大于邻居数据，则进行采样\n",
    "            _sample = random.sample\n",
    "            samp_neighs = [_set(_sample(to_neigh, num_sample)) if len(to_neigh) >= num_sample else to_neigh for to_neigh in to_neighs]\n",
    "        else:\n",
    "            samp_neighs = to_neighs\n",
    "        samp_neighs = [samp_neigh | set([nodes[i]]) for i, samp_neigh in enumerate(samp_neighs)]  # 聚合本身节点和邻居节点\n",
    "        _unique_nodes_list = list(set.union(*samp_neighs))  # 这个batch涉及到的所有节点\n",
    "        i = list(range(len(_unique_nodes_list)))\n",
    "        unique_nodes = dict(list(zip(_unique_nodes_list, i)))  # 字典编号\n",
    "        return samp_neighs, unique_nodes, _unique_nodes_list   # 聚合自己和邻居节点，点的dict，涉及到的所有节点\n",
    "\n",
    "    def aggregate(self, nodes, pre_hidden_embs, pre_neighs, num_sample=10):\n",
    "        unique_nodes_list, samp_neighs, unique_nodes = pre_neighs   # 聚合自己和邻居节点，涉及到的所有节点，点的dict\n",
    "\n",
    "        assert len(nodes) == len(samp_neighs)\n",
    "        indicator = [(nodes[i] in samp_neighs[i]) for i in range(len(samp_neighs))]  # 都是True，因为上文中，将nodes加入到neighs中了\n",
    "        assert (False not in indicator)\n",
    "        if not self.gcn:\n",
    "            samp_neighs = [(samp_neighs[i]-set([nodes[i]])) for i in range(len(samp_neighs))]  # 在把中心节点去掉\n",
    "        # self.dc.logger.info('2')\n",
    "        if len(pre_hidden_embs) == len(unique_nodes):  # 如果涉及到所有节点，保留原矩阵。如果不涉及所有节点，保留部分矩阵。\n",
    "            embed_matrix = pre_hidden_embs\n",
    "        else:\n",
    "            embed_matrix = pre_hidden_embs[torch.LongTensor(unique_nodes_list)]\n",
    "        # self.dc.logger.info('3')  将对应到的边，构建邻接矩阵\n",
    "        mask = torch.zeros(len(samp_neighs), len(unique_nodes))   # 本层节点数量，涉及到上层节点数量\n",
    "        column_indices = [unique_nodes[n] for samp_neigh in samp_neighs for n in samp_neigh]  # 构建邻接矩阵\n",
    "        row_indices = [i for i in range(len(samp_neighs)) for j in range(len(samp_neighs[i]))]\n",
    "        mask[row_indices, column_indices] = 1   # 加上上两个步骤，都是构建邻接矩阵;\n",
    "        # self.dc.logger.info('4')\n",
    "        # mask - 邻接矩阵\n",
    "        if self.agg_func == 'MEAN':\n",
    "            num_neigh = mask.sum(1, keepdim=True)    # 按行求和，保持和输入一个维度\n",
    "            mask = mask.div(num_neigh).to(embed_matrix.device)  # 归一化操作\n",
    "            aggregate_feats = mask.mm(embed_matrix)   # 矩阵相乘，相当于聚合周围邻接信息求和\n",
    "\n",
    "        elif self.agg_func == 'MAX':\n",
    "            # print(mask)\n",
    "            indexs = [x.nonzero() for x in mask==1]\n",
    "            aggregate_feats = []\n",
    "            # self.dc.logger.info('5')\n",
    "            for feat in [embed_matrix[x.squeeze()] for x in indexs]:\n",
    "                if len(feat.size()) == 1:\n",
    "                    aggregate_feats.append(feat.view(1, -1))\n",
    "                else:\n",
    "                    aggregate_feats.append(torch.max(feat,0)[0].view(1, -1))\n",
    "            aggregate_feats = torch.cat(aggregate_feats, 0)\n",
    "\n",
    "        # self.dc.logger.info('6')\n",
    "\n",
    "        return aggregate_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型二：全连接层\n",
    "    第三层：[2708, 128] -> [2708, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classification(nn.Module):\n",
    "    def __init__(self, emb_size, num_classes):\n",
    "        super(Classification, self).__init__()\n",
    "\n",
    "        #self.weight = nn.Parameter(torch.FloatTensor(emb_size, num_classes)) 最终的输出 (128, num_classes)\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(emb_size, num_classes)\n",
    "            #nn.ReLU()\n",
    "        )\n",
    "        self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "        for param in self.parameters():\n",
    "            if len(param.size()) == 2:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "\n",
    "    def forward(self, embeds):\n",
    "        logists = torch.log_softmax(self.layer(embeds), 1)\n",
    "        return logists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 初始化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphSage = GraphSage(config['setting.num_layers'], features.size(1), config['setting.hidden_emb_size'], features, getattr(dataCenter, ds+'_adj_lists'), device, gcn=args.gcn, agg_func=args.agg_func)\n",
    "graphSage = graphSage.to(device)\n",
    "\n",
    "num_labels = len(set(getattr(dataCenter, ds + '_labels')))  # label的数量\n",
    "classification = Classification(config['setting.hidden_emb_size'], num_labels).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正负采样函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnsupervisedLoss(object):\n",
    "    \"\"\"docstring for UnsupervisedLoss\"\"\"\n",
    "    def __init__(self, adj_lists, train_nodes, device):\n",
    "        super(UnsupervisedLoss, self).__init__()\n",
    "        self.Q = 10\n",
    "        self.N_WALKS = 6\n",
    "        self.WALK_LEN = 1\n",
    "        self.N_WALK_LEN = 5\n",
    "        self.MARGIN = 3\n",
    "        self.adj_lists = adj_lists\n",
    "        self.train_nodes = train_nodes\n",
    "        self.device = device\n",
    "\n",
    "        self.target_nodes = None\n",
    "        self.positive_pairs = []\n",
    "        self.negtive_pairs = []\n",
    "        self.node_positive_pairs = {}\n",
    "        self.node_negtive_pairs = {}\n",
    "        self.unique_nodes_batch = []\n",
    "\n",
    "    def get_loss_sage(self, embeddings, nodes):\n",
    "        assert len(embeddings) == len(self.unique_nodes_batch)\n",
    "        assert False not in [nodes[i]==self.unique_nodes_batch[i] for i in range(len(nodes))]\n",
    "        node2index = {n:i for i,n in enumerate(self.unique_nodes_batch)}\n",
    "\n",
    "        nodes_score = []\n",
    "        assert len(self.node_positive_pairs) == len(self.node_negtive_pairs)\n",
    "        for node in self.node_positive_pairs:\n",
    "            pps = self.node_positive_pairs[node]\n",
    "            nps = self.node_negtive_pairs[node]\n",
    "            if len(pps) == 0 or len(nps) == 0:\n",
    "                continue\n",
    "\n",
    "            # Q * Exception(negative score)\n",
    "            indexs = [list(x) for x in zip(*nps)]\n",
    "            node_indexs = [node2index[x] for x in indexs[0]]\n",
    "            neighb_indexs = [node2index[x] for x in indexs[1]]\n",
    "            neg_score = F.cosine_similarity(embeddings[node_indexs], embeddings[neighb_indexs])\n",
    "            neg_score = self.Q*torch.mean(torch.log(torch.sigmoid(-neg_score)), 0)\n",
    "            # print(neg_score)\n",
    "\n",
    "            # multiple positive score\n",
    "            indexs = [list(x) for x in zip(*pps)]\n",
    "            node_indexs = [node2index[x] for x in indexs[0]]\n",
    "            neighb_indexs = [node2index[x] for x in indexs[1]]\n",
    "            pos_score = F.cosine_similarity(embeddings[node_indexs], embeddings[neighb_indexs])\n",
    "            pos_score = torch.log(torch.sigmoid(pos_score))\n",
    "            # print(pos_score)\n",
    "\n",
    "            nodes_score.append(torch.mean(- pos_score - neg_score).view(1,-1))\n",
    "                \n",
    "        loss = torch.mean(torch.cat(nodes_score, 0))\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def get_loss_margin(self, embeddings, nodes):\n",
    "        assert len(embeddings) == len(self.unique_nodes_batch)\n",
    "        assert False not in [nodes[i]==self.unique_nodes_batch[i] for i in range(len(nodes))]\n",
    "        node2index = {n:i for i,n in enumerate(self.unique_nodes_batch)}\n",
    "\n",
    "        nodes_score = []\n",
    "        assert len(self.node_positive_pairs) == len(self.node_negtive_pairs)\n",
    "        for node in self.node_positive_pairs:\n",
    "            pps = self.node_positive_pairs[node]\n",
    "            nps = self.node_negtive_pairs[node]\n",
    "            if len(pps) == 0 or len(nps) == 0:\n",
    "                continue\n",
    "\n",
    "            indexs = [list(x) for x in zip(*pps)]\n",
    "            node_indexs = [node2index[x] for x in indexs[0]]\n",
    "            neighb_indexs = [node2index[x] for x in indexs[1]]\n",
    "            pos_score = F.cosine_similarity(embeddings[node_indexs], embeddings[neighb_indexs])\n",
    "            pos_score, _ = torch.min(torch.log(torch.sigmoid(pos_score)), 0)\n",
    "\n",
    "            indexs = [list(x) for x in zip(*nps)]\n",
    "            node_indexs = [node2index[x] for x in indexs[0]]\n",
    "            neighb_indexs = [node2index[x] for x in indexs[1]]\n",
    "            neg_score = F.cosine_similarity(embeddings[node_indexs], embeddings[neighb_indexs])\n",
    "            neg_score, _ = torch.max(torch.log(torch.sigmoid(neg_score)), 0)\n",
    "\n",
    "            nodes_score.append(torch.max(torch.tensor(0.0).to(self.device), neg_score-pos_score+self.MARGIN).view(1,-1))\n",
    "            # nodes_score.append((-pos_score - neg_score).view(1,-1))\n",
    "\n",
    "        loss = torch.mean(torch.cat(nodes_score, 0),0)\n",
    "\n",
    "        # loss = -torch.log(torch.sigmoid(pos_score))-4*torch.log(torch.sigmoid(-neg_score))\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "    def extend_nodes(self, nodes, num_neg=6):\n",
    "        self.positive_pairs = []\n",
    "        self.node_positive_pairs = {}\n",
    "        self.negtive_pairs = []\n",
    "        self.node_negtive_pairs = {}\n",
    "\n",
    "        self.target_nodes = nodes\n",
    "        self.get_positive_nodes(nodes)\n",
    "        # print(self.positive_pairs)\n",
    "        self.get_negtive_nodes(nodes, num_neg)\n",
    "        # print(self.negtive_pairs)\n",
    "        self.unique_nodes_batch = list(set([i for x in self.positive_pairs for i in x]) | set([i for x in self.negtive_pairs for i in x]))\n",
    "        assert set(self.target_nodes) < set(self.unique_nodes_batch)\n",
    "        return self.unique_nodes_batch\n",
    "\n",
    "    def get_positive_nodes(self, nodes):\n",
    "        return self._run_random_walks(nodes)\n",
    "\n",
    "    def get_negtive_nodes(self, nodes, num_neg):\n",
    "        for node in nodes:\n",
    "            neighbors = set([node])\n",
    "            frontier = set([node])\n",
    "            for i in range(self.N_WALK_LEN):\n",
    "                current = set()\n",
    "                for outer in frontier:\n",
    "                    current |= self.adj_lists[int(outer)]\n",
    "                frontier = current - neighbors\n",
    "                neighbors |= current\n",
    "            far_nodes = set(self.train_nodes) - neighbors\n",
    "            neg_samples = random.sample(far_nodes, num_neg) if num_neg < len(far_nodes) else far_nodes\n",
    "            self.negtive_pairs.extend([(node, neg_node) for neg_node in neg_samples])\n",
    "            self.node_negtive_pairs[node] = [(node, neg_node) for neg_node in neg_samples]\n",
    "        return self.negtive_pairs\n",
    "\n",
    "    def _run_random_walks(self, nodes):\n",
    "        for node in nodes:\n",
    "            if len(self.adj_lists[int(node)]) == 0:\n",
    "                continue\n",
    "            cur_pairs = []\n",
    "            for i in range(self.N_WALKS):\n",
    "                curr_node = node\n",
    "                for j in range(self.WALK_LEN):\n",
    "                    neighs = self.adj_lists[int(curr_node)]\n",
    "                    next_node = random.choice(list(neighs))\n",
    "                    # self co-occurrences are useless\n",
    "                    if next_node != node and next_node in self.train_nodes:\n",
    "                        self.positive_pairs.append((node,next_node))\n",
    "                        cur_pairs.append((node,next_node))\n",
    "                    curr_node = next_node\n",
    "            \n",
    "            self.node_positive_pairs[node] = cur_pairs\n",
    "        return self.positive_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 初始化正负采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsupervised_loss = UnsupervisedLoss(getattr(dataCenter, ds + '_adj_lists'), getattr(dataCenter, ds + '_train'), device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GraphSage层特征编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gnn_embeddings(gnn_model, dataCenter, ds):\n",
    "    print('Loading embeddings from trained GraphSAGE model.')\n",
    "    features = np.zeros((len(getattr(dataCenter, ds + '_labels')), gnn_model.out_size))\n",
    "    nodes = np.arange(len(getattr(dataCenter, ds+'_labels'))).tolist()\n",
    "    b_sz = 500\n",
    "    batches = math.ceil(len(nodes) / b_sz)\n",
    "    embs = []\n",
    "    for index in range(batches):\n",
    "        nodes_batch = nodes[index * b_sz: (index + 1) * b_sz]\n",
    "        embs_batch = gnn_model(nodes_batch)\n",
    "        assert len(embs_batch) == len(nodes_batch)\n",
    "        embs.append(embs_batch)\n",
    "        # if ((index+1)*b_sz) % 10000 == 0:\n",
    "        #     print(f'Dealed Nodes [{(index+1)*b_sz}/{len(nodes)}]')\n",
    "\n",
    "    assert len(embs) == batches\n",
    "    embs = torch.cat(embs, 0)\n",
    "    assert len(embs) == len(nodes)\n",
    "    print('Embeddings loaded.')\n",
    "    return embs.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embeddings from trained GraphSAGE model.\n",
      "Embeddings loaded.\n",
      "tensor([[0.0052, 0.0000, 0.1288,  ..., 0.1129, 0.0452, 0.0432],\n",
      "        [0.0000, 0.0000, 0.0713,  ..., 0.0618, 0.0213, 0.0733],\n",
      "        [0.0000, 0.0000, 0.0330,  ..., 0.0000, 0.0866, 0.0316],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.3030,  ..., 0.0000, 0.0195, 0.1570],\n",
      "        [0.0000, 0.0000, 0.2029,  ..., 0.0070, 0.0190, 0.0342],\n",
      "        [0.0151, 0.0000, 0.1297,  ..., 0.0725, 0.0000, 0.0512]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# 测试embedding效果\n",
    "features = get_gnn_embeddings(graphSage, dataCenter, ds)\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一个Epoch的模型训练，生成：\n",
    "- graphSage模型：对节点进行编码\n",
    "- classification模型：对节点进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def apply_model(dataCenter, ds, graphSage, classification, unsupervised_loss, b_sz, unsup_loss, device, learn_method):\n",
    "    test_nodes = getattr(dataCenter, ds + '_test')\n",
    "    val_nodes = getattr(dataCenter, ds + '_val')\n",
    "    train_nodes = getattr(dataCenter, ds + '_train')\n",
    "    labels = getattr(dataCenter, ds + '_labels')\n",
    "\n",
    "    if unsup_loss == 'margin':\n",
    "        num_neg = 6\n",
    "    elif unsup_loss == 'normal':\n",
    "        num_neg = 100\n",
    "    else:\n",
    "        print(\"unsup_loss can be only 'margin' or 'normal'.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    train_nodes = shuffle(train_nodes)\n",
    "\n",
    "    models = [graphSage, classification]\n",
    "    params = []\n",
    "    for model in models:\n",
    "        for param in model.parameters():\n",
    "            if param.requires_grad:\n",
    "                params.append(param)\n",
    "\n",
    "    optimizer = torch.optim.SGD(params, lr=0.7)\n",
    "    optimizer.zero_grad()\n",
    "    for model in models:\n",
    "        model.zero_grad()\n",
    "\n",
    "    batches = math.ceil(len(train_nodes) / b_sz)\n",
    "\n",
    "    visited_nodes = set()\n",
    "    for index in range(batches):\n",
    "        nodes_batch = train_nodes[index * b_sz: (index + 1) * b_sz]  # batch训练的节点\n",
    "        \n",
    "        # print(nodes_batch.shape)\n",
    "        # print(nodes_batch)\n",
    "\n",
    "        # extend nodes batch for unspervised learning\n",
    "        # no conflicts with supervised learning\n",
    "        nodes_batch = np.asarray(list(unsupervised_loss.extend_nodes(nodes_batch, num_neg=num_neg)))\n",
    "        visited_nodes |= set(nodes_batch)\n",
    "\n",
    "        # get ground-truth for the nodes batch\n",
    "        labels_batch = labels[nodes_batch]\n",
    "\n",
    "        # feed nodes batch to the graphSAGE\n",
    "        # returning the nodes embeddings。 得到GraphSAGE后的ebmedding向量\n",
    "        embs_batch = graphSage(nodes_batch)  # 跳到models的GraphSge\n",
    "\n",
    "        if learn_method == 'sup':\n",
    "            # superivsed learning\n",
    "            logists = classification(embs_batch)\n",
    "            loss_sup = -torch.sum(logists[range(logists.size(0)), labels_batch], 0)\n",
    "            loss_sup /= len(nodes_batch)\n",
    "            loss = loss_sup\n",
    "        elif learn_method == 'plus_unsup':\n",
    "            # superivsed learning\n",
    "            logists = classification(embs_batch)\n",
    "            loss_sup = -torch.sum(logists[range(logists.size(0)), labels_batch], 0)\n",
    "            loss_sup /= len(nodes_batch)\n",
    "            # unsuperivsed learning\n",
    "            if unsup_loss == 'margin':\n",
    "                loss_net = unsupervised_loss.get_loss_margin(embs_batch, nodes_batch)\n",
    "            elif unsup_loss == 'normal':\n",
    "                loss_net = unsupervised_loss.get_loss_sage(embs_batch, nodes_batch)\n",
    "            loss = loss_sup + loss_net\n",
    "        else:\n",
    "            if unsup_loss == 'margin':\n",
    "                loss_net = unsupervised_loss.get_loss_margin(embs_batch, nodes_batch)\n",
    "            elif unsup_loss == 'normal':\n",
    "                loss_net = unsupervised_loss.get_loss_sage(embs_batch, nodes_batch)\n",
    "            loss = loss_net\n",
    "\n",
    "        print('Step [{}/{}], Loss: {:.4f}, Dealed Nodes [{}/{}] '.format(index+1, batches, loss.item(), len(visited_nodes), len(train_nodes)))\n",
    "        loss.backward()\n",
    "        for model in models:\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 5)  # 梯度的二范数和不超过5（平方和开根号）\n",
    "        optimizer.step()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        for model in models:\n",
    "            model.zero_grad()\n",
    "\n",
    "    return graphSage, classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataCenter, ds, graphSage, classification, device, max_vali_f1, name, cur_epoch):\n",
    "    test_nodes = getattr(dataCenter, ds+'_test')\n",
    "    val_nodes = getattr(dataCenter, ds+'_val')\n",
    "    labels = getattr(dataCenter, ds+'_labels')\n",
    "\n",
    "    models = [graphSage, classification]\n",
    "\n",
    "    params = []\n",
    "    for model in models:\n",
    "        for param in model.parameters():\n",
    "            if param.requires_grad:\n",
    "                param.requires_grad = False\n",
    "                params.append(param)\n",
    "\n",
    "    embs = graphSage(val_nodes)\n",
    "    logists = classification(embs)\n",
    "    _, predicts = torch.max(logists, 1)\n",
    "    labels_val = labels[val_nodes]\n",
    "    assert len(labels_val) == len(predicts)\n",
    "    comps = zip(labels_val, predicts.data)\n",
    "\n",
    "    vali_f1 = f1_score(labels_val, predicts.cpu().data, average=\"micro\")\n",
    "    print(\"Validation F1:\", vali_f1)\n",
    "\n",
    "    if vali_f1 > max_vali_f1:\n",
    "        max_vali_f1 = vali_f1\n",
    "        embs = graphSage(test_nodes)\n",
    "        logists = classification(embs)\n",
    "        _, predicts = torch.max(logists, 1)\n",
    "        labels_test = labels[test_nodes]\n",
    "        assert len(labels_test) == len(predicts)\n",
    "        comps = zip(labels_test, predicts.data)\n",
    "\n",
    "        test_f1 = f1_score(labels_test, predicts.cpu().data, average=\"micro\")\n",
    "        print(\"Test F1:\", test_f1)\n",
    "\n",
    "        for param in params:\n",
    "            param.requires_grad = True\n",
    "\n",
    "        torch.save(models, './model_best_{}_ep{}_{:.4f}.torch'.format(name, cur_epoch, test_f1))\n",
    "\n",
    "    for param in params:\n",
    "        param.requires_grad = True\n",
    "\n",
    "    return max_vali_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多个Epoch进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------EPOCH 0-----------------------\n",
      "Step [1/68], Loss: 1.9446, Dealed Nodes [1018/1355] \n",
      "Step [2/68], Loss: 1.8598, Dealed Nodes [1263/1355] \n",
      "Step [3/68], Loss: 1.7914, Dealed Nodes [1325/1355] \n",
      "Step [4/68], Loss: 1.7095, Dealed Nodes [1344/1355] \n",
      "Step [5/68], Loss: 1.6209, Dealed Nodes [1353/1355] \n",
      "Step [6/68], Loss: 1.5517, Dealed Nodes [1355/1355] \n",
      "Step [7/68], Loss: 1.4243, Dealed Nodes [1355/1355] \n",
      "Step [8/68], Loss: 1.3042, Dealed Nodes [1355/1355] \n",
      "Step [9/68], Loss: 1.1833, Dealed Nodes [1355/1355] \n",
      "Step [10/68], Loss: 1.0458, Dealed Nodes [1355/1355] \n",
      "Step [11/68], Loss: 0.9221, Dealed Nodes [1355/1355] \n",
      "Step [12/68], Loss: 0.8258, Dealed Nodes [1355/1355] \n",
      "Step [13/68], Loss: 0.7263, Dealed Nodes [1355/1355] \n",
      "Step [14/68], Loss: 0.6574, Dealed Nodes [1355/1355] \n",
      "Step [15/68], Loss: 0.6323, Dealed Nodes [1355/1355] \n",
      "Step [16/68], Loss: 0.8439, Dealed Nodes [1355/1355] \n",
      "Step [17/68], Loss: 2.1830, Dealed Nodes [1355/1355] \n",
      "Step [18/68], Loss: 1.2149, Dealed Nodes [1355/1355] \n",
      "Step [19/68], Loss: 0.8208, Dealed Nodes [1355/1355] \n",
      "Step [20/68], Loss: 0.6701, Dealed Nodes [1355/1355] \n",
      "Step [21/68], Loss: 0.5391, Dealed Nodes [1355/1355] \n",
      "Step [22/68], Loss: 0.4565, Dealed Nodes [1355/1355] \n",
      "Step [23/68], Loss: 0.4121, Dealed Nodes [1355/1355] \n",
      "Step [24/68], Loss: 0.3599, Dealed Nodes [1355/1355] \n",
      "Step [25/68], Loss: 0.3601, Dealed Nodes [1355/1355] \n",
      "Step [26/68], Loss: 0.3265, Dealed Nodes [1355/1355] \n",
      "Step [27/68], Loss: 0.2944, Dealed Nodes [1355/1355] \n",
      "Step [28/68], Loss: 0.2844, Dealed Nodes [1355/1355] \n",
      "Step [29/68], Loss: 0.2879, Dealed Nodes [1355/1355] \n",
      "Step [30/68], Loss: 0.3210, Dealed Nodes [1355/1355] \n",
      "Step [31/68], Loss: 0.3606, Dealed Nodes [1355/1355] \n",
      "Step [32/68], Loss: 0.5826, Dealed Nodes [1355/1355] \n",
      "Step [33/68], Loss: 0.5431, Dealed Nodes [1355/1355] \n",
      "Step [34/68], Loss: 0.4118, Dealed Nodes [1355/1355] \n",
      "Step [35/68], Loss: 0.2012, Dealed Nodes [1355/1355] \n",
      "Step [36/68], Loss: 0.1904, Dealed Nodes [1355/1355] \n",
      "Step [37/68], Loss: 0.1697, Dealed Nodes [1355/1355] \n",
      "Step [38/68], Loss: 0.1608, Dealed Nodes [1355/1355] \n",
      "Step [39/68], Loss: 0.1431, Dealed Nodes [1355/1355] \n",
      "Step [40/68], Loss: 0.1468, Dealed Nodes [1355/1355] \n",
      "Step [41/68], Loss: 0.1306, Dealed Nodes [1355/1355] \n",
      "Step [42/68], Loss: 0.1231, Dealed Nodes [1355/1355] \n",
      "Step [43/68], Loss: 0.1183, Dealed Nodes [1355/1355] \n",
      "Step [44/68], Loss: 0.1052, Dealed Nodes [1355/1355] \n",
      "Step [45/68], Loss: 0.1201, Dealed Nodes [1355/1355] \n",
      "Step [46/68], Loss: 0.1091, Dealed Nodes [1355/1355] \n",
      "Step [47/68], Loss: 0.0943, Dealed Nodes [1355/1355] \n",
      "Step [48/68], Loss: 0.0854, Dealed Nodes [1355/1355] \n",
      "Step [49/68], Loss: 0.0837, Dealed Nodes [1355/1355] \n",
      "Step [50/68], Loss: 0.0875, Dealed Nodes [1355/1355] \n",
      "Step [51/68], Loss: 0.0839, Dealed Nodes [1355/1355] \n",
      "Step [52/68], Loss: 0.0814, Dealed Nodes [1355/1355] \n",
      "Step [53/68], Loss: 0.0819, Dealed Nodes [1355/1355] \n",
      "Step [54/68], Loss: 0.0731, Dealed Nodes [1355/1355] \n",
      "Step [55/68], Loss: 0.0622, Dealed Nodes [1355/1355] \n",
      "Step [56/68], Loss: 0.0696, Dealed Nodes [1355/1355] \n",
      "Step [57/68], Loss: 0.0595, Dealed Nodes [1355/1355] \n",
      "Step [58/68], Loss: 0.0655, Dealed Nodes [1355/1355] \n",
      "Step [59/68], Loss: 0.0570, Dealed Nodes [1355/1355] \n",
      "Step [60/68], Loss: 0.0573, Dealed Nodes [1355/1355] \n",
      "Step [61/68], Loss: 0.0539, Dealed Nodes [1355/1355] \n",
      "Step [62/68], Loss: 0.0534, Dealed Nodes [1355/1355] \n",
      "Step [63/68], Loss: 0.0523, Dealed Nodes [1355/1355] \n",
      "Step [64/68], Loss: 0.0424, Dealed Nodes [1355/1355] \n",
      "Step [65/68], Loss: 0.0434, Dealed Nodes [1355/1355] \n",
      "Step [66/68], Loss: 0.0458, Dealed Nodes [1355/1355] \n",
      "Step [67/68], Loss: 0.0413, Dealed Nodes [1355/1355] \n",
      "Step [68/68], Loss: 0.0369, Dealed Nodes [1355/1355] \n",
      "Validation F1: 0.893569844789357\n",
      "Test F1: 0.8725055432372505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\sss\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\serialization.py:250: UserWarning: Couldn't retrieve source code for container of type GraphSage. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "c:\\users\\sss\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\serialization.py:250: UserWarning: Couldn't retrieve source code for container of type SageLayer. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "c:\\users\\sss\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\serialization.py:250: UserWarning: Couldn't retrieve source code for container of type Classification. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------EPOCH 1-----------------------\n",
      "Step [1/68], Loss: 0.0433, Dealed Nodes [1039/1355] \n",
      "Step [2/68], Loss: 0.0375, Dealed Nodes [1262/1355] \n",
      "Step [3/68], Loss: 0.0345, Dealed Nodes [1323/1355] \n",
      "Step [4/68], Loss: 0.0379, Dealed Nodes [1338/1355] \n",
      "Step [5/68], Loss: 0.0325, Dealed Nodes [1348/1355] \n",
      "Step [6/68], Loss: 0.0335, Dealed Nodes [1350/1355] \n",
      "Step [7/68], Loss: 0.0334, Dealed Nodes [1351/1355] \n",
      "Step [8/68], Loss: 0.0323, Dealed Nodes [1354/1355] \n",
      "Step [9/68], Loss: 0.0310, Dealed Nodes [1355/1355] \n",
      "Step [10/68], Loss: 0.0272, Dealed Nodes [1355/1355] \n",
      "Step [11/68], Loss: 0.0243, Dealed Nodes [1355/1355] \n",
      "Step [12/68], Loss: 0.0278, Dealed Nodes [1355/1355] \n",
      "Step [13/68], Loss: 0.0256, Dealed Nodes [1355/1355] \n",
      "Step [14/68], Loss: 0.0274, Dealed Nodes [1355/1355] \n",
      "Step [15/68], Loss: 0.0244, Dealed Nodes [1355/1355] \n",
      "Step [16/68], Loss: 0.0262, Dealed Nodes [1355/1355] \n",
      "Step [17/68], Loss: 0.0236, Dealed Nodes [1355/1355] \n",
      "Step [18/68], Loss: 0.0252, Dealed Nodes [1355/1355] \n",
      "Step [19/68], Loss: 0.0247, Dealed Nodes [1355/1355] \n",
      "Step [20/68], Loss: 0.0214, Dealed Nodes [1355/1355] \n",
      "Step [21/68], Loss: 0.0208, Dealed Nodes [1355/1355] \n",
      "Step [22/68], Loss: 0.0192, Dealed Nodes [1355/1355] \n",
      "Step [23/68], Loss: 0.0193, Dealed Nodes [1355/1355] \n",
      "Step [24/68], Loss: 0.0190, Dealed Nodes [1355/1355] \n",
      "Step [25/68], Loss: 0.0207, Dealed Nodes [1355/1355] \n",
      "Step [26/68], Loss: 0.0181, Dealed Nodes [1355/1355] \n",
      "Step [27/68], Loss: 0.0197, Dealed Nodes [1355/1355] \n",
      "Step [28/68], Loss: 0.0195, Dealed Nodes [1355/1355] \n",
      "Step [29/68], Loss: 0.0190, Dealed Nodes [1355/1355] \n",
      "Step [30/68], Loss: 0.0188, Dealed Nodes [1355/1355] \n",
      "Step [31/68], Loss: 0.0171, Dealed Nodes [1355/1355] \n",
      "Step [32/68], Loss: 0.0162, Dealed Nodes [1355/1355] \n",
      "Step [33/68], Loss: 0.0159, Dealed Nodes [1355/1355] \n",
      "Step [34/68], Loss: 0.0172, Dealed Nodes [1355/1355] \n",
      "Step [35/68], Loss: 0.0170, Dealed Nodes [1355/1355] \n",
      "Step [36/68], Loss: 0.0166, Dealed Nodes [1355/1355] \n",
      "Step [37/68], Loss: 0.0147, Dealed Nodes [1355/1355] \n",
      "Step [38/68], Loss: 0.0155, Dealed Nodes [1355/1355] \n",
      "Step [39/68], Loss: 0.0145, Dealed Nodes [1355/1355] \n",
      "Step [40/68], Loss: 0.0154, Dealed Nodes [1355/1355] \n",
      "Step [41/68], Loss: 0.0143, Dealed Nodes [1355/1355] \n",
      "Step [42/68], Loss: 0.0148, Dealed Nodes [1355/1355] \n",
      "Step [43/68], Loss: 0.0148, Dealed Nodes [1355/1355] \n",
      "Step [44/68], Loss: 0.0127, Dealed Nodes [1355/1355] \n",
      "Step [45/68], Loss: 0.0123, Dealed Nodes [1355/1355] \n",
      "Step [46/68], Loss: 0.0124, Dealed Nodes [1355/1355] \n",
      "Step [47/68], Loss: 0.0127, Dealed Nodes [1355/1355] \n",
      "Step [48/68], Loss: 0.0130, Dealed Nodes [1355/1355] \n",
      "Step [49/68], Loss: 0.0113, Dealed Nodes [1355/1355] \n",
      "Step [50/68], Loss: 0.0133, Dealed Nodes [1355/1355] \n",
      "Step [51/68], Loss: 0.0118, Dealed Nodes [1355/1355] \n",
      "Step [52/68], Loss: 0.0120, Dealed Nodes [1355/1355] \n",
      "Step [53/68], Loss: 0.0115, Dealed Nodes [1355/1355] \n",
      "Step [54/68], Loss: 0.0116, Dealed Nodes [1355/1355] \n",
      "Step [55/68], Loss: 0.0108, Dealed Nodes [1355/1355] \n",
      "Step [56/68], Loss: 0.0112, Dealed Nodes [1355/1355] \n",
      "Step [57/68], Loss: 0.0100, Dealed Nodes [1355/1355] \n",
      "Step [58/68], Loss: 0.0106, Dealed Nodes [1355/1355] \n",
      "Step [59/68], Loss: 0.0103, Dealed Nodes [1355/1355] \n",
      "Step [60/68], Loss: 0.0103, Dealed Nodes [1355/1355] \n",
      "Step [61/68], Loss: 0.0102, Dealed Nodes [1355/1355] \n",
      "Step [62/68], Loss: 0.0098, Dealed Nodes [1355/1355] \n",
      "Step [63/68], Loss: 0.0098, Dealed Nodes [1355/1355] \n",
      "Step [64/68], Loss: 0.0094, Dealed Nodes [1355/1355] \n",
      "Step [65/68], Loss: 0.0085, Dealed Nodes [1355/1355] \n",
      "Step [66/68], Loss: 0.0094, Dealed Nodes [1355/1355] \n",
      "Step [67/68], Loss: 0.0091, Dealed Nodes [1355/1355] \n",
      "Step [68/68], Loss: 0.0081, Dealed Nodes [1355/1355] \n",
      "Validation F1: 0.8891352549889135\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.epochs):\n",
    "    print('----------------------EPOCH %d-----------------------' % epoch)\n",
    "    graphSage, classification = apply_model(dataCenter, ds, graphSage, classification, unsupervised_loss, args.b_sz, args.unsup_loss, device, args.learn_method)\n",
    "    \n",
    "    # 模型测试\n",
    "    if (epoch + 1) % 2 == 0 and args.learn_method == 'unsup':\n",
    "        classification, args.max_vali_f1 = train_classification(dataCenter, graphSage, classification, ds, device, args.max_vali_f1, args.name)\n",
    "    if args.learn_method != 'unsup':\n",
    "        args.max_vali_f1 = evaluate(dataCenter, ds, graphSage, classification, device, args.max_vali_f1, args.name, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
