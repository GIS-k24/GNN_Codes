{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import random\n",
    "import logging\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergeLayer(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, dim1, dim2, dim3, dim4):\n",
    "        super().__init__()\n",
    "        # self.layer_norm = torch.nn.LayerNorm(dim1 + dim2)\n",
    "        self.fc1 = torch.nn.Linear(dim1 + dim2, dim3)\n",
    "        self.fc2 = torch.nn.Linear(dim3, dim4)\n",
    "        self.act = torch.nn.ReLU()\n",
    "\n",
    "        torch.nn.init.xavier_normal_(self.fc1.weight)\n",
    "        torch.nn.init.xavier_normal_(self.fc2.weight)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        # x = self.layer_norm(x)\n",
    "        h = self.act(self.fc1(x))\n",
    "        return self.fc2(h)\n",
    "\n",
    "\n",
    "class ScaledDotProductAttention(torch.nn.Module):\n",
    "    \"\"\" Scaled Dot-Product Attention \"\"\"\n",
    "\n",
    "    def __init__(self, temperature, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.dropout = torch.nn.Dropout(attn_dropout)\n",
    "        self.softmax = torch.nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        attn = torch.bmm(q, k.transpose(1, 2))\n",
    "        attn = attn / self.temperature\n",
    "\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask, -1e10)\n",
    "\n",
    "        attn = self.softmax(attn)  # [n * b, l_q, l_k]\n",
    "        attn = self.dropout(attn)  # [n * b, l_v, d]\n",
    "\n",
    "        output = torch.bmm(attn, v)\n",
    "\n",
    "        return output, attn\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" Multi-Head Attention module \"\"\"\n",
    "\n",
    "    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "\n",
    "        self.w_qs = nn.Linear(d_model, n_head * d_k, bias=False)\n",
    "        self.w_ks = nn.Linear(d_model, n_head * d_k, bias=False)\n",
    "        self.w_vs = nn.Linear(d_model, n_head * d_v, bias=False)\n",
    "        nn.init.normal_(self.w_qs.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_k)))\n",
    "        nn.init.normal_(self.w_ks.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_k)))\n",
    "        nn.init.normal_(self.w_vs.weight, mean=0, std=np.sqrt(2.0 / (d_model + d_v)))\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(temperature=np.power(d_k, 0.5), attn_dropout=dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.fc = nn.Linear(n_head * d_v, d_model)\n",
    "\n",
    "        nn.init.xavier_normal_(self.fc.weight)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head\n",
    "\n",
    "        sz_b, len_q, _ = q.size()\n",
    "        sz_b, len_k, _ = k.size()\n",
    "        sz_b, len_v, _ = v.size()\n",
    "\n",
    "        residual = q\n",
    "\n",
    "        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)\n",
    "        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)\n",
    "        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)\n",
    "\n",
    "        q = q.permute(2, 0, 1, 3).contiguous().view(-1, len_q, d_k)  # (n*b) x lq x dk\n",
    "        k = k.permute(2, 0, 1, 3).contiguous().view(-1, len_k, d_k)  # (n*b) x lk x dk\n",
    "        v = v.permute(2, 0, 1, 3).contiguous().view(-1, len_v, d_v)  # (n*b) x lv x dv\n",
    "\n",
    "        mask = mask.repeat(n_head, 1, 1)  # (n*b) x .. x ..\n",
    "        output, attn = self.attention(q, k, v, mask=mask)\n",
    "\n",
    "        output = output.view(n_head, sz_b, len_q, d_v)\n",
    "\n",
    "        output = output.permute(1, 2, 0, 3).contiguous().view(sz_b, len_q, -1)  # b x lq x (n*dv)\n",
    "\n",
    "        output = self.dropout(self.fc(output))\n",
    "        output = self.layer_norm(output + residual)\n",
    "        # output = self.layer_norm(output)\n",
    "\n",
    "        return output, attn\n",
    "\n",
    "\n",
    "class MapBasedMultiHeadAttention(nn.Module):\n",
    "    \"\"\" Multi-Head Attention module \"\"\"\n",
    "\n",
    "    def __init__(self, n_head, d_model, d_k, d_v, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_head = n_head\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "\n",
    "        self.wq_node_transform = nn.Linear(d_model, n_head * d_k, bias=False)\n",
    "        self.wk_node_transform = nn.Linear(d_model, n_head * d_k, bias=False)\n",
    "        self.wv_node_transform = nn.Linear(d_model, n_head * d_k, bias=False)\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.fc = nn.Linear(n_head * d_v, d_model)\n",
    "\n",
    "        self.act = nn.LeakyReLU(negative_slope=0.2)\n",
    "        self.weight_map = nn.Linear(2 * d_k, 1, bias=False)\n",
    "\n",
    "        nn.init.xavier_normal_(self.fc.weight)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.softmax = torch.nn.Softmax(dim=2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        d_k, d_v, n_head = self.d_k, self.d_v, self.n_head\n",
    "\n",
    "        sz_b, len_q, _ = q.size()\n",
    "\n",
    "        sz_b, len_k, _ = k.size()\n",
    "        sz_b, len_v, _ = v.size()\n",
    "\n",
    "        residual = q\n",
    "\n",
    "        q = self.wq_node_transform(q).view(sz_b, len_q, n_head, d_k)\n",
    "\n",
    "        k = self.wk_node_transform(k).view(sz_b, len_k, n_head, d_k)\n",
    "\n",
    "        v = self.wv_node_transform(v).view(sz_b, len_v, n_head, d_v)\n",
    "\n",
    "        q = q.permute(2, 0, 1, 3).contiguous().view(-1, len_q, d_k)  # (n*b) x lq x dk\n",
    "        q = torch.unsqueeze(q, dim=2)  # [(n*b), lq, 1, dk]\n",
    "        q = q.expand(q.shape[0], q.shape[1], len_k, q.shape[3])  # [(n*b), lq, lk, dk]\n",
    "\n",
    "        k = k.permute(2, 0, 1, 3).contiguous().view(-1, len_k, d_k)  # (n*b) x lk x dk\n",
    "        k = torch.unsqueeze(k, dim=1)  # [(n*b), 1, lk, dk]\n",
    "        k = k.expand(k.shape[0], len_q, k.shape[2], k.shape[3])  # [(n*b), lq, lk, dk]\n",
    "\n",
    "        v = v.permute(2, 0, 1, 3).contiguous().view(-1, len_v, d_v)  # (n*b) x lv x dv\n",
    "\n",
    "        mask = mask.repeat(n_head, 1, 1)  # (n*b) x lq x lk\n",
    "\n",
    "        # Map based Attention\n",
    "        # output, attn = self.attention(q, k, v, mask=mask)\n",
    "        q_k = torch.cat([q, k], dim=3)  # [(n*b), lq, lk, dk * 2]\n",
    "        attn = self.weight_map(q_k).squeeze(dim=3)  # [(n*b), lq, lk]\n",
    "\n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask, -1e10)\n",
    "\n",
    "        attn = self.softmax(attn)  # [n * b, l_q, l_k]\n",
    "        attn = self.dropout(attn)  # [n * b, l_q, l_k]\n",
    "\n",
    "        # [n * b, l_q, l_k] * [n * b, l_v, d_v] >> [n * b, l_q, d_v]\n",
    "        output = torch.bmm(attn, v)\n",
    "\n",
    "        output = output.view(n_head, sz_b, len_q, d_v)\n",
    "\n",
    "        output = output.permute(1, 2, 0, 3).contiguous().view(sz_b, len_q, -1)  # b x lq x (n*dv)\n",
    "\n",
    "        output = self.dropout(self.act(self.fc(output)))\n",
    "        output = self.layer_norm(output + residual)\n",
    "\n",
    "        return output, attn\n",
    "\n",
    "\n",
    "def expand_last_dim(x, num):\n",
    "    view_size = list(x.size()) + [1]\n",
    "    expand_size = list(x.size()) + [num]\n",
    "    return x.view(view_size).expand(expand_size)\n",
    "\n",
    "\n",
    "class TimeEncode(torch.nn.Module):\n",
    "    def __init__(self, expand_dim, factor=5):\n",
    "        super(TimeEncode, self).__init__()\n",
    "        # init_len = np.array([1e8**(i/(time_dim-1)) for i in range(time_dim)])\n",
    "\n",
    "        time_dim = expand_dim\n",
    "        self.factor = factor\n",
    "        self.basis_freq = torch.nn.Parameter((torch.from_numpy(1 / 10 ** np.linspace(0, 9, time_dim))).float())\n",
    "        self.phase = torch.nn.Parameter(torch.zeros(time_dim).float())\n",
    "\n",
    "        # self.dense = torch.nn.Linear(time_dim, expand_dim, bias=False)\n",
    "\n",
    "        # torch.nn.init.xavier_normal_(self.dense.weight)\n",
    "\n",
    "    def forward(self, ts):\n",
    "        # ts: [N, L]\n",
    "        batch_size = ts.size(0)\n",
    "        seq_len = ts.size(1)\n",
    "\n",
    "        ts = ts.view(batch_size, seq_len, 1)  # [N, L, 1]\n",
    "        map_ts = ts * self.basis_freq.view(1, 1, -1)  # [N, L, time_dim]\n",
    "        map_ts += self.phase.view(1, 1, -1)\n",
    "\n",
    "        harmonic = torch.cos(map_ts)\n",
    "\n",
    "        return harmonic  # self.dense(harmonic)\n",
    "\n",
    "\n",
    "class PosEncode(torch.nn.Module):\n",
    "    def __init__(self, expand_dim, seq_len):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pos_embeddings = nn.Embedding(num_embeddings=seq_len, embedding_dim=expand_dim)\n",
    "\n",
    "    def forward(self, ts):\n",
    "        # ts: [N, L]\n",
    "        order = ts.argsort()\n",
    "        ts_emb = self.pos_embeddings(order)\n",
    "        return ts_emb\n",
    "\n",
    "\n",
    "class EmptyEncode(torch.nn.Module):\n",
    "    def __init__(self, expand_dim):\n",
    "        super().__init__()\n",
    "        self.expand_dim = expand_dim\n",
    "\n",
    "    def forward(self, ts):\n",
    "        out = torch.zeros_like(ts).float()\n",
    "        out = torch.unsqueeze(out, dim=-1)\n",
    "        out = out.expand(out.shape[0], out.shape[1], self.expand_dim)\n",
    "        return out\n",
    "\n",
    "\n",
    "class LSTMPool(torch.nn.Module):\n",
    "    def __init__(self, feat_dim, edge_dim, time_dim):\n",
    "        super(LSTMPool, self).__init__()\n",
    "        self.feat_dim = feat_dim\n",
    "        self.time_dim = time_dim\n",
    "        self.edge_dim = edge_dim\n",
    "\n",
    "        self.att_dim = feat_dim + edge_dim + time_dim\n",
    "\n",
    "        self.act = torch.nn.ReLU()\n",
    "\n",
    "        self.lstm = torch.nn.LSTM(input_size=self.att_dim,\n",
    "                                  hidden_size=self.feat_dim,\n",
    "                                  num_layers=1,\n",
    "                                  batch_first=True)\n",
    "        self.merger = MergeLayer(feat_dim, feat_dim, feat_dim, feat_dim)\n",
    "\n",
    "    def forward(self, src, src_t, seq, seq_t, seq_e, mask):\n",
    "        # seq [B, N, D]\n",
    "        # mask [B, N]\n",
    "        seq_x = torch.cat([seq, seq_e, seq_t], dim=2)\n",
    "\n",
    "        _, (hn, _) = self.lstm(seq_x)\n",
    "\n",
    "        hn = hn[-1, :, :]  # hn.squeeze(dim=0)\n",
    "\n",
    "        out = self.merger.forward(hn, src)\n",
    "        return out, None\n",
    "\n",
    "\n",
    "class MeanPool(torch.nn.Module):\n",
    "    def __init__(self, feat_dim, edge_dim):\n",
    "        super(MeanPool, self).__init__()\n",
    "        self.edge_dim = edge_dim\n",
    "        self.feat_dim = feat_dim\n",
    "        self.act = torch.nn.ReLU()\n",
    "        self.merger = MergeLayer(edge_dim + feat_dim, feat_dim, feat_dim, feat_dim)\n",
    "\n",
    "    def forward(self, src, src_t, seq, seq_t, seq_e, mask):\n",
    "        # seq [B, N, D]\n",
    "        # mask [B, N]\n",
    "        src_x = src\n",
    "        seq_x = torch.cat([seq, seq_e], dim=2)  # [B, N, De + D]\n",
    "        hn = seq_x.mean(dim=1)  # [B, De + D]\n",
    "        output = self.merger(hn, src_x)\n",
    "        return output, None\n",
    "\n",
    "\n",
    "class AttnModel(torch.nn.Module):\n",
    "    \"\"\"Attention based temporal layers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feat_dim, edge_dim, time_dim,\n",
    "                 attn_mode='prod', n_head=2, drop_out=0.1):\n",
    "        \"\"\"\n",
    "        args:\n",
    "          feat_dim: dim for the node features\n",
    "          edge_dim: dim for the temporal edge features\n",
    "          time_dim: dim for the time encoding\n",
    "          attn_mode: choose from 'prod' and 'map'\n",
    "          n_head: number of heads in attention\n",
    "          drop_out: probability of dropping a neural.\n",
    "        \"\"\"\n",
    "        super(AttnModel, self).__init__()\n",
    "\n",
    "        self.feat_dim = feat_dim\n",
    "        self.time_dim = time_dim\n",
    "\n",
    "        self.edge_in_dim = (feat_dim + edge_dim + time_dim)\n",
    "        self.model_dim = self.edge_in_dim\n",
    "        # self.edge_fc = torch.nn.Linear(self.edge_in_dim, self.feat_dim, bias=False)\n",
    "\n",
    "        self.merger = MergeLayer(self.model_dim, feat_dim, feat_dim, feat_dim)\n",
    "\n",
    "        # self.act = torch.nn.ReLU()\n",
    "\n",
    "        assert (self.model_dim % n_head == 0)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.attn_mode = attn_mode\n",
    "\n",
    "        if attn_mode == 'prod':\n",
    "            self.multi_head_target = MultiHeadAttention(n_head,\n",
    "                                                        d_model=self.model_dim,\n",
    "                                                        d_k=self.model_dim // n_head,\n",
    "                                                        d_v=self.model_dim // n_head,\n",
    "                                                        dropout=drop_out)\n",
    "            self.logger.info('Using scaled prod attention')\n",
    "\n",
    "        elif attn_mode == 'map':\n",
    "            self.multi_head_target = MapBasedMultiHeadAttention(n_head,\n",
    "                                                                d_model=self.model_dim,\n",
    "                                                                d_k=self.model_dim // n_head,\n",
    "                                                                d_v=self.model_dim // n_head,\n",
    "                                                                dropout=drop_out)\n",
    "            self.logger.info('Using map based attention')\n",
    "        else:\n",
    "            raise ValueError('attn_mode can only be prod or map')\n",
    "\n",
    "    def forward(self, src, src_t, seq, seq_t, seq_e, mask):\n",
    "        \"\"\"\"Attention based temporal attention forward pass\n",
    "        args:\n",
    "          src: float Tensor of shape [B, D]\n",
    "          src_t: float Tensor of shape [B, Dt], Dt == D\n",
    "          seq: float Tensor of shape [B, N, D]\n",
    "          seq_t: float Tensor of shape [B, N, Dt]\n",
    "          seq_e: float Tensor of shape [B, N, De], De == D\n",
    "          mask: boolean Tensor of shape [B, N], where the true value indicate a null value in the sequence.\n",
    "\n",
    "        returns:\n",
    "          output, weight\n",
    "\n",
    "          output: float Tensor of shape [B, D]\n",
    "          weight: float Tensor of shape [B, N]\n",
    "        \"\"\"\n",
    "\n",
    "        src_ext = torch.unsqueeze(src, dim=1)  # src [B, 1, D]\n",
    "        src_e_ph = torch.zeros_like(src_ext)\n",
    "        q = torch.cat([src_ext, src_e_ph, src_t], dim=2)  # [B, 1, D + De + Dt] -> [B, 1, D]\n",
    "        k = torch.cat([seq, seq_e, seq_t], dim=2)  # [B, 1, D + De + Dt] -> [B, 1, D]\n",
    "\n",
    "        mask = torch.unsqueeze(mask, dim=2)  # mask [B, N, 1]\n",
    "        mask = mask.permute([0, 2, 1])  # mask [B, 1, N]\n",
    "\n",
    "        # # target-attention\n",
    "        output, attn = self.multi_head_target(q=q, k=k, v=k, mask=mask)  # output: [B, 1, D + Dt], attn: [B, 1, N]\n",
    "        output = output.squeeze()\n",
    "        attn = attn.squeeze()\n",
    "\n",
    "        output = self.merger(output, src)\n",
    "        return output, attn\n",
    "\n",
    "\n",
    "class TGAN(torch.nn.Module):\n",
    "    def __init__(self, ngh_finder, n_feat, e_feat,\n",
    "                 attn_mode='prod', use_time='time', agg_method='attn', node_dim=None, time_dim=None,\n",
    "                 num_layers=3, n_head=4, null_idx=0, num_heads=1, drop_out=0.1, seq_len=None):\n",
    "        super(TGAN, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.ngh_finder = ngh_finder\n",
    "        self.null_idx = null_idx\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.n_feat_th = torch.nn.Parameter(torch.from_numpy(n_feat.astype(np.float32)))\n",
    "        self.e_feat_th = torch.nn.Parameter(torch.from_numpy(e_feat.astype(np.float32)))\n",
    "        self.edge_raw_embed = torch.nn.Embedding.from_pretrained(self.e_feat_th, padding_idx=0, freeze=True)\n",
    "        self.node_raw_embed = torch.nn.Embedding.from_pretrained(self.n_feat_th, padding_idx=0, freeze=True)\n",
    "\n",
    "        self.feat_dim = self.n_feat_th.shape[1]\n",
    "\n",
    "        self.n_feat_dim = self.feat_dim\n",
    "        self.e_feat_dim = self.feat_dim\n",
    "        self.model_dim = self.feat_dim\n",
    "\n",
    "        self.use_time = use_time\n",
    "        self.merge_layer = MergeLayer(self.feat_dim, self.feat_dim, self.feat_dim, self.feat_dim)\n",
    "\n",
    "        if agg_method == 'attn':\n",
    "            self.logger.info('Aggregation uses attention model')\n",
    "            self.attn_model_list = torch.nn.ModuleList([AttnModel(self.feat_dim,\n",
    "                                                                  self.feat_dim,\n",
    "                                                                  self.feat_dim,\n",
    "                                                                  attn_mode=attn_mode,\n",
    "                                                                  n_head=n_head,\n",
    "                                                                  drop_out=drop_out) for _ in range(num_layers)])\n",
    "        elif agg_method == 'lstm':\n",
    "            self.logger.info('Aggregation uses LSTM model')\n",
    "            self.attn_model_list = torch.nn.ModuleList([LSTMPool(self.feat_dim,\n",
    "                                                                 self.feat_dim,\n",
    "                                                                 self.feat_dim) for _ in range(num_layers)])\n",
    "        elif agg_method == 'mean':\n",
    "            self.logger.info('Aggregation uses constant mean model')\n",
    "            self.attn_model_list = torch.nn.ModuleList([MeanPool(self.feat_dim,\n",
    "                                                                 self.feat_dim) for _ in range(num_layers)])\n",
    "        else:\n",
    "\n",
    "            raise ValueError('invalid agg_method value, use attn or lstm')\n",
    "\n",
    "        if use_time == 'time':\n",
    "            self.logger.info('Using time encoding')\n",
    "            self.time_encoder = TimeEncode(expand_dim=self.n_feat_th.shape[1])\n",
    "        elif use_time == 'pos':\n",
    "            assert (seq_len is not None)\n",
    "            self.logger.info('Using positional encoding')\n",
    "            self.time_encoder = PosEncode(expand_dim=self.n_feat_th.shape[1], seq_len=seq_len)\n",
    "        elif use_time == 'empty':\n",
    "            self.logger.info('Using empty encoding')\n",
    "            self.time_encoder = EmptyEncode(expand_dim=self.n_feat_th.shape[1])\n",
    "        else:\n",
    "            raise ValueError('invalid time option!')\n",
    "\n",
    "        self.affinity_score = MergeLayer(self.feat_dim, self.feat_dim, self.feat_dim,\n",
    "                                         1)  # torch.nn.Bilinear(self.feat_dim, self.feat_dim, 1, bias=True)\n",
    "\n",
    "    def forward(self, src_idx_l, target_idx_l, cut_time_l, num_neighbors=20):\n",
    "\n",
    "        src_embed = self.tem_conv(src_idx_l, cut_time_l, self.num_layers, num_neighbors)\n",
    "        target_embed = self.tem_conv(target_idx_l, cut_time_l, self.num_layers, num_neighbors)\n",
    "\n",
    "        score = self.affinity_score(src_embed, target_embed).squeeze(dim=-1)\n",
    "\n",
    "        return score\n",
    "\n",
    "    def contrast(self, src_idx_l, target_idx_l, background_idx_l, cut_time_l, num_neighbors=20):\n",
    "        src_embed = self.tem_conv(src_idx_l, cut_time_l, self.num_layers, num_neighbors)\n",
    "        target_embed = self.tem_conv(target_idx_l, cut_time_l, self.num_layers, num_neighbors)\n",
    "        background_embed = self.tem_conv(background_idx_l, cut_time_l, self.num_layers, num_neighbors)\n",
    "        pos_score = self.affinity_score(src_embed, target_embed).squeeze(dim=-1)\n",
    "        neg_score = self.affinity_score(src_embed, background_embed).squeeze(dim=-1)\n",
    "        return pos_score.sigmoid(), neg_score.sigmoid()\n",
    "\n",
    "    def tem_conv(self, src_idx_l, cut_time_l, curr_layers, num_neighbors=20):\n",
    "        assert (curr_layers >= 0)\n",
    "\n",
    "        device = self.n_feat_th.device\n",
    "\n",
    "        batch_size = len(src_idx_l)\n",
    "\n",
    "        src_node_batch_th = torch.from_numpy(src_idx_l).long().to(device)\n",
    "        cut_time_l_th = torch.from_numpy(cut_time_l).float().to(device)\n",
    "\n",
    "        cut_time_l_th = torch.unsqueeze(cut_time_l_th, dim=1)\n",
    "        # query node always has the start time -> time span == 0\n",
    "        src_node_t_embed = self.time_encoder(torch.zeros_like(cut_time_l_th))\n",
    "        src_node_feat = self.node_raw_embed(src_node_batch_th)\n",
    "\n",
    "        if curr_layers == 0:\n",
    "            return src_node_feat\n",
    "        else:\n",
    "            src_node_conv_feat = self.tem_conv(src_idx_l,\n",
    "                                               cut_time_l,\n",
    "                                               curr_layers=curr_layers - 1,\n",
    "                                               num_neighbors=num_neighbors)\n",
    "\n",
    "            src_ngh_node_batch, src_ngh_eidx_batch, src_ngh_t_batch = self.ngh_finder.get_temporal_neighbor(\n",
    "                src_idx_l,\n",
    "                cut_time_l,\n",
    "                num_neighbors=num_neighbors)\n",
    "\n",
    "            src_ngh_node_batch_th = torch.from_numpy(src_ngh_node_batch).long().to(device)\n",
    "            src_ngh_eidx_batch = torch.from_numpy(src_ngh_eidx_batch).long().to(device)\n",
    "\n",
    "            src_ngh_t_batch_delta = cut_time_l[:, np.newaxis] - src_ngh_t_batch\n",
    "            src_ngh_t_batch_th = torch.from_numpy(src_ngh_t_batch_delta).float().to(device)\n",
    "\n",
    "            # get previous layer's node features\n",
    "            src_ngh_node_batch_flat = src_ngh_node_batch.flatten()  # reshape(batch_size, -1)\n",
    "            src_ngh_t_batch_flat = src_ngh_t_batch.flatten()  # reshape(batch_size, -1)\n",
    "            src_ngh_node_conv_feat = self.tem_conv(src_ngh_node_batch_flat,\n",
    "                                                   src_ngh_t_batch_flat,\n",
    "                                                   curr_layers=curr_layers - 1,\n",
    "                                                   num_neighbors=num_neighbors)\n",
    "            src_ngh_feat = src_ngh_node_conv_feat.view(batch_size, num_neighbors, -1)\n",
    "\n",
    "            # get edge time features and node features\n",
    "            src_ngh_t_embed = self.time_encoder(src_ngh_t_batch_th)\n",
    "            src_ngn_edge_feat = self.edge_raw_embed(src_ngh_eidx_batch)\n",
    "\n",
    "            # attention aggregation\n",
    "            mask = src_ngh_node_batch_th == 0\n",
    "            attn_m = self.attn_model_list[curr_layers - 1]\n",
    "\n",
    "            local, weight = attn_m(src_node_conv_feat,\n",
    "                                   src_node_t_embed,\n",
    "                                   src_ngh_feat,\n",
    "                                   src_ngh_t_embed,\n",
    "                                   src_ngn_edge_feat,\n",
    "                                   mask)\n",
    "            return local"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NeighborFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeighborFinder:\n",
    "\n",
    "    def __init__(self, adj_list, uniform=False):\n",
    "        \"\"\"\n",
    "        Params\n",
    "        ------\n",
    "        node_idx_l: List[int]\n",
    "        node_ts_l: List[int]\n",
    "        off_set_l: List[int], such that node_idx_l[off_set_l[i]:off_set_l[i + 1]] = adjacent_list[i]\n",
    "        \"\"\"\n",
    "\n",
    "        node_idx_l, node_ts_l, edge_idx_l, off_set_l = self.init_off_set(adj_list)\n",
    "        self.node_idx_l = node_idx_l\n",
    "        self.node_ts_l = node_ts_l\n",
    "        self.edge_idx_l = edge_idx_l\n",
    "\n",
    "        self.off_set_l = off_set_l\n",
    "\n",
    "        self.uniform = uniform\n",
    "\n",
    "    def init_off_set(self, adj_list):\n",
    "        \"\"\"\n",
    "        Params\n",
    "        ------\n",
    "        adj_list: List[List[int]]\n",
    "        \n",
    "        \"\"\"\n",
    "        n_idx_l = []\n",
    "        n_ts_l = []\n",
    "        e_idx_l = []\n",
    "        off_set_l = [0]\n",
    "        for i in range(len(adj_list)):\n",
    "            curr = adj_list[i]\n",
    "            curr = sorted(curr, key=lambda x: x[1])\n",
    "            n_idx_l.extend([x[0] for x in curr])\n",
    "            e_idx_l.extend([x[1] for x in curr])\n",
    "            n_ts_l.extend([x[2] for x in curr])\n",
    "\n",
    "            off_set_l.append(len(n_idx_l))\n",
    "\n",
    "        n_idx_l = np.array(n_idx_l)\n",
    "        n_ts_l = np.array(n_ts_l)\n",
    "        e_idx_l = np.array(e_idx_l)\n",
    "        off_set_l = np.array(off_set_l)\n",
    "\n",
    "        assert (len(n_idx_l) == len(n_ts_l))\n",
    "        assert (off_set_l[-1] == len(n_ts_l))\n",
    "\n",
    "        return n_idx_l, n_ts_l, e_idx_l, off_set_l\n",
    "\n",
    "    def find_before(self, src_idx, cut_time):\n",
    "        \"\"\"\n",
    "        Params\n",
    "        ------\n",
    "        src_idx: int\n",
    "        cut_time: float\n",
    "        \"\"\"\n",
    "        node_idx_l = self.node_idx_l\n",
    "        node_ts_l = self.node_ts_l\n",
    "        edge_idx_l = self.edge_idx_l\n",
    "        off_set_l = self.off_set_l\n",
    "\n",
    "        neighbors_idx = node_idx_l[off_set_l[src_idx]:off_set_l[src_idx + 1]]\n",
    "        neighbors_ts = node_ts_l[off_set_l[src_idx]:off_set_l[src_idx + 1]]\n",
    "        neighbors_e_idx = edge_idx_l[off_set_l[src_idx]:off_set_l[src_idx + 1]]\n",
    "\n",
    "        if len(neighbors_idx) == 0 or len(neighbors_ts) == 0:\n",
    "            return neighbors_idx, neighbors_ts, neighbors_e_idx\n",
    "\n",
    "        left = 0\n",
    "        right = len(neighbors_idx) - 1\n",
    "\n",
    "        while left + 1 < right:\n",
    "            mid = (left + right) // 2\n",
    "            curr_t = neighbors_ts[mid]\n",
    "            if curr_t < cut_time:\n",
    "                left = mid\n",
    "            else:\n",
    "                right = mid\n",
    "\n",
    "        if neighbors_ts[right] < cut_time:\n",
    "            return neighbors_idx[:right], neighbors_e_idx[:right], neighbors_ts[:right]\n",
    "        else:\n",
    "            return neighbors_idx[:left], neighbors_e_idx[:left], neighbors_ts[:left]\n",
    "\n",
    "    def get_temporal_neighbor(self, src_idx_l, cut_time_l, num_neighbors=20):\n",
    "        \"\"\"\n",
    "        Params\n",
    "        ------\n",
    "        src_idx_l: List[int]\n",
    "        cut_time_l: List[float],\n",
    "        num_neighbors: int\n",
    "        \"\"\"\n",
    "        assert (len(src_idx_l) == len(cut_time_l))\n",
    "\n",
    "        out_ngh_node_batch = np.zeros((len(src_idx_l), num_neighbors)).astype(np.int32)\n",
    "        out_ngh_t_batch = np.zeros((len(src_idx_l), num_neighbors)).astype(np.float32)\n",
    "        out_ngh_eidx_batch = np.zeros((len(src_idx_l), num_neighbors)).astype(np.int32)\n",
    "\n",
    "        for i, (src_idx, cut_time) in enumerate(zip(src_idx_l, cut_time_l)):\n",
    "            ngh_idx, ngh_eidx, ngh_ts = self.find_before(src_idx, cut_time)\n",
    "\n",
    "            if len(ngh_idx) > 0:\n",
    "                if self.uniform:\n",
    "                    sampled_idx = np.random.randint(0, len(ngh_idx), num_neighbors)\n",
    "\n",
    "                    out_ngh_node_batch[i, :] = ngh_idx[sampled_idx]\n",
    "                    out_ngh_t_batch[i, :] = ngh_ts[sampled_idx]\n",
    "                    out_ngh_eidx_batch[i, :] = ngh_eidx[sampled_idx]\n",
    "\n",
    "                    # resort based on time\n",
    "                    pos = out_ngh_t_batch[i, :].argsort()\n",
    "                    out_ngh_node_batch[i, :] = out_ngh_node_batch[i, :][pos]\n",
    "                    out_ngh_t_batch[i, :] = out_ngh_t_batch[i, :][pos]\n",
    "                    out_ngh_eidx_batch[i, :] = out_ngh_eidx_batch[i, :][pos]\n",
    "                else:\n",
    "                    ngh_ts = ngh_ts[:num_neighbors]\n",
    "                    ngh_idx = ngh_idx[:num_neighbors]\n",
    "                    ngh_eidx = ngh_eidx[:num_neighbors]\n",
    "\n",
    "                    assert (len(ngh_idx) <= num_neighbors)\n",
    "                    assert (len(ngh_ts) <= num_neighbors)\n",
    "                    assert (len(ngh_eidx) <= num_neighbors)\n",
    "\n",
    "                    out_ngh_node_batch[i, num_neighbors - len(ngh_idx):] = ngh_idx\n",
    "                    out_ngh_t_batch[i, num_neighbors - len(ngh_ts):] = ngh_ts\n",
    "                    out_ngh_eidx_batch[i, num_neighbors - len(ngh_eidx):] = ngh_eidx\n",
    "\n",
    "        return out_ngh_node_batch, out_ngh_eidx_batch, out_ngh_t_batch\n",
    "\n",
    "    def find_k_hop(self, k, src_idx_l, cut_time_l, num_neighbors=20):\n",
    "        \"\"\"\n",
    "        Sampling the k-hop sub graph\n",
    "        \"\"\"\n",
    "        x, y, z = self.get_temporal_neighbor(src_idx_l, cut_time_l, num_neighbors)\n",
    "        node_records = [x]\n",
    "        eidx_records = [y]\n",
    "        t_records = [z]\n",
    "        for _ in range(k - 1):\n",
    "            ngn_node_est, ngh_t_est = node_records[-1], t_records[-1]  # [N, *([num_neighbors] * (k - 1))]\n",
    "            orig_shape = ngn_node_est.shape\n",
    "            ngn_node_est = ngn_node_est.flatten()\n",
    "            ngn_t_est = ngh_t_est.flatten()\n",
    "            out_ngh_node_batch, out_ngh_eidx_batch, out_ngh_t_batch = self.get_temporal_neighbor(ngn_node_est,\n",
    "                                                                                                 ngn_t_est,\n",
    "                                                                                                 num_neighbors)\n",
    "            out_ngh_node_batch = out_ngh_node_batch.reshape(*orig_shape, num_neighbors)  # [N, *([num_neighbors] * k)]\n",
    "            out_ngh_eidx_batch = out_ngh_eidx_batch.reshape(*orig_shape, num_neighbors)\n",
    "            out_ngh_t_batch = out_ngh_t_batch.reshape(*orig_shape, num_neighbors)\n",
    "\n",
    "            node_records.append(out_ngh_node_batch)\n",
    "            eidx_records.append(out_ngh_eidx_batch)\n",
    "            t_records.append(out_ngh_t_batch)\n",
    "            \n",
    "        return node_records, eidx_records, t_records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EarlyStopMonitor & RandEdgeSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function and class\n",
    "class EarlyStopMonitor(object):\n",
    "    def __init__(self, max_round=3, higher_better=True, tolerance=1e-3):\n",
    "        self.max_round = max_round\n",
    "        self.num_round = 0\n",
    "\n",
    "        self.epoch_count = 0\n",
    "        self.best_epoch = 0\n",
    "\n",
    "        self.last_best = None\n",
    "        self.higher_better = higher_better\n",
    "        self.tolerance = tolerance\n",
    "\n",
    "    def early_stop_check(self, curr_val):\n",
    "        self.epoch_count += 1\n",
    "\n",
    "        if not self.higher_better:\n",
    "            curr_val *= -1\n",
    "        if self.last_best is None:\n",
    "            self.last_best = curr_val\n",
    "        elif (curr_val - self.last_best) / np.abs(self.last_best) > self.tolerance:\n",
    "            self.last_best = curr_val\n",
    "            self.num_round = 0\n",
    "            self.best_epoch = self.epoch_count\n",
    "        else:\n",
    "            self.num_round += 1\n",
    "        return self.num_round >= self.max_round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandEdgeSampler(object):\n",
    "    def __init__(self, src_list, dst_list):\n",
    "        self.src_list = np.unique(src_list)\n",
    "        self.dst_list = np.unique(dst_list)\n",
    "\n",
    "    def sample(self, size):\n",
    "        src_index = np.random.randint(0, len(self.src_list), size)\n",
    "        dst_index = np.random.randint(0, len(self.dst_list), size)\n",
    "        return self.src_list[src_index], self.dst_list[dst_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Parser_Dict():\n",
    "    # Argument and global variables\n",
    "    parser = argparse.ArgumentParser('Interface for TGAT experiments on link predictions')\n",
    "    \n",
    "    parser.add_argument('-d', '--data', type=str, help='data sources to use, try wikipedia or reddit', default='wikipedia')\n",
    "    parser.add_argument('--bs', type=int, default=200, help='batch_size')\n",
    "    parser.add_argument('--prefix', type=str, default='', help='prefix to name the checkpoints')\n",
    "    parser.add_argument('--n_degree', type=int, default=20, help='number of neighbors to sample')\n",
    "    parser.add_argument('--n_head', type=int, default=2, help='number of heads used in attention layer')\n",
    "    parser.add_argument('--n_epoch', type=int, default=50, help='number of epochs')\n",
    "    parser.add_argument('--n_layer', type=int, default=2, help='number of network layers')\n",
    "    parser.add_argument('--lr', type=float, default=0.0001, help='learning rate')\n",
    "    parser.add_argument('--drop_out', type=float, default=0.1, help='dropout probability')\n",
    "    # parser.add_argument('--gpu', type=int, default=0, help='idx for the gpu to use')\n",
    "    parser.add_argument('--node_dim', type=int, default=100, help='Dimensions of the node embedding')\n",
    "    parser.add_argument('--time_dim', type=int, default=100, help='Dimensions of the time embedding')\n",
    "    parser.add_argument('--agg_method', type=str, choices=['attn', 'lstm', 'mean'], help='local aggregation method', default='attn')\n",
    "    parser.add_argument('--attn_mode', type=str, choices=['prod', 'map'], default='prod', help='use dot product attention or mapping based')\n",
    "    parser.add_argument('--time', type=str, choices=['time', 'pos', 'empty'], help='how to use time information', default='time')\n",
    "    parser.add_argument('--uniform', action='store_true', help='take uniform sampling from temporal neighbors')\n",
    "\n",
    "    try:\n",
    "        args = parser.parse_args(args=[])\n",
    "    except:\n",
    "        parser.print_help()\n",
    "        sys.exit(0)\n",
    "        \n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Parser_Dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(agg_method='attn', attn_mode='prod', bs=200, data='wikipedia', drop_out=0.1, lr=0.0001, n_degree=20, n_epoch=50, n_head=2, n_layer=2, node_dim=100, prefix='', time='time', time_dim=100, uniform=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = args.bs\n",
    "NUM_NEIGHBORS = args.n_degree\n",
    "NUM_NEG = 1\n",
    "NUM_EPOCH = args.n_epoch\n",
    "NUM_HEADS = args.n_head\n",
    "DROP_OUT = args.drop_out\n",
    "# GPU = args.gpu\n",
    "UNIFORM = args.uniform\n",
    "# NEW_NODE = args.new_node\n",
    "USE_TIME = args.time\n",
    "AGG_METHOD = args.agg_method\n",
    "ATTN_MODE = args.attn_mode\n",
    "SEQ_LEN = NUM_NEIGHBORS\n",
    "DATA = args.data\n",
    "NUM_LAYER = args.n_layer\n",
    "LEARNING_RATE = args.lr\n",
    "NODE_DIM = args.node_dim\n",
    "TIME_DIM = args.time_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = r\"C:\\Users\\sss\\Desktop\\TGAT-REF-master\\TGAT-REF\"\n",
    "\n",
    "MODEL_SAVE_PATH = root + f'/saved_models/{args.prefix}-{args.agg_method}-{args.attn_mode}-{args.data}.pth'\n",
    "get_checkpoint_path = lambda epoch: root + f'/saved_checkpoints/{args.prefix}-{args.agg_method}-{args.attn_mode}-{args.data}-{epoch}.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\sss\\\\Desktop\\\\TGAT-REF-master\\\\TGAT-REF/saved_models/-attn-prod-wikipedia.pth'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_SAVE_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- set up logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Namespace(agg_method='attn', attn_mode='prod', bs=200, data='wikipedia', drop_out=0.1, lr=0.0001, n_degree=20, n_epoch=50, n_head=2, n_layer=2, node_dim=100, prefix='', time='time', time_dim=100, uniform=False)\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "fh = logging.FileHandler(r'C:\\Users\\sss\\Desktop\\TGAT-REF-master\\TGAT-REF/log/{}.log'.format(str(time.time())))\n",
    "fh.setLevel(logging.DEBUG)\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.WARN)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "fh.setFormatter(formatter)\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(fh)\n",
    "logger.addHandler(ch)\n",
    "logger.info(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 测试函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_one_epoch(hint, tgan, sampler, src, dst, ts, label):\n",
    "    val_acc, val_ap, val_f1, val_auc = [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        tgan = tgan.eval()\n",
    "        TEST_BATCH_SIZE = 30\n",
    "        num_test_instance = len(src)\n",
    "        num_test_batch = math.ceil(num_test_instance / TEST_BATCH_SIZE)\n",
    "        for k in range(num_test_batch):\n",
    "            # percent = 100 * k / num_test_batch\n",
    "            # if k % int(0.2 * num_test_batch) == 0:\n",
    "            #     logger.info('{0} progress: {1:10.4f}'.format(hint, percent))\n",
    "            s_idx = k * TEST_BATCH_SIZE\n",
    "            e_idx = min(num_test_instance - 1, s_idx + TEST_BATCH_SIZE)\n",
    "            src_l_cut = src[s_idx:e_idx]\n",
    "            dst_l_cut = dst[s_idx:e_idx]\n",
    "            ts_l_cut = ts[s_idx:e_idx]\n",
    "            # label_l_cut = label[s_idx:e_idx]\n",
    "\n",
    "            size = len(src_l_cut)\n",
    "            src_l_fake, dst_l_fake = sampler.sample(size)\n",
    "\n",
    "            pos_prob, neg_prob = tgan.contrast(src_l_cut, dst_l_cut, dst_l_fake, ts_l_cut, NUM_NEIGHBORS)\n",
    "\n",
    "            pred_score = np.concatenate([(pos_prob).cpu().numpy(), (neg_prob).cpu().numpy()])\n",
    "            pred_label = pred_score > 0.5\n",
    "            true_label = np.concatenate([np.ones(size), np.zeros(size)])\n",
    "\n",
    "            val_acc.append((pred_label == true_label).mean())\n",
    "            val_ap.append(average_precision_score(true_label, pred_score))\n",
    "            # val_f1.append(f1_score(true_label, pred_label))\n",
    "            val_auc.append(roc_auc_score(true_label, pred_score))\n",
    "            \n",
    "    return np.mean(val_acc), np.mean(val_ap), np.mean(val_f1), np.mean(val_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load data and train val test split\n",
    "g_df = np.load(root + '/processed/ml_{}.npy'.format(DATA))\n",
    "e_feat = np.load(root + '/processed/ml_{}_edge.npy'.format(DATA))\n",
    "n_feat = np.load(root + '/processed/ml_{}_node.npy'.format(DATA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(157474, 6), (157475, 172), (8227, 172)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[g_df.shape, e_feat.shape, n_feat.shape]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_df = pd.DataFrame(g_df, columns = [\"\", \"u\", \"i\", \"ts\", \"label\", \"idx\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_time, test_time = list(np.quantile(g_df.ts, [0.70, 0.85]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1862652.0999999999, 2218288.5999999996]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[val_time, test_time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_l = g_df.u.values\n",
    "dst_l = g_df.i.values\n",
    "e_idx_l = g_df.idx.values\n",
    "label_l = g_df.label.values\n",
    "ts_l = g_df.ts.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_src_index = src_l.max()\n",
    "max_idx = max(src_l.max(), dst_l.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_node_set = set(np.unique(np.hstack([g_df.u.values, g_df.i.values])))\n",
    "num_total_unique_nodes = len(total_node_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_node_set = set(random.sample(set(src_l[ts_l > val_time]).union(set(dst_l[ts_l > val_time])), int(0.1 * num_total_unique_nodes)))\n",
    "mask_src_flag = g_df.u.map(lambda x: x in mask_node_set).values\n",
    "mask_dst_flag = g_df.i.map(lambda x: x in mask_node_set).values\n",
    "none_node_flag = (1 - mask_src_flag) * (1 - mask_dst_flag)\n",
    "\n",
    "valid_train_flag = (ts_l <= val_time) * (none_node_flag > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_src_l = src_l[valid_train_flag]\n",
    "train_dst_l = dst_l[valid_train_flag]\n",
    "train_ts_l = ts_l[valid_train_flag]\n",
    "train_e_idx_l = e_idx_l[valid_train_flag]\n",
    "train_label_l = label_l[valid_train_flag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the new nodes sets for testing inductiveness of the model\n",
    "train_node_set = set(train_src_l).union(train_dst_l)\n",
    "assert (len(train_node_set - mask_node_set) == len(train_node_set))\n",
    "new_node_set = total_node_set - train_node_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select validation and test dataset\n",
    "valid_val_flag = (ts_l <= test_time) * (ts_l > val_time)\n",
    "valid_test_flag = ts_l > test_time\n",
    "\n",
    "is_new_node_edge = np.array([(a in new_node_set or b in new_node_set) for a, b in zip(src_l, dst_l)])\n",
    "nn_val_flag = valid_val_flag * is_new_node_edge\n",
    "nn_test_flag = valid_test_flag * is_new_node_edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation and test with all edges\n",
    "val_src_l = src_l[valid_val_flag]\n",
    "val_dst_l = dst_l[valid_val_flag]\n",
    "val_ts_l = ts_l[valid_val_flag]\n",
    "val_e_idx_l = e_idx_l[valid_val_flag]\n",
    "val_label_l = label_l[valid_val_flag]\n",
    "\n",
    "test_src_l = src_l[valid_test_flag]\n",
    "test_dst_l = dst_l[valid_test_flag]\n",
    "test_ts_l = ts_l[valid_test_flag]\n",
    "test_e_idx_l = e_idx_l[valid_test_flag]\n",
    "test_label_l = label_l[valid_test_flag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation and test with edges that at least has one new node (not in training set)\n",
    "nn_val_src_l = src_l[nn_val_flag]\n",
    "nn_val_dst_l = dst_l[nn_val_flag]\n",
    "nn_val_ts_l = ts_l[nn_val_flag]\n",
    "nn_val_e_idx_l = e_idx_l[nn_val_flag]\n",
    "nn_val_label_l = label_l[nn_val_flag]\n",
    "\n",
    "nn_test_src_l = src_l[nn_test_flag]\n",
    "nn_test_dst_l = dst_l[nn_test_flag]\n",
    "nn_test_ts_l = ts_l[nn_test_flag]\n",
    "nn_test_e_idx_l = e_idx_l[nn_test_flag]\n",
    "nn_test_label_l = label_l[nn_test_flag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Initialize the data structure for graph and edge sampling\n",
    "# build the graph for fast query\n",
    "# graph only contains the training data (with 10% nodes removal)\n",
    "adj_list = [[] for _ in range(max_idx + 1)]\n",
    "for src, dst, eidx, ts in zip(train_src_l, train_dst_l, train_e_idx_l, train_ts_l):\n",
    "    adj_list[src].append((dst, eidx, ts))\n",
    "    adj_list[dst].append((src, eidx, ts))\n",
    "    \n",
    "train_ngh_finder = NeighborFinder(adj_list, uniform=UNIFORM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full graph with all the data for the test and validation purpose\n",
    "full_adj_list = [[] for _ in range(max_idx + 1)]\n",
    "for src, dst, eidx, ts in zip(src_l, dst_l, e_idx_l, ts_l):\n",
    "    full_adj_list[src].append((dst, eidx, ts))\n",
    "    full_adj_list[dst].append((src, eidx, ts))\n",
    "full_ngh_finder = NeighborFinder(full_adj_list, uniform=UNIFORM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rand_sampler = RandEdgeSampler(train_src_l, train_dst_l)\n",
    "val_rand_sampler = RandEdgeSampler(src_l, dst_l)\n",
    "nn_val_rand_sampler = RandEdgeSampler(nn_val_src_l, nn_val_dst_l)\n",
    "test_rand_sampler = RandEdgeSampler(src_l, dst_l)\n",
    "nn_test_rand_sampler = RandEdgeSampler(nn_test_src_l, nn_test_dst_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Aggregation uses attention model\n",
      "INFO:__main__:Using scaled prod attention\n",
      "INFO:__main__:Using scaled prod attention\n",
      "INFO:__main__:Using time encoding\n"
     ]
    }
   ],
   "source": [
    "### Model initialize\n",
    "# device = torch.device('cuda:{}'.format(GPU))\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tgan = TGAN(train_ngh_finder, n_feat, e_feat,\n",
    "            num_layers=NUM_LAYER, use_time=USE_TIME, agg_method=AGG_METHOD, attn_mode=ATTN_MODE,\n",
    "            seq_len=SEQ_LEN, n_head=NUM_HEADS, drop_out=DROP_OUT, node_dim=NODE_DIM, time_dim=TIME_DIM)\n",
    "optimizer = torch.optim.Adam(tgan.parameters(), lr=LEARNING_RATE)\n",
    "criterion = torch.nn.BCELoss()\n",
    "tgan = tgan.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TGAN(\n",
       "  (edge_raw_embed): Embedding(157475, 172, padding_idx=0)\n",
       "  (node_raw_embed): Embedding(8227, 172, padding_idx=0)\n",
       "  (merge_layer): MergeLayer(\n",
       "    (fc1): Linear(in_features=344, out_features=172, bias=True)\n",
       "    (fc2): Linear(in_features=172, out_features=172, bias=True)\n",
       "    (act): ReLU()\n",
       "  )\n",
       "  (attn_model_list): ModuleList(\n",
       "    (0): AttnModel(\n",
       "      (merger): MergeLayer(\n",
       "        (fc1): Linear(in_features=688, out_features=172, bias=True)\n",
       "        (fc2): Linear(in_features=172, out_features=172, bias=True)\n",
       "        (act): ReLU()\n",
       "      )\n",
       "      (multi_head_target): MultiHeadAttention(\n",
       "        (w_qs): Linear(in_features=516, out_features=516, bias=False)\n",
       "        (w_ks): Linear(in_features=516, out_features=516, bias=False)\n",
       "        (w_vs): Linear(in_features=516, out_features=516, bias=False)\n",
       "        (attention): ScaledDotProductAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (softmax): Softmax(dim=2)\n",
       "        )\n",
       "        (layer_norm): LayerNorm((516,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc): Linear(in_features=516, out_features=516, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): AttnModel(\n",
       "      (merger): MergeLayer(\n",
       "        (fc1): Linear(in_features=688, out_features=172, bias=True)\n",
       "        (fc2): Linear(in_features=172, out_features=172, bias=True)\n",
       "        (act): ReLU()\n",
       "      )\n",
       "      (multi_head_target): MultiHeadAttention(\n",
       "        (w_qs): Linear(in_features=516, out_features=516, bias=False)\n",
       "        (w_ks): Linear(in_features=516, out_features=516, bias=False)\n",
       "        (w_vs): Linear(in_features=516, out_features=516, bias=False)\n",
       "        (attention): ScaledDotProductAttention(\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (softmax): Softmax(dim=2)\n",
       "        )\n",
       "        (layer_norm): LayerNorm((516,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc): Linear(in_features=516, out_features=516, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (time_encoder): TimeEncode()\n",
       "  (affinity_score): MergeLayer(\n",
       "    (fc1): Linear(in_features=344, out_features=172, bias=True)\n",
       "    (fc2): Linear(in_features=172, out_features=1, bias=True)\n",
       "    (act): ReLU()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:num of training instances: 77901\n",
      "INFO:root:num of batches per epoch: 390\n"
     ]
    }
   ],
   "source": [
    "num_instance = len(train_src_l)\n",
    "num_batch = math.ceil(num_instance / BATCH_SIZE)\n",
    "\n",
    "logger.info('num of training instances: {}'.format(num_instance))\n",
    "logger.info('num of batches per epoch: {}'.format(num_batch))\n",
    "idx_list = np.arange(num_instance)\n",
    "np.random.shuffle(idx_list)\n",
    "\n",
    "early_stopper = EarlyStopMonitor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:start 1 epoch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1 | Loss:1.8414644002914429\n",
      "Epoch:1 | Loss:1.406896710395813\n",
      "Epoch:1 | Loss:1.304837703704834\n",
      "Epoch:1 | Loss:1.2079803943634033\n",
      "Epoch:1 | Loss:1.2463066577911377\n",
      "Epoch:1 | Loss:1.1827261447906494\n",
      "Epoch:1 | Loss:1.1616902351379395\n",
      "Epoch:1 | Loss:1.1514474153518677\n",
      "Epoch:1 | Loss:1.2140980958938599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, NUM_EPOCH + 1):\n",
    "    # Training \n",
    "    # training use only training graph\n",
    "    tgan.ngh_finder = train_ngh_finder\n",
    "    acc, ap, f1, auc, m_loss = [], [], [], [], []\n",
    "    np.random.shuffle(idx_list)\n",
    "    logger.info('start {} epoch'.format(epoch))\n",
    "    \n",
    "    for k in range(1, num_batch + 1):\n",
    "        # percent = 100 * k / num_batch\n",
    "        # if k % int(0.2 * num_batch) == 0:\n",
    "        #     logger.info('progress: {0:10.4f}'.format(percent))\n",
    "\n",
    "        s_idx = k * BATCH_SIZE\n",
    "        e_idx = min(num_instance - 1, s_idx + BATCH_SIZE)\n",
    "        src_l_cut, dst_l_cut = train_src_l[s_idx:e_idx], train_dst_l[s_idx:e_idx]\n",
    "        ts_l_cut = train_ts_l[s_idx:e_idx]\n",
    "        label_l_cut = train_label_l[s_idx:e_idx]\n",
    "        size = len(src_l_cut)\n",
    "        src_l_fake, dst_l_fake = train_rand_sampler.sample(size)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pos_label = torch.ones(size, dtype=torch.float, device=device)\n",
    "            neg_label = torch.zeros(size, dtype=torch.float, device=device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        tgan = tgan.train()\n",
    "        pos_prob, neg_prob = tgan.contrast(src_l_cut, dst_l_cut, dst_l_fake, ts_l_cut, NUM_NEIGHBORS)\n",
    "\n",
    "        loss = criterion(pos_prob, pos_label)\n",
    "        loss += criterion(neg_prob, neg_label)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if k % 10 == 0:\n",
    "            print(\"Epoch:{} | Loss:{}\".format(epoch, loss.item()))\n",
    "        \n",
    "        # get training results\n",
    "        with torch.no_grad():\n",
    "            tgan = tgan.eval()\n",
    "            pred_score = np.concatenate([(pos_prob).cpu().detach().numpy(), (neg_prob).cpu().detach().numpy()])\n",
    "            pred_label = pred_score > 0.5\n",
    "            true_label = np.concatenate([np.ones(size), np.zeros(size)])\n",
    "            acc.append((pred_label == true_label).mean())\n",
    "            ap.append(average_precision_score(true_label, pred_score))\n",
    "            # f1.append(f1_score(true_label, pred_label))\n",
    "            m_loss.append(loss.item())\n",
    "            auc.append(roc_auc_score(true_label, pred_score))\n",
    "\n",
    "    # validation phase use all information\n",
    "    tgan.ngh_finder = full_ngh_finder\n",
    "    val_acc, val_ap, val_f1, val_auc = eval_one_epoch('val for old nodes', tgan, val_rand_sampler, val_src_l,\n",
    "                                                      val_dst_l, val_ts_l, val_label_l)\n",
    "\n",
    "    nn_val_acc, nn_val_ap, nn_val_f1, nn_val_auc = eval_one_epoch('val for new nodes', tgan, val_rand_sampler,\n",
    "                                                                  nn_val_src_l,\n",
    "                                                                  nn_val_dst_l, nn_val_ts_l, nn_val_label_l)\n",
    "\n",
    "    logger.info('epoch: {}:'.format(epoch))\n",
    "    logger.info('Epoch mean loss: {}'.format(np.mean(m_loss)))\n",
    "    logger.info('train acc: {}, val acc: {}, new node val acc: {}'.format(np.mean(acc), val_acc, nn_val_acc))\n",
    "    logger.info('train auc: {}, val auc: {}, new node val auc: {}'.format(np.mean(auc), val_auc, nn_val_auc))\n",
    "    logger.info('train ap: {}, val ap: {}, new node val ap: {}'.format(np.mean(ap), val_ap, nn_val_ap))\n",
    "    # logger.info('train f1: {}, val f1: {}, new node val f1: {}'.format(np.mean(f1), val_f1, nn_val_f1))\n",
    "\n",
    "    if early_stopper.early_stop_check(val_ap):\n",
    "        logger.info('No improvment over {} epochs, stop training'.format(early_stopper.max_round))\n",
    "        logger.info(f'Loading the best model at epoch {early_stopper.best_epoch}')\n",
    "        best_model_path = get_checkpoint_path(early_stopper.best_epoch)\n",
    "        tgan.load_state_dict(torch.load(best_model_path))\n",
    "        logger.info(f'Loaded the best model at epoch {early_stopper.best_epoch} for inference')\n",
    "        tgan.eval()\n",
    "        break\n",
    "    else:\n",
    "        torch.save(tgan.state_dict(), get_checkpoint_path(epoch))\n",
    "\n",
    "# testing phase use all information\n",
    "tgan.ngh_finder = full_ngh_finder\n",
    "test_acc, test_ap, test_f1, test_auc = eval_one_epoch('test for old nodes', tgan, test_rand_sampler, test_src_l,\n",
    "                                                      test_dst_l, test_ts_l, test_label_l)\n",
    "\n",
    "nn_test_acc, nn_test_ap, nn_test_f1, nn_test_auc = eval_one_epoch('test for new nodes', tgan, nn_test_rand_sampler,\n",
    "                                                                  nn_test_src_l,\n",
    "                                                                  nn_test_dst_l, nn_test_ts_l, nn_test_label_l)\n",
    "\n",
    "logger.info('Test statistics: Old nodes -- acc: {}, auc: {}, ap: {}'.format(test_acc, test_auc, test_ap))\n",
    "logger.info('Test statistics: New nodes -- acc: {}, auc: {}, ap: {}'.format(nn_test_acc, nn_test_auc, nn_test_ap))\n",
    "\n",
    "logger.info('Saving TGAN model')\n",
    "torch.save(tgan.state_dict(), MODEL_SAVE_PATH)\n",
    "logger.info('TGAN models saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
